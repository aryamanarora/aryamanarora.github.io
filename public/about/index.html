<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>About | Aryaman Arora</title>
<meta name=keywords content><meta name=description content="I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.
I want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I&rsquo;ve been thinking about recently:
How can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/about/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/about/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="About"><meta property="og:description" content="I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.
I want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I&rsquo;ve been thinking about recently:
How can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/about/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="About"><meta name=twitter:description content="I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.
I want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I&rsquo;ve been thinking about recently:
How can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"About","item":"http://localhost:1313/about/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"About","name":"About","description":"I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.\nI want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I\u0026rsquo;ve been thinking about recently:\nHow can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc.","keywords":[],"articleBody":"I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.\nI want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I’ve been thinking about recently:\nHow can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc. In a self-supervised learning world, can linguistics still be useful in guiding how we do interpretability on language models? Can interpretability provide actionable findings that help us make better models? Machine learning is still a kind of alchemy. We should turn it into a science. To that end, I am inspired by work in NLP, causal inference, information theory, and psycholinguistics.\nOh, and besides doing research, I enjoy eating spicy food, (attempting to) play basketball and climb, and listening to rap. And if you handed me a violin, I would probably be able to make some sounds that are not too unpleasant.\nIf you want to chat about research or life, feel free to send me an email (aryamana [at] stanford [dot] edu)!\nBrief history I was born in New Delhi, India, raised in Savannah, Georgia (the U.S. state), and I think of home as Washington, D.C.—where I spent part of high school and my undergrad. Still, I’ve wanted to move to the Bay Area for a long time, and I’m glad I made it here!\nBefore coming to Stanford to start my Ph.D. in 2023, I completed my B.S. in Computer Science and Linguistics at Georgetown University. There, I was mentored by Nathan Schneider as a member of his research group NERT. In those days, I primarily worked on computational linguistics and did a lot of linguistic annotation for Indian languages. Regardless of what I currently work on, my research style is probably largely copied from Nathan’s.\nSince 2021, I have also been closely working with Ryan Cotterell at ETH Zürich on information theory, and I visited Switzerland in Summer 2021 and 2023. From working with Ryan, I learned to be a little less scared of doing math.\nIn 2022, I spent the summer at Apple in Seattle with Robert Daland working on evaluating robustness on a ton of languages for Siri, and winter at Redwood Research in Berkeley working on mechanistic interpretability.\nMy research interests pivoted significantly in late 2022 towards interpretability, but I still have a love for language(s).\n","wordCount":"426","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/about/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span class=active>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title=search><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a></div><h1 class="post-title entry-hint-parent">About</h1><div class=post-meta>2 min&nbsp;·&nbsp;426 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#brief-history>Brief history</a></li></ul></nav></div></details></div><div class=post-content><p>I am a first-year Ph.D. student at <a href=https://nlp.stanford.edu/><strong>Stanford NLP</strong></a> advised by <a href=http://web.stanford.edu/~jurafsky/>Dan Jurafsky</a> and <a href=https://web.stanford.edu/~cgpotts/>Christopher Potts</a>. My research is focused on <strong>interpretability</strong>.</p><p>I want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I&rsquo;ve been thinking about recently:</p><ul><li>How can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc.</li><li>In a self-supervised learning world, can linguistics still be useful in guiding how we do interpretability on language models?</li><li>Can interpretability provide actionable findings that help us make better models?</li></ul><p>Machine learning is still a kind of alchemy. We should turn it into a science. To that end, I am inspired by work in NLP, causal inference, information theory, and psycholinguistics.</p><p>Oh, and besides doing research, I enjoy eating spicy food, (attempting to) play basketball and climb, and listening to rap. And if you handed me a violin, I would probably be able to make some sounds that are not too unpleasant.</p><p>If you want to chat about research or life, feel free to send me an email (aryamana [at] stanford [dot] edu)!</p><h2 id=brief-history>Brief history<a hidden class=anchor aria-hidden=true href=#brief-history>#</a></h2><p>I was born in New Delhi, India, raised in Savannah, Georgia (the U.S. state), and I think of home as Washington, D.C.&mdash;where I spent part of high school and my undergrad. Still, I&rsquo;ve wanted to move to the Bay Area for a long time, and I&rsquo;m glad I made it here!</p><p>Before coming to Stanford to start my Ph.D. in 2023, I completed my B.S. in Computer Science and Linguistics at <a href=https://www.georgetown.edu/>Georgetown University</a>. There, I was mentored by <a href=https://people.cs.georgetown.edu/nschneid/>Nathan Schneider</a> as a member of his research group <a href=http://nert.georgetown.edu/>NERT</a>. In those days, I primarily worked on computational linguistics and did a lot of linguistic annotation for Indian languages. Regardless of what I currently work on, my research <em>style</em> is probably largely copied from Nathan&rsquo;s.</p><p>Since 2021, I have also been closely working with <a href=https://rycolab.io/authors/ryan/>Ryan Cotterell</a> at ETH Zürich on information theory, and I visited Switzerland in Summer 2021 and 2023. From working with Ryan, I learned to be a little less scared of doing math.</p><p>In 2022, I spent the summer at <a href=https://www.apple.com/>Apple</a> in Seattle with <a href=https://www.linkedin.com/in/robert-daland-176362111>Robert Daland</a> working on evaluating robustness on a ton of languages for Siri, and winter at <a href=https://www.redwoodresearch.org/>Redwood Research</a> in Berkeley working on mechanistic interpretability.</p><p>My research interests pivoted significantly in late 2022 towards interpretability, but I still have a love for language(s).</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>