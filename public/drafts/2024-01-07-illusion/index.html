<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why DAS may fail | Aryaman Arora</title>
<meta name=keywords content><meta name=description content="Why DAS may fail Aryaman Arora
January 7, 2024
Recently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:
a causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i."><meta name=author content="Aryaman Arora"><link rel=canonical href=http://localhost:1313/drafts/2024-01-07-illusion/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/drafts/2024-01-07-illusion/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="Why DAS may fail"><meta property="og:description" content="Why DAS may fail Aryaman Arora
January 7, 2024
Recently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:
a causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/drafts/2024-01-07-illusion/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content="drafts"><meta property="article:published_time" content="2024-01-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-07T00:00:00+00:00"><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="Why DAS may fail"><meta name=twitter:description content="Why DAS may fail Aryaman Arora
January 7, 2024
Recently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:
a causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Drafts","item":"http://localhost:1313/drafts/"},{"@type":"ListItem","position":2,"name":"Why DAS may fail","item":"http://localhost:1313/drafts/2024-01-07-illusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why DAS may fail","name":"Why DAS may fail","description":"Why DAS may fail Aryaman Arora\nJanuary 7, 2024\nRecently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:\na causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i.","keywords":[],"articleBody":"Why DAS may fail Aryaman Arora\nJanuary 7, 2024\nRecently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:\na causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i.e. intervening along it should have no causal effect a dormant direction that achieves the desired causal effect but is not activated differently by the base and source Furthermore, they provide experimental evidence of DAS producing apparently large causal effects (according to their logit-difference metric) when applied to transformer MLP modules, which disappear when the DAS vector is projected onto the MLP output rowspace.\nA forthcoming response by Wu et al. (2024) shows that their definition of an interpretability illusion is flawed, even on the toy model they present to illustrate the definition—it rules out the obviously non-illusory direction as having a significant illusory component. It’s also immediately apparent to us that the experimental setup they use in many instances is not one we would espouse for DAS: they overfit on one example for the fact-editing setup, they report FLDD and don’t note that the IIA metric is more robust to their claimed “illusion”, etc.\nHowever, I do not think this closes the matter. They were obviously wrong on many counts, but they are pointing to something concerning about DAS that neither of us really know how to explain but which we both recognise. It is experimentally true that DAS can “overfit”. In this brief post, I outline how this might be possible.\nToy examples The key interpretability problem we are trying to overcome with DAS, and others are trying to overcome with methods like LEACE (Belrose et al., 2023), is that we do not know the basis of the feature space that models operate on. We attempt to learn it by making a claim about counterfactual pairs of inputs and how they should be processed differently by the model; a method like DAS allows us to test such a hypothesis by trying to find a direction along which we can flip the model behaviour with regard to the counterfactual pair being tested.\nLet’s consider some simple scenarious about some toy feature spaces that can illustrate why 1D DAS may fail. Recall that 1D DAS learns a unit vector $\\mathbf{v}$ and applies the following patch along it:\n$$\\mathbf{act}{\\text{patched}} = \\mathbf{act}{\\text{base}} + (\\mathbf{v}^\\top\\mathbf{act}{\\text{src}} - \\mathbf{v}^\\top\\mathbf{act}{\\text{base}})\\mathbf{v}$$\n1D DAS on 1D feature The ideal scenario is that the feature we are trying to find with 1D DAS is indeed a 1D direction. This will obviously work perfectly, so I won’t walk through it.\n1D DAS on 2D feature Suppose that the feature we care about is actually represented as 2 orthogonal directions in activation space that take on values $\\in {0, 1}$, and the model’s output is some function of those. Can 1D DAS still learn a successful patch?\nWell, it is apparent that no matter what, 1D DAS cannot entirely patch the exact values along both directions. This is because the 1D DAS edit can only move the activation along the angle of the vector $\\mathbf{v}$, and a line is not sufficient to cover all possible 2D values.\nE.g., given $\\mathbf{act}{\\text{base}} = [0, 1]$, $\\mathbf{act}{\\text{src}} = [1, 0]$, and the basis directions being the two features, here are the results of the patch for some settings of DAS’s $\\mathbf{v}$:\n$\\mathbf{v}$ $\\mathbf{v}^\\top\\mathbf{act}_{\\text{base}}$ $\\mathbf{v}^\\top\\mathbf{act}_{\\text{src}}$ $\\mathbf{act}_{\\text{patched}}$ $[0, 1]$ $1$ $0$ $[0, 0]$ $[1, 0]$ $0$ $1$ $[1, 1]$ $\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]$ $\\frac{1}{\\sqrt{2}}$ $\\frac{1}{\\sqrt{2}}$ $[0, 1]$ $\\left[\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right]$ $\\frac{\\sqrt{3}}{2}$ $\\frac{1}{2}$ $[-0.183, 0.683]$ Also note that our 1D DAS projects from $\\mathbb{R}^2$ to $\\mathbb{R}^1$, so there will be collisions between activations that are an equal angle away from $\\mathbf{v}$, e.g. the third row in the table above.\nOkay, so 1D DAS does can’t learn to patch a 2D feature, which is great! We have successfully shown that illusions are impossible, and the blog post can end.\nNot quite. I will show in the next toy example how illusions may be possible.\n1D DAS to activate unrelated feature Let’s say our activation space is in $\\mathbb{R}^2$. Let’s say only the first dimension is causally relevant for our task, and all our inputs only vary along that: $[0, 0]$, $[1, 0]$. The second dimension is $0$ for all inputs.\nIf turning on the unrelated feature along the second dimension is somehow useful for the behaviour we want to induce, can 1D DAS activate it? Yes!\n$\\mathbf{v}$ Type Patch 1 Patch 2 $[1, 0]$ non-illusory $[1, 0] \\to [0, 0]$ $[0, 0] \\to [1, 0]$ $[0, 1]$ dormant (no effect) $[1, 0] \\to [1, 0]$ $[0, 0] \\to [0, 0]$ $\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]$ ??? $[1, 0] \\to [0.293, -0.707]$ $[0, 0] \\to [0.707, 0.707]$ In the last row above, we successfully modified the value of the second dimension such that it depends on the value along the first dimension. This isn’t new though; this is exactly what Makelov et al. (2023) claimed. Their definition was wrong, but this is the phenomenon they were talking about.\nHere is how I would describe this illusory behaviour as:\nGiven two sets of activations $X$ and $Y$:\nA feature that has (approximately) one value for $X$ and (approximately) a different value for $Y$ A feature, orthogonal and thus uncorrelated to the first, that has (approximately) the same value for both $X$ and $Y$ 1D DAS can induce a correlation between the two features, such that a patch can modify the value of (2) based on the value of (1). This results in patched activations where (2) takes on values never encountered in the input sets $X$, $Y$.\nThis is possible because the DAS vector $\\mathbf{v}$ is non-orthogonal to both features. Essentially, we give up some causal effect on feature 1 in order to create a causal effect on feature 2.\nNotice that we are only talking about features and not the rowspace/nullspace of downstream components. While Makelov et al. were on the right track, I think the downstream component subspace stuff is a distraction from the core point about DAS illusion.\nBut is this actually relevant beyond being a cute toy example? I will show how this idea lets us predict actual illusory behaviour by DAS!\nReal example A dataset for DAS on an autogressive language model includes pairs of counterfactual inputs and associated next-token predictions, such that the model should give one output for one input in a pair and a different output for the other input in the pair.\nWe will focus on interventions at the last block_output at the last token position. We will also only consider a single pair of inputs, i.e. the “factual recall” setting in Makelov et al. (2023) where DAS can overfit.\nConsider the activations at the last position at the last block_output for the two inputs. We will call those $\\mathbf{a}$ and $\\mathbf{b}$. I claim that DAS can learn an vector $\\mathbf{v}$ that perfectly turns $\\mathbf{a}$ into $\\mathbf{b}$ and vice versa.\nProof. Set $\\mathbf{v} := \\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})$, i.e. the unit vector along the difference between the two activations. If we run the intervention along this direction, we achieve the desired output:\n$$ \\begin{aligned} \\text{patch}(\\mathbf{b} \\to \\mathbf{a}) \u0026= \\mathbf{b} + (\\mathbf{v}^\\top\\mathbf{a} - \\mathbf{v}^\\top\\mathbf{b})\\mathbf{v} \\ \u0026= \\mathbf{b} + (\\mathbf{v}^\\top(\\mathbf{a} - \\mathbf{b}))\\mathbf{v}\\ \u0026= \\mathbf{b} + \\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{a} - \\mathbf{b})\\right)\\mathbf{v}\\ \u0026= \\mathbf{b} + \\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert} \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^2\\right)\\mathbf{v}\\ \u0026= \\mathbf{b} + \\lVert \\mathbf{a} - \\mathbf{b} \\rVert\\mathbf{v}\\ \u0026= \\mathbf{b} + \\lVert \\mathbf{a} - \\mathbf{b} \\rVert\\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})\\right)\\ \u0026= \\mathbf{b} + \\mathbf{a} - \\mathbf{b} \\ \u0026= \\mathbf{a} \\end{aligned} $$\nAnd the other way:\n$$ \\begin{aligned} \\text{patch}(\\mathbf{a} \\to \\mathbf{b}) \u0026= \\mathbf{a} + (\\mathbf{v}^\\top\\mathbf{b} - \\mathbf{v}^\\top\\mathbf{a})\\mathbf{v} \\ \u0026= \\mathbf{a} + (\\mathbf{v}^\\top(\\mathbf{b} - \\mathbf{a}))\\mathbf{v}\\ \u0026= \\mathbf{a} - (\\mathbf{v}^\\top(\\mathbf{a} - \\mathbf{b}))\\mathbf{v}\\ \u0026= \\mathbf{a} - (\\mathbf{a} - \\mathbf{b}) \\ \u0026= \\mathbf{b} \\end{aligned} $$\nThus, setting $\\mathbf{v} := \\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})$ perfectly swaps the two inputs.\nOkay, so DAS should learn that, right? Actually, I will show that DAS will do something weird. First, note that the training objective of DAS (at least, as Zen and I have been using in our code) is to maximise the probability of the counterfactual next-token label using a cross entropy loss. The objective is not to swap the activations!\nThese activations (at last block_output at last position) will get unembeded into next-token logits. The logit and resulting probability for each token $i$ in the vocab is calculated as thus:\n$$\\text{logit}(\\mathbf{act}, \\text{vocab}_i) = \\mathbf{act}^\\top \\text{unembed}(\\text{vocab}_i)\\ \\text{prob}(\\mathbf{act}, \\text{vocab}_i) = \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j))}$$\nDAS wants to maximise the probability of the counterfactual output, which implies maximising the logit of the counterfactual output relative to all other logits, which implies maximising the dot product of the final activation with the counterfactual output unembedding relative to all other items in the vocabulary.\nWhen we add a vector $\\mathbf{u}$ to the final activation, how does that affect a particular logit?\n$$ \\begin{aligned} \\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i) \u0026= (\\mathbf{act} + \\mathbf{u})^\\top \\text{unembed}(\\text{vocab}_i)\\ \u0026= \\mathbf{act}^\\top \\text{unembed}(\\text{vocab}_i) + \\mathbf{u}^\\top \\text{unembed}(\\text{vocab}_i)\\ \u0026= \\text{logit}(\\mathbf{act}, \\text{vocab}_i) + \\text{logit}(\\mathbf{u}, \\text{vocab}_i) \\end{aligned} $$\nAnd how about a particular probability?\n$$ \\begin{aligned} \\text{prob}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i) \u0026= \\frac{\\exp(\\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_j))}\\ \u0026= \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i) + \\text{logit}(\\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j) + \\text{logit}(\\mathbf{u}, \\text{vocab}_j))}\\ \u0026= \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i))\\exp(\\text{logit}(\\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j))\\exp(\\text{logit}(\\mathbf{u}, \\text{vocab}_j))} \\end{aligned} $$\nSince, in this secnario, the training objective for DAS is just a constrained optimisation of an added vector $\\mathbf{u}$ to the activation, it seems that DAS should learn a patch that brings the activation as close to the next token label $\\text{vocab}_\\text{target}$, which is not necessarily the perfect patch for flipping the activation. Does this happen in practice?\nExamples ","wordCount":"1657","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"2024-01-07T00:00:00Z","dateModified":"2024-01-07T00:00:00Z","author":{"@type":"Person","name":"Aryaman Arora"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/drafts/2024-01-07-illusion/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title=search><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/drafts/>Drafts</a></div><h1 class="post-title entry-hint-parent">Why DAS may fail</h1><div class=post-meta><span title='2024-01-07 00:00:00 +0000 UTC'>January 7, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1657 words&nbsp;·&nbsp;Aryaman Arora</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#toy-examples>Toy examples</a><ul><li><a href=#1d-das-on-1d-feature>1D DAS on 1D feature</a></li><li><a href=#1d-das-on-2d-feature>1D DAS on 2D feature</a></li><li><a href=#1d-das-to-activate-unrelated-feature>1D DAS to activate unrelated feature</a></li></ul></li><li><a href=#real-example>Real example</a><ul><li><a href=#examples>Examples</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=why-das-may-fail>Why DAS may fail<a hidden class=anchor aria-hidden=true href=#why-das-may-fail>#</a></h1><p>Aryaman Arora<br><em>January 7, 2024</em></p><p>Recently, <a href=https://arxiv.org/abs/2311.17030>Makelov et al. (2023)</a> claimed that <strong>distributed alignment search</strong> (DAS; <a href=https://arxiv.org/abs/2303.02536>Geiger et al., 2023</a>) can produce an <strong>interpretability illusion</strong> that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:</p><ol><li>a <em>causally disconnected direction</em> that distinguishes between the source and base but is in the nullspace of all downstream model projections, i.e. intervening along it should have no causal effect</li><li>a <em>dormant direction</em> that achieves the desired causal effect but is not activated differently by the base and source</li></ol><p>Furthermore, they provide experimental evidence of DAS producing apparently large causal effects (according to their logit-difference metric) when applied to transformer MLP modules, which disappear when the DAS vector is projected onto the MLP output rowspace.</p><p>A forthcoming response by Wu et al. (2024) shows that their definition of an interpretability illusion is flawed, even on the toy model they present to illustrate the definition&mdash;it rules out the obviously non-illusory direction as having a significant illusory component. It&rsquo;s also immediately apparent to us that the experimental setup they use in many instances is not one we would espouse for DAS: they overfit on one example for the fact-editing setup, they report FLDD and don&rsquo;t note that the IIA metric is more robust to their claimed &ldquo;illusion&rdquo;, etc.</p><p>However, I do not think this closes the matter. They were obviously wrong on many counts, but they are pointing to something concerning about DAS that neither of us really know how to explain but which we both recognise. <strong>It is experimentally true that DAS can &ldquo;overfit&rdquo;.</strong> In this brief post, I outline how this might be possible.</p><h2 id=toy-examples>Toy examples<a hidden class=anchor aria-hidden=true href=#toy-examples>#</a></h2><p>The key interpretability problem we are trying to overcome with DAS, and others are trying to overcome with methods like LEACE (<a href=https://arxiv.org/abs/2306.03819>Belrose et al., 2023</a>), is that <strong>we do not know the basis of the feature space</strong> that models operate on. We attempt to learn it by making a claim about counterfactual pairs of inputs and how they should be processed differently by the model; a method like DAS allows us to test such a hypothesis by trying to find a direction along which we can flip the model behaviour with regard to the counterfactual pair being tested.</p><p>Let&rsquo;s consider some simple scenarious about some toy feature spaces that can illustrate why 1D DAS may fail. Recall that 1D DAS learns a unit vector $\mathbf{v}$ and applies the following patch along it:</p><p>$$\mathbf{act}<em>{\text{patched}} = \mathbf{act}</em>{\text{base}} + (\mathbf{v}^\top\mathbf{act}<em>{\text{src}} - \mathbf{v}^\top\mathbf{act}</em>{\text{base}})\mathbf{v}$$</p><h3 id=1d-das-on-1d-feature>1D DAS on 1D feature<a hidden class=anchor aria-hidden=true href=#1d-das-on-1d-feature>#</a></h3><p>The ideal scenario is that the feature we are trying to find with 1D DAS is indeed a 1D direction. This will obviously work perfectly, so I won&rsquo;t walk through it.</p><h3 id=1d-das-on-2d-feature>1D DAS on 2D feature<a hidden class=anchor aria-hidden=true href=#1d-das-on-2d-feature>#</a></h3><p>Suppose that the feature we care about is actually represented as 2 orthogonal directions in activation space that take on values $\in {0, 1}$, and the model&rsquo;s output is some function of those. Can 1D DAS still learn a successful patch?</p><p>Well, it is apparent that no matter what, 1D DAS cannot entirely patch the exact values along both directions. This is because the 1D DAS edit can only move the activation along the angle of the vector $\mathbf{v}$, and a line is not sufficient to cover all possible 2D values.</p><p>E.g., given $\mathbf{act}<em>{\text{base}} = [0, 1]$, $\mathbf{act}</em>{\text{src}} = [1, 0]$, and the basis directions being the two features, here are the results of the patch for some settings of DAS&rsquo;s $\mathbf{v}$:</p><table><thead><tr><th style=text-align:center>$\mathbf{v}$</th><th style=text-align:center>$\mathbf{v}^\top\mathbf{act}_{\text{base}}$</th><th style=text-align:center>$\mathbf{v}^\top\mathbf{act}_{\text{src}}$</th><th style=text-align:center>$\mathbf{act}_{\text{patched}}$</th></tr></thead><tbody><tr><td style=text-align:center>$[0, 1]$</td><td style=text-align:center>$1$</td><td style=text-align:center>$0$</td><td style=text-align:center>$[0, 0]$</td></tr><tr><td style=text-align:center>$[1, 0]$</td><td style=text-align:center>$0$</td><td style=text-align:center>$1$</td><td style=text-align:center>$[1, 1]$</td></tr><tr><td style=text-align:center>$\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right]$</td><td style=text-align:center>$\frac{1}{\sqrt{2}}$</td><td style=text-align:center>$\frac{1}{\sqrt{2}}$</td><td style=text-align:center>$[0, 1]$</td></tr><tr><td style=text-align:center>$\left[\frac{1}{2}, \frac{\sqrt{3}}{2}\right]$</td><td style=text-align:center>$\frac{\sqrt{3}}{2}$</td><td style=text-align:center>$\frac{1}{2}$</td><td style=text-align:center>$[-0.183, 0.683]$</td></tr></tbody></table><p>Also note that our 1D DAS projects from $\mathbb{R}^2$ to $\mathbb{R}^1$, so there will be collisions between activations that are an equal angle away from $\mathbf{v}$, e.g. the third row in the table above.</p><p>Okay, so 1D DAS does can&rsquo;t learn to patch a 2D feature, which is great! We have successfully shown that illusions are impossible, and the blog post can end.</p><p>Not quite. I will show in the next toy example how illusions may be possible.</p><h3 id=1d-das-to-activate-unrelated-feature>1D DAS to activate unrelated feature<a hidden class=anchor aria-hidden=true href=#1d-das-to-activate-unrelated-feature>#</a></h3><p>Let&rsquo;s say our activation space is in $\mathbb{R}^2$. Let&rsquo;s say only the first dimension is causally relevant for our task, and all our inputs only vary along that: $[0, 0]$, $[1, 0]$. The second dimension is $0$ for all inputs.</p><p>If turning on the unrelated feature along the second dimension is somehow useful for the behaviour we want to induce, can 1D DAS activate it? Yes!</p><table><thead><tr><th style=text-align:center>$\mathbf{v}$</th><th style=text-align:center>Type</th><th style=text-align:center>Patch 1</th><th style=text-align:center>Patch 2</th></tr></thead><tbody><tr><td style=text-align:center>$[1, 0]$</td><td style=text-align:center>non-illusory</td><td style=text-align:center>$[1, 0] \to [0, 0]$</td><td style=text-align:center>$[0, 0] \to [1, 0]$</td></tr><tr><td style=text-align:center>$[0, 1]$</td><td style=text-align:center>dormant (no effect)</td><td style=text-align:center>$[1, 0] \to [1, 0]$</td><td style=text-align:center>$[0, 0] \to [0, 0]$</td></tr><tr><td style=text-align:center>$\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right]$</td><td style=text-align:center>???</td><td style=text-align:center>$[1, 0] \to [0.293, -0.707]$</td><td style=text-align:center>$[0, 0] \to [0.707, 0.707]$</td></tr></tbody></table><p>In the last row above, we successfully modified the value of the second dimension such that it depends on the value along the first dimension. This isn&rsquo;t new though; this is exactly what <a href=https://arxiv.org/abs/2311.17030>Makelov et al. (2023)</a> claimed. Their definition was wrong, but this is the phenomenon they were talking about.</p><p>Here is how I would describe this illusory behaviour as:</p><blockquote><p>Given two sets of activations $X$ and $Y$:</p><ol><li>A feature that has (approximately) one value for $X$ and (approximately) a different value for $Y$</li><li>A feature, orthogonal and thus uncorrelated to the first, that has (approximately) the same value for both $X$ and $Y$</li></ol><p>1D DAS can induce a correlation between the two features, such that a patch can modify the value of (2) based on the value of (1). This results in patched activations where (2) takes on values never encountered in the input sets $X$, $Y$.</p></blockquote><p>This is possible because the DAS vector $\mathbf{v}$ is non-orthogonal to both features. Essentially, we give up some causal effect on feature 1 in order to create a causal effect on feature 2.</p><p>Notice that we are only talking about features and not the rowspace/nullspace of downstream components. While <a href=https://arxiv.org/abs/2311.17030>Makelov et al.</a> were on the right track, I think the downstream component subspace stuff is a distraction from the core point about DAS illusion.</p><p>But is this actually relevant beyond being a cute toy example? I will show how this idea lets us predict actual illusory behaviour by DAS!</p><h2 id=real-example>Real example<a hidden class=anchor aria-hidden=true href=#real-example>#</a></h2><p>A dataset for DAS on an autogressive language model includes pairs of counterfactual inputs and associated next-token predictions, such that the model should give one output for one input in a pair and a different output for the other input in the pair.</p><p>We will focus on interventions at the <strong>last</strong> <code>block_output</code> at the <strong>last</strong> token position. We will also only consider a single pair of inputs, i.e. the &ldquo;factual recall&rdquo; setting in <a href=https://arxiv.org/abs/2311.17030>Makelov et al. (2023)</a> where DAS can overfit.</p><p>Consider the activations at the last position at the last <code>block_output</code> for the two inputs. We will call those $\mathbf{a}$ and $\mathbf{b}$. I claim that DAS can learn an vector $\mathbf{v}$ that perfectly turns $\mathbf{a}$ into $\mathbf{b}$ and vice versa.</p><hr><p><em>Proof</em>. Set $\mathbf{v} := \frac{1}{\lVert \mathbf{a} - \mathbf{b} \rVert}(\mathbf{a} - \mathbf{b})$, i.e. the unit vector along the difference between the two activations. If we run the intervention along this direction, we achieve the desired output:</p><p>$$
\begin{aligned}
\text{patch}(\mathbf{b} \to \mathbf{a}) &= \mathbf{b} + (\mathbf{v}^\top\mathbf{a} - \mathbf{v}^\top\mathbf{b})\mathbf{v} \
&= \mathbf{b} + (\mathbf{v}^\top(\mathbf{a} - \mathbf{b}))\mathbf{v}\
&= \mathbf{b} + \left(\frac{1}{\lVert \mathbf{a} - \mathbf{b} \rVert}(\mathbf{a} - \mathbf{b})^\top (\mathbf{a} - \mathbf{b})\right)\mathbf{v}\
&= \mathbf{b} + \left(\frac{1}{\lVert \mathbf{a} - \mathbf{b} \rVert} \lVert \mathbf{a} - \mathbf{b} \rVert^2\right)\mathbf{v}\
&= \mathbf{b} + \lVert \mathbf{a} - \mathbf{b} \rVert\mathbf{v}\
&= \mathbf{b} + \lVert \mathbf{a} - \mathbf{b} \rVert\left(\frac{1}{\lVert \mathbf{a} - \mathbf{b} \rVert}(\mathbf{a} - \mathbf{b})\right)\
&= \mathbf{b} + \mathbf{a} - \mathbf{b} \
&= \mathbf{a}
\end{aligned}
$$</p><p>And the other way:</p><p>$$
\begin{aligned}
\text{patch}(\mathbf{a} \to \mathbf{b}) &= \mathbf{a} + (\mathbf{v}^\top\mathbf{b} - \mathbf{v}^\top\mathbf{a})\mathbf{v} \
&= \mathbf{a} + (\mathbf{v}^\top(\mathbf{b} - \mathbf{a}))\mathbf{v}\
&= \mathbf{a} - (\mathbf{v}^\top(\mathbf{a} - \mathbf{b}))\mathbf{v}\
&= \mathbf{a} - (\mathbf{a} - \mathbf{b}) \
&= \mathbf{b}
\end{aligned}
$$</p><p>Thus, setting $\mathbf{v} := \frac{1}{\lVert \mathbf{a} - \mathbf{b} \rVert}(\mathbf{a} - \mathbf{b})$ perfectly swaps the two inputs.</p><hr><p>Okay, so DAS should learn that, right? Actually, I will show that DAS will do something weird. First, note that the training objective of DAS (at least, as Zen and I have been using in our code) is to maximise the probability of the counterfactual next-token label using a cross entropy loss. The objective is not to swap the activations!</p><p>These activations (at last <code>block_output</code> at last position) will get unembeded into next-token logits. The logit and resulting probability for each token $i$ in the vocab is calculated as thus:</p><p>$$\text{logit}(\mathbf{act}, \text{vocab}_i) = \mathbf{act}^\top \text{unembed}(\text{vocab}_i)\
\text{prob}(\mathbf{act}, \text{vocab}_i) = \frac{\exp(\text{logit}(\mathbf{act}, \text{vocab}_i))}{\sum_j \exp(\text{logit}(\mathbf{act}, \text{vocab}_j))}$$</p><p>DAS wants to maximise the probability of the counterfactual output, which implies maximising the logit of the counterfactual output relative to all other logits, which implies maximising the dot product of the final activation with the counterfactual output unembedding relative to all other items in the vocabulary.</p><p>When we add a vector $\mathbf{u}$ to the final activation, how does that affect a particular logit?</p><p>$$
\begin{aligned}
\text{logit}(\mathbf{act} + \mathbf{u}, \text{vocab}_i) &= (\mathbf{act} + \mathbf{u})^\top \text{unembed}(\text{vocab}_i)\
&= \mathbf{act}^\top \text{unembed}(\text{vocab}_i) + \mathbf{u}^\top \text{unembed}(\text{vocab}_i)\
&= \text{logit}(\mathbf{act}, \text{vocab}_i) + \text{logit}(\mathbf{u}, \text{vocab}_i)
\end{aligned}
$$</p><p>And how about a particular probability?</p><p>$$
\begin{aligned}
\text{prob}(\mathbf{act} + \mathbf{u}, \text{vocab}_i) &= \frac{\exp(\text{logit}(\mathbf{act} + \mathbf{u}, \text{vocab}_i))}{\sum_j \exp(\text{logit}(\mathbf{act} + \mathbf{u}, \text{vocab}_j))}\
&= \frac{\exp(\text{logit}(\mathbf{act}, \text{vocab}_i) + \text{logit}(\mathbf{u}, \text{vocab}_i))}{\sum_j \exp(\text{logit}(\mathbf{act}, \text{vocab}_j) + \text{logit}(\mathbf{u}, \text{vocab}_j))}\
&= \frac{\exp(\text{logit}(\mathbf{act}, \text{vocab}_i))\exp(\text{logit}(\mathbf{u}, \text{vocab}_i))}{\sum_j \exp(\text{logit}(\mathbf{act}, \text{vocab}_j))\exp(\text{logit}(\mathbf{u}, \text{vocab}_j))}
\end{aligned}
$$</p><p>Since, in this secnario, the training objective for DAS is just a constrained optimisation of an added vector $\mathbf{u}$ to the activation, it seems that DAS should learn a patch that brings the activation as close to the next token label $\text{vocab}_\text{target}$, which is <strong>not</strong> necessarily the perfect patch for flipping the activation. Does this happen in practice?</p><h3 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h3></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>