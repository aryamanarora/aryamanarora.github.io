<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Interventional interpretability I: Causal abstraction | Aryaman Arora</title>
<meta name=keywords content="NLP,causality"><meta name=description content="Overview of the new intervention-based methods for interpretability."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/drafts/2023-12-14-intervention/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/drafts/2023-12-14-intervention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="Interventional interpretability I: Causal abstraction"><meta property="og:description" content="Overview of the new intervention-based methods for interpretability."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/drafts/2023-12-14-intervention/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content="drafts"><meta property="article:published_time" content="2023-12-14T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-14T00:00:00+00:00"><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="Interventional interpretability I: Causal abstraction"><meta name=twitter:description content="Overview of the new intervention-based methods for interpretability."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Drafts","item":"http://localhost:1313/drafts/"},{"@type":"ListItem","position":2,"name":"Interventional interpretability I: Causal abstraction","item":"http://localhost:1313/drafts/2023-12-14-intervention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Interventional interpretability I: Causal abstraction","name":"Interventional interpretability I: Causal abstraction","description":"Overview of the new intervention-based methods for interpretability.","keywords":["NLP","causality"],"articleBody":"There is a new kind of interpretability being done these days: mechanistic interpretability. It is sometimes unclear whether mechanistic interpretability really is a departure from the methods that comprised “interpretability”, but if you are charitable, what sets it apart is that it is aims to “reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program” (Olah, 2022). This is a far more ambitious goal than previous research programmes in the realm of interpretability, whose goal is adequately summarised as “to provide explanations in understandable terms to a human” (Zhang et al., 2021). Mechanistic interpretability is model-first; it may not be a priori possible to develop faithful explanations for neural network behaviours based on what we currently know, and mechanistic approaches are willing to do the hard work of creating a new basis for understanding neural networks from the ground up.\nThis blog post is a comprehensive overview of one class of genuinely novel methods in (mechanistic) interpretability, which I call interventional interpretability. These approaches all share the broad idea of intervening on model internals to see how they affect model behaviour. Interventional experiments are similar in spirit to how biologists experiment on living organisms (except with finer-grained control and perfect observational accuracy).1\nThe key intuition behind interventional interpretability is that when we mess with things and they break, this tells us that the thing we messed with is important. Causal abstraction provides the framework to map our “mental models” about such processes to low-level implementation details of the thing we are messing with.\n[!warning]\nAlmost nothing in this blog post is original and I don’t claim to have invented this stuff—think of it as my notes on existing papers that I have read. If you found it useful, feel free to cite it, but also make sure you cite the original works I reference in each section. This structure of this post is inspired by Lilian Weng’s blog.\n[!info] Sources\n“Causal Abstraction for Faithful Model Interpretation” (Geiger et al., 2023) “Causal Abstractions of Neural Networks” (Geiger et al., 2021) A causal view of neural networks The mathematical basis for understanding what interventions do to a neural network is often overlooked. A view of neural networks as causal models is, in my opinion, best laid out by Geiger et al. (2023). This section presents the definitions from that paper as a foundation for how we should think about interventional methods in interpretability. I will recast all existing interventional interpretability methods to fit in this framework.\nFirst, let’s define variables, which will constitute nodes in the computational graph which our neural network implements.\nDefinition 1: A variable $X$ has a range $\\mathsf{Val}(X)$ of possible values. In a set of variables $\\mathbf{V}$, no two variables can share the same value. Given a subset of variables $\\mathbf{X} \\subseteq \\mathbf{V}$, the values $\\mathbf{x} \\in \\mathsf{Val}(\\mathbf{X})$ is a partial setting. If $\\mathbf{v} \\in \\mathsf{Val}(\\mathbf{V})$ (i.e. all variables are set to something), then we have a total setting.\nNow, we define a causal model on a set of variables (nodes) and a set of structural functions (edges).\nDefinition 2: A causal model is a pair $\\mathcal{M} = (\\mathbf{V}, \\mathcal{F})$, where:\n$\\mathbf{V}$ is a set of variables, where each $X \\in \\mathbf{V}$ has a range $\\mathsf{Val}(X)$ of possible values. $\\mathcal{F} = \\{f_V\\}_{V \\in \\mathbf{V}}$ is a set of structural functions, with each $f_V: \\mathsf{Val}(\\mathbf{V}) \\to \\mathsf{Val}(V)$ assigns a value to variable $V \\in \\mathbf{V}$ as a function of the values of all the variables. To induce causal structure in this model, we note that, while each structural function $f_V$ takes in a setting of all the variables $\\mathbf{V}$ as input, its output may not necessarily depend on all of those variables. We say we can define a causal order between two variables $Y$ and $X$ where $Y \\prec X$ ($Y$ is a parent of $X$), if there is a setting $\\mathbf{w}$ of the variables $\\mathbf{W} = \\mathbf{V} \\setminus \\{X, Y\\}$ and settings $y, y’$ of $Y$ such that $f_X(\\mathbf{w}, y) \\neq f_X(\\mathbf{w}, y’)$, i.e. $Y$ has a direct effect on $X$.\nFinally, there are some special subsets of variables that we will need to refer to. One is input variables $\\mathbf{X}^{\\text{In}}_\\mathcal{M}$, which depend on no other variables (i.e. they have no parents). The other is output variables $\\mathbf{X}^{\\text{Out}}_\\mathcal{M}$, which no other variables depend on (i.e. they have no children). All remaining variables $\\mathbf{V}_{\\mathcal{M}} \\setminus (\\mathbf{X}_{\\mathcal{M}}^{\\text{In}} \\cup \\mathbf{X}_{\\mathcal{M}}^{\\text{Out}})$ are intermediate variables.\nOnce we have the variable ordering, we can easily make the computational graph that corresponds to this causal model. The nodes are the variables. The directed edges are the ordering relations that we found based on the structural functions.\nAlso note that for the purposes of this post, we only need to consider acyclic causal models.\nExample: Causal model Let’s consider a simple neural network that computes the identity function, and define a causal model where the variables are individual neurons. (We can define the model at a coarser-grained level too, e.g. layers, but we will keep it simple for this example). This example is taken from Makelov et al. (2023) because it will come up again later.\nThe network takes in an input $x \\in \\mathbb{R}$, computes a hidden state $\\mathbf{h} = x\\mathbf{w}_1$, and the output $y = \\mathbf{w}_2^T\\mathbf{h}$. We define the weights as:\n$$ \\begin{aligned} \\mathbf{w}_1 \u0026= \\begin{bmatrix} 1 \u0026 0 \u0026 1\\end{bmatrix} \\ \\mathbf{w}_2 \u0026= \\begin{bmatrix} 0 \u0026 2 \u0026 1\\end{bmatrix} \\end{aligned} $$\nThus we have the variables and functions:\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ — $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x$ $\\mathbf{h}_2$ $f_{\\mathbf{h}_2}(x) = 0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x$ $y$ $f_y(\\mathbf{h}_1, \\mathbf{h}_2, \\mathbf{h}_3) = 0 \\cdot \\mathbf{h}_1 + 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3$ But note that our definition for the causal order between variables in a model actually isn’t satisfied by all of these functions. We have to get rid of dead connections since they don’t mediate any causal effect:\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ — $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3$ Thus, we get the ordering relations $x \\prec \\mathbf{h}_1$, $x \\prec \\mathbf{h}_3$, $\\mathbf{h}_2 \\prec y$, $\\mathbf{h}_3 \\prec y$. Per our earlier remarks, the input variable is $x$ and the output variable is $y$, and the rest are intermediate variables. Finally, we can make the computational graph representing our causal model:\nDefining Interventions Okay, now we have our causal model $\\mathcal{M}$. Let’s mess with it.\nFirst, we will define a partial setting of variables, which permits us to “override” the values of arbitrary subsets of variables.\nDefinition 3: Given a partial setting $\\mathbf{u}$ for a set of variables $\\mathbf{U} \\supseteq \\mathbf{X}$, we define $\\mathsf{Proj}(\\mathbf{u}, \\mathbf{X})$ to be the restriction of $\\mathbf{u}$ to values of the variables in the set $\\mathbf{X}$. Also, given a partial setting $\\mathbf{x}$ and set $\\mathbf{U} \\supseteq \\mathbf{X}$: $$\\mathsf{Proj}^{-1}(\\mathbf{x}, \\mathbf{U}) = \\{\\mathbf{u} \\in \\mathsf{Val}(\\mathbf{U}) : \\mathsf{Proj}(\\mathbf{u}, \\mathbf{X}) = \\mathbf{x}\\}$$ Basically, for all possible settings of the variables in $\\mathbf{U}$, this fixes the partial setting $\\mathbf{x}$ on $\\mathbf{X}$.\nNow, we use this to define an intervention!\nDefinition 4: Given a causal model $\\mathcal{M}$, an intervention is a partial setting of variables $\\mathbf{i} \\in \\mathsf{Val}(\\mathbf{I})$ for a subset $\\mathbf{I} \\subseteq \\mathbf{V}$. We define the intervened model $\\mathcal{M}_{\\mathbf{i}}$ as the same as $\\mathcal{M}$, except we replace $f_X$ with $\\mathbf{v} \\mapsto \\mathsf{Proj}(\\mathbf{x}, X)$ for each variable $X \\in \\mathbf{I}$.\nIn non-mathematical English, an intervention is just a partial setting of variables that we enforce on the causal model. We do this by “overriding” the structural functions associated with those variables to output the value we gave to that variable in our partial setting. Note that this severs the overriden variables from their parents.\nFinally, we define what a solution to a causal model is so that we can measure the effect of an intervention.\nDefinition 5: Given a causal model $\\mathcal{M} = \\langle \\mathbf{V}, \\mathcal{F} \\rangle$, the set of solutions called $\\textsf{Solve}(\\mathcal{M})$ is the set of all $\\mathbf{v} \\in \\mathsf{Val}(\\mathbf{V})$ such that all the structural equations $v = f_V(\\mathbf{v})$ are satisfied for each variable $v \\in \\mathbf{v}$. In an acyclic model, any intervention $\\mathbf{i}$ results in a model with a single solution and we use the notation $\\mathsf{Solve}(\\mathcal{M}_{\\mathbf{i}})$ to refer to that singleton solution as well as the one-member set of solutions.\nExample: Intervention We will now (1) run our example causal model $\\mathcal{M}$ without any interventions, and (2) intervene on it at one hidden neuron to see what happens.\nFirst, let’s compute the total setting of variables for input $x := 5$. Technically, this is an intervention where we do $\\mathbf{i} := \\{x: 5\\}$ and compute $\\mathsf{Solve}(\\mathcal{M}_\\mathbf{i})$.\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ $5$ $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x = 5$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x = 5$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3 = 0 + 5 = 5$ Wow, the output number is also $5$. Now let’s intervene on $\\mathbf{h}_3$ too and see what happens. We define $\\mathbf{i} := \\{x: 5;~\\mathbf{h}_3: 42\\}$ and compute $\\mathsf{Solve}(\\mathcal{M}_\\mathbf{i})$.\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ $5$ $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x = 5$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $42$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3 = 0 + 42 = 42$ Woah, we change the output! If you look at our computation graph now, notice that we severed the edge between $x$ and $\\mathbf{h}_3$ when performing this intervention:\nSince our intervention did affect the output $y$, we also found a causally-relevant pathway in the graph! Modifying $\\mathbf{h}_3$ directly modifies $y$. This obviously taught us nothing new about this model because we already understand it completely. The power of interventional interpretability is that it allows us to test hypotheses about intermediate variables in arbitrarily complex models—such as multi-billion-parameter transformers.\nInterchange intervention Definition 6: Consider a neural network $\\mathcal{N}$, with input and output variables $\\mathbf{X}_{\\mathcal{L}}^{\\text{In}}, \\mathbf{X}_{\\mathcal{L}}^{\\text{Out}} \\subseteq \\mathbf{V}_{\\mathcal{L}}$, disjoint subsets of intermediate variables $\\mathbf{X}_{\\mathcal{L}}^1, \\ldots, \\mathbf{X}_{\\mathcal{L}}^k \\subseteq \\mathbf{V}_{\\mathcal{L}} \\setminus (\\mathbf{X}_{\\mathcal{L}}^{\\text{In}} \\cup \\mathbf{X}_{\\mathcal{L}}^{\\text{Out}})$, and base input $\\mathbf{b}$ and source inputs $\\mathbf{s}_1, \\ldots, \\mathbf{s}_k \\in \\mathbf{X}_{\\mathcal{L}}^{\\text{In}}$.\nWe define an interchange intervention to be the partial setting that sets the input variables to the base input $\\mathbf{b}$ and the intermediate variables $\\mathbf{X}_{\\mathcal{L}}^i$ to the values they would take on if the source input $\\mathbf{s}_i$ were provided to $\\mathcal{N}$: $$ \\begin{aligned} \\mathsf{IntInv}\u0026(\\mathcal{N}, \\mathbf{b}, \\langle \\mathbf{s}_1, \\ldots, \\mathbf{s}_k \\rangle, \\langle \\mathbf{X}_{\\mathcal{L}}^1, \\ldots, \\mathbf{X}_{\\mathcal{L}}^k \\rangle)\\ \u0026:= \\mathbf{b} \\cup \\mathsf{Proj}(\\mathsf{Solve}(\\mathcal{N}_{\\mathbf{s}_1}), \\mathbf{X}_{\\mathcal{L}}^1) \\cup \\ldots \\cup \\mathsf{Proj}(\\mathsf{Solve}(\\mathcal{N}_{\\mathbf{s}_k}), \\mathbf{X}_{\\mathcal{L}}^k) \\end{aligned} $$\nThinking about transformers [!info] Sources\n“A Mathematical Framework for Transformer Circuits” (Elhage et al., 2019) “Softmax Linear Units” (Elhage et al., 2022) “Toy Models of Superposition” (Elhage et al., 2022) In the example I presented, we mapped our neural network into a causal model at the neuron level. But given a transformer language model (the setting which we actually care about) that is potentially very large, is this the right level of granularity to be performing interventions at?\nLet’s summarise our understanding of the transformer architecture first, which I believe has only seriously been tackled by Elhage et al. (2019). They proposed some new conceptual ways that we should think about the transformer (when doing interpretability at least):\nThere is a single communication channel spanning all layers called the residual stream. Each attention layer and MLP layer of the transformer takes linear transformers of the residual stream as input, and additively contributes its output back to the residual stream. (In the traditional view, the residual stream is the skip connections between layers, which people sometimes forget about!) The residual stream does not have a privileged basis. “we could rotate it by rotating all the matrices interacting with it, without changing model behavior”. Modules (attention heads/MLP layers) communicate via the residual stream: if one layer wants to pass information to another layer, it must write to some subspace of the residual stream that the later layer will read from. This makes the residual stream an information bottleneck. Attention is the only mechanism in the transformer that can move information between token positions. Attention heads operate entirely in parallel, and thus their inputs/outputs with respect to the residual stream can be examined independently. Each attention head is composed of two independent circuits: the QK-circuit which computes attention probabilities, and the OV-circuit which actually applies the transformation of attention outputs to get attention outputs. The QK-circuit is basically a filter on what the OV-circuit computes. While all of this is important, the key point I wanted to introduce here is that the residual stream does not have a privileged basis. Since reads and writes with respect to the residual stream are linear transformations (e.g. QKV matrices in attention, the input transform in an MLP), the ‘features’ in the representation space that we want to modify with our interventions may not be neuron-aligned. Thus, the best level of granularity with which to represent nodes in our causal models may not be a single neuron, but instead e.g. a whole transformer block and the residual stream, or each attention head and MLP and the residual stream.\nThis poses no problems for our definition of a causal model; we can simply define a structural function as having vector-valued inputs and outputs.\nChris Olah (2023) says: “The beauty of deep learning and scale is a kind of biological beauty.” Another one I have heard and agree with is from Buck Shlegeris, who compares modern deep learning to alchemy; mechanistic interpretability researchers are striving towards turning this field into chemistry. ↩︎\n","wordCount":"2240","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"2023-12-14T00:00:00Z","dateModified":"2023-12-14T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/drafts/2023-12-14-intervention/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title=search><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/drafts/>Drafts</a></div><h1 class="post-title entry-hint-parent">Interventional interpretability I: Causal abstraction</h1><div class=post-description>Overview of the new intervention-based methods for interpretability.</div><div class=post-meta><span title='2023-12-14 00:00:00 +0000 UTC'>December 14, 2023</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2240 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#a-causal-view-of-neural-networks>A causal view of neural networks</a><ul><li><a href=#example-causal-model>Example: Causal model</a></li></ul></li><li><a href=#defining-interventions>Defining Interventions</a><ul><li><a href=#example-intervention>Example: Intervention</a></li></ul></li><li><a href=#interchange-intervention>Interchange intervention</a></li><li><a href=#thinking-about-transformers>Thinking about transformers</a></li></ul></nav></div></details></div><div class=post-content><p>There is a new kind of interpretability being done these days: <strong>mechanistic interpretability</strong>. It is sometimes unclear whether mechanistic interpretability really is a departure from the methods that comprised &ldquo;interpretability&rdquo;, but if you are charitable, what sets it apart is that it is aims to &ldquo;<strong>reverse engineer neural networks</strong>, similar to how one might reverse engineer a compiled binary computer program&rdquo; (<a href=https://transformer-circuits.pub/2022/mech-interp-essay/index.html>Olah, 2022</a>). This is a far more ambitious goal than previous research programmes in the realm of interpretability, whose goal is adequately summarised as &ldquo;to provide explanations in understandable terms to a human&rdquo; (<a href=https://arxiv.org/abs/2012.14261>Zhang et al., 2021</a>). Mechanistic interpretability is model-first; it may not be <em>a priori</em> possible to develop faithful explanations for neural network behaviours based on what we currently know, and mechanistic approaches are willing to do the hard work of creating a new basis for understanding neural networks from the ground up.</p><p>This blog post is a comprehensive overview of one class of genuinely novel methods in (mechanistic) interpretability, which I call <strong>interventional interpretability</strong>. These approaches all share the broad idea of intervening on model internals to see how they affect model behaviour. Interventional experiments are similar in spirit to how biologists experiment on living organisms (except with finer-grained control and perfect observational accuracy).<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>The key intuition behind interventional interpretability is that when we mess with things and they break, this tells us that the thing we messed with is important. Causal abstraction provides the framework to map our &ldquo;mental models&rdquo; about such processes to low-level implementation details of the thing we are messing with.</p><blockquote><p>[!warning]</p><p>Almost nothing in this blog post is original and I don&rsquo;t claim to have invented this stuff&mdash;think of it as my notes on existing papers that I have read. If you found it useful, feel free to cite it, but also make sure you cite the original works I reference in each section. This structure of this post is inspired by <a href=https://lilianweng.github.io/>Lilian Weng&rsquo;s blog</a>.</p></blockquote><blockquote><p>[!info] Sources</p><ul><li><a href=https://arxiv.org/abs/2301.04709>&ldquo;Causal Abstraction for Faithful Model Interpretation&rdquo; (Geiger et al., 2023)</a></li><li><a href=https://arxiv.org/abs/2106.02997>&ldquo;Causal Abstractions of Neural Networks&rdquo; (Geiger et al., 2021)</a></li></ul></blockquote><h2 id=a-causal-view-of-neural-networks>A causal view of neural networks<a hidden class=anchor aria-hidden=true href=#a-causal-view-of-neural-networks>#</a></h2><p>The mathematical basis for understanding what interventions do to a neural network is often overlooked. A view of neural networks as causal models is, in my opinion, best laid out by <a href=https://arxiv.org/abs/2301.04709>Geiger et al. (2023)</a>. This section presents the definitions from that paper as a foundation for how we should think about interventional methods in interpretability. I will recast all existing interventional interpretability methods to fit in this framework.</p><p>First, let&rsquo;s define variables, which will constitute nodes in the computational graph which our neural network implements.</p><blockquote><p><strong>Definition 1</strong>: A <strong>variable</strong> $X$ has a range $\mathsf{Val}(X)$ of possible values. In a set of variables $\mathbf{V}$, no two variables can share the same value. Given a subset of variables $\mathbf{X} \subseteq \mathbf{V}$, the values $\mathbf{x} \in \mathsf{Val}(\mathbf{X})$ is a <em>partial setting</em>. If $\mathbf{v} \in \mathsf{Val}(\mathbf{V})$ (i.e. all variables are set to something), then we have a <em>total setting</em>.</p></blockquote><p>Now, we define a causal model on a set of variables (nodes) and a set of structural functions (edges).</p><blockquote><p><strong>Definition 2</strong>: A <strong>causal model</strong> is a pair $\mathcal{M} = (\mathbf{V}, \mathcal{F})$, where:</p><ul><li>$\mathbf{V}$ is a set of <em>variables</em>, where each $X \in \mathbf{V}$ has a range $\mathsf{Val}(X)$ of possible values.</li><li>$\mathcal{F} = \{f_V\}_{V \in \mathbf{V}}$ is a set of <em>structural functions</em>, with each $f_V: \mathsf{Val}(\mathbf{V}) \to \mathsf{Val}(V)$ assigns a value to variable $V \in \mathbf{V}$ as a function of the values of all the variables.</li></ul></blockquote><p>To induce causal structure in this model, we note that, while each structural function $f_V$ takes in a setting of all the variables $\mathbf{V}$ as input, its output may not necessarily depend on all of those variables. We say we can define a <strong>causal order</strong> between two variables $Y$ and $X$ where $Y \prec X$ ($Y$ is a parent of $X$), if there is a setting $\mathbf{w}$ of the variables $\mathbf{W} = \mathbf{V} \setminus \{X, Y\}$ and settings $y, y&rsquo;$ of $Y$ such that $f_X(\mathbf{w}, y) \neq f_X(\mathbf{w}, y&rsquo;)$, i.e. $Y$ has a direct effect on $X$.</p><p>Finally, there are some special subsets of variables that we will need to refer to. One is <em>input variables</em> $\mathbf{X}^{\text{In}}_\mathcal{M}$, which depend on no other variables (i.e. they have no parents). The other is <em>output variables</em> $\mathbf{X}^{\text{Out}}_\mathcal{M}$, which no other variables depend on (i.e. they have no children). All remaining variables $\mathbf{V}_{\mathcal{M}} \setminus (\mathbf{X}_{\mathcal{M}}^{\text{In}} \cup \mathbf{X}_{\mathcal{M}}^{\text{Out}})$ are <em>intermediate variables</em>.</p><p>Once we have the variable ordering, we can easily make the computational graph that corresponds to this causal model. The nodes are the variables. The directed edges are the ordering relations that we found based on the structural functions.</p><p>Also note that for the purposes of this post, we only need to consider <strong>acyclic</strong> causal models.</p><h3 id=example-causal-model>Example: Causal model<a hidden class=anchor aria-hidden=true href=#example-causal-model>#</a></h3><p>Let&rsquo;s consider a simple neural network that computes the identity function, and define a causal model where the variables are individual neurons. (We can define the model at a coarser-grained level too, e.g. layers, but we will keep it simple for this example). This example is taken from <a href=https://arxiv.org/abs/2311.17030>Makelov et al. (2023)</a> because it will come up again later.</p><p>The network takes in an input $x \in \mathbb{R}$, computes a hidden state $\mathbf{h} = x\mathbf{w}_1$, and the output $y = \mathbf{w}_2^T\mathbf{h}$. We define the weights as:</p><p>$$
\begin{aligned}
\mathbf{w}_1 &= \begin{bmatrix} 1 & 0 & 1\end{bmatrix} \
\mathbf{w}_2 &= \begin{bmatrix} 0 & 2 & 1\end{bmatrix}
\end{aligned}
$$</p><p>Thus we have the variables and functions:</p><table><thead><tr><th style=text-align:left>Variable: $V \in \mathbf{V}$</th><th style=text-align:left>Function: $f_V \in \mathcal{F}$</th></tr></thead><tbody><tr><td style=text-align:left>$x$</td><td style=text-align:left>&mdash;</td></tr><tr><td style=text-align:left>$\mathbf{h}_1$</td><td style=text-align:left>$f_{\mathbf{h}_1}(x) = x$</td></tr><tr><td style=text-align:left>$\mathbf{h}_2$</td><td style=text-align:left>$f_{\mathbf{h}_2}(x) = 0$</td></tr><tr><td style=text-align:left>$\mathbf{h}_3$</td><td style=text-align:left>$f_{\mathbf{h}_3}(x) = x$</td></tr><tr><td style=text-align:left>$y$</td><td style=text-align:left>$f_y(\mathbf{h}_1, \mathbf{h}_2, \mathbf{h}_3) = 0 \cdot \mathbf{h}_1 + 2 \cdot \mathbf{h}_2 + \mathbf{h}_3$</td></tr></tbody></table><p>But note that our definition for the causal order between variables in a model actually isn&rsquo;t satisfied by all of these functions. We have to get rid of dead connections since they don&rsquo;t mediate any causal effect:</p><table><thead><tr><th style=text-align:left>Variable: $V \in \mathbf{V}$</th><th style=text-align:left>Function: $f_V \in \mathcal{F}$</th></tr></thead><tbody><tr><td style=text-align:left>$x$</td><td style=text-align:left>&mdash;</td></tr><tr><td style=text-align:left>$\mathbf{h}_1$</td><td style=text-align:left>$f_{\mathbf{h}_1}(x) = x$</td></tr><tr><td style=text-align:left>$\mathbf{h}_2$</td><td style=text-align:left>$0$</td></tr><tr><td style=text-align:left>$\mathbf{h}_3$</td><td style=text-align:left>$f_{\mathbf{h}_3}(x) = x$</td></tr><tr><td style=text-align:left>$y$</td><td style=text-align:left>$f_y(\mathbf{h}_2, \mathbf{h}_3) = 2 \cdot \mathbf{h}_2 + \mathbf{h}_3$</td></tr></tbody></table><p>Thus, we get the ordering relations $x \prec \mathbf{h}_1$, $x \prec \mathbf{h}_3$, $\mathbf{h}_2 \prec y$, $\mathbf{h}_3 \prec y$. Per our earlier remarks, the input variable is $x$ and the output variable is $y$, and the rest are intermediate variables. Finally, we can make the computational graph representing our causal model:</p><p><img loading=lazy src=/img/makelov_model.png alt title="Figure 1: The computation graph for the neural network in this example, taken from Makelov et al. (2023)."></p><h2 id=defining-interventions>Defining Interventions<a hidden class=anchor aria-hidden=true href=#defining-interventions>#</a></h2><p>Okay, now we have our causal model $\mathcal{M}$. Let&rsquo;s mess with it.</p><p>First, we will define a <em>partial setting</em> of variables, which permits us to &ldquo;override&rdquo; the values of arbitrary subsets of variables.</p><blockquote><p><strong>Definition 3</strong>: Given a partial setting $\mathbf{u}$ for a set of variables $\mathbf{U} \supseteq \mathbf{X}$, we define $\mathsf{Proj}(\mathbf{u}, \mathbf{X})$ to be the restriction of $\mathbf{u}$ to values of the variables in the set $\mathbf{X}$. Also, given a partial setting $\mathbf{x}$ and set $\mathbf{U} \supseteq \mathbf{X}$:
$$\mathsf{Proj}^{-1}(\mathbf{x}, \mathbf{U}) = \{\mathbf{u} \in \mathsf{Val}(\mathbf{U}) : \mathsf{Proj}(\mathbf{u}, \mathbf{X}) = \mathbf{x}\}$$
Basically, for all possible settings of the variables in $\mathbf{U}$, this fixes the partial setting $\mathbf{x}$ on $\mathbf{X}$.</p></blockquote><p>Now, we use this to define an intervention!</p><blockquote><p><strong>Definition 4</strong>: Given a causal model $\mathcal{M}$, an <strong>intervention</strong> is a partial setting of variables $\mathbf{i} \in \mathsf{Val}(\mathbf{I})$ for a subset $\mathbf{I} \subseteq \mathbf{V}$. We define the intervened model $\mathcal{M}_{\mathbf{i}}$ as the same as $\mathcal{M}$, except we replace $f_X$ with $\mathbf{v} \mapsto \mathsf{Proj}(\mathbf{x}, X)$ for each variable $X \in \mathbf{I}$.</p></blockquote><p>In non-mathematical English, an intervention is just a partial setting of variables that we enforce on the causal model. We do this by &ldquo;overriding&rdquo; the structural functions associated with those variables to output the value we gave to that variable in our partial setting. Note that this severs the overriden variables from their parents.</p><p>Finally, we define what a solution to a causal model is so that we can measure the effect of an intervention.</p><blockquote><p><strong>Definition 5</strong>: Given a causal model $\mathcal{M} = \langle \mathbf{V}, \mathcal{F} \rangle$, the <strong>set of solutions</strong> called $\textsf{Solve}(\mathcal{M})$ is the set of all $\mathbf{v} \in \mathsf{Val}(\mathbf{V})$ such that all the structural equations $v = f_V(\mathbf{v})$ are satisfied for each variable $v \in \mathbf{v}$. In an acyclic model, any intervention $\mathbf{i}$ results in a model with a single solution and we use the notation $\mathsf{Solve}(\mathcal{M}_{\mathbf{i}})$ to refer to that singleton solution as well as the one-member set of solutions.</p></blockquote><h3 id=example-intervention>Example: Intervention<a hidden class=anchor aria-hidden=true href=#example-intervention>#</a></h3><p>We will now (1) run our example causal model $\mathcal{M}$ without any interventions, and (2) intervene on it at one hidden neuron to see what happens.</p><p>First, let&rsquo;s compute the total setting of variables for input $x := 5$. Technically, this <em>is</em> an intervention where we do $\mathbf{i} := \{x: 5\}$ and compute $\mathsf{Solve}(\mathcal{M}_\mathbf{i})$.</p><table><thead><tr><th style=text-align:left>Variable: $V \in \mathbf{V}$</th><th style=text-align:left>Function: $f_V \in \mathcal{F}$</th></tr></thead><tbody><tr><td style=text-align:left>$x$</td><td style=text-align:left>$5$</td></tr><tr><td style=text-align:left>$\mathbf{h}_1$</td><td style=text-align:left>$f_{\mathbf{h}_1}(x) = x = 5$</td></tr><tr><td style=text-align:left>$\mathbf{h}_2$</td><td style=text-align:left>$0$</td></tr><tr><td style=text-align:left>$\mathbf{h}_3$</td><td style=text-align:left>$f_{\mathbf{h}_3}(x) = x = 5$</td></tr><tr><td style=text-align:left>$y$</td><td style=text-align:left>$f_y(\mathbf{h}_2, \mathbf{h}_3) = 2 \cdot \mathbf{h}_2 + \mathbf{h}_3 = 0 + 5 = 5$</td></tr></tbody></table><p><img loading=lazy src=/img/makelov_model2.png alt title="Figure 2: Our computation graph with input x = 5."></p><p>Wow, the output number is also $5$. Now let&rsquo;s intervene on $\mathbf{h}_3$ too and see what happens. We define $\mathbf{i} := \{x: 5;~\mathbf{h}_3: 42\}$ and compute $\mathsf{Solve}(\mathcal{M}_\mathbf{i})$.</p><table><thead><tr><th style=text-align:left>Variable: $V \in \mathbf{V}$</th><th style=text-align:left>Function: $f_V \in \mathcal{F}$</th></tr></thead><tbody><tr><td style=text-align:left>$x$</td><td style=text-align:left>$5$</td></tr><tr><td style=text-align:left>$\mathbf{h}_1$</td><td style=text-align:left>$f_{\mathbf{h}_1}(x) = x = 5$</td></tr><tr><td style=text-align:left>$\mathbf{h}_2$</td><td style=text-align:left>$0$</td></tr><tr><td style=text-align:left>$\mathbf{h}_3$</td><td style=text-align:left>$42$</td></tr><tr><td style=text-align:left>$y$</td><td style=text-align:left>$f_y(\mathbf{h}_2, \mathbf{h}_3) = 2 \cdot \mathbf{h}_2 + \mathbf{h}_3 = 0 + 42 = 42$</td></tr></tbody></table><p>Woah, we change the output! If you look at our computation graph now, notice that we severed the edge between $x$ and $\mathbf{h}_3$ when performing this intervention:</p><p><img loading=lazy src=/img/makelov_model3.png alt title="Figure 3: Oops I ruined the causal model."></p><p>Since our intervention did affect the output $y$, we also found a causally-relevant pathway in the graph! Modifying $\mathbf{h}_3$ directly modifies $y$. This obviously taught us nothing new about this model because we already understand it completely. The power of interventional interpretability is that it allows us to test hypotheses about intermediate variables in arbitrarily complex models&mdash;such as multi-billion-parameter transformers.</p><h2 id=interchange-intervention>Interchange intervention<a hidden class=anchor aria-hidden=true href=#interchange-intervention>#</a></h2><blockquote><p><strong>Definition 6</strong>: Consider a neural network $\mathcal{N}$, with input and output variables $\mathbf{X}_{\mathcal{L}}^{\text{In}}, \mathbf{X}_{\mathcal{L}}^{\text{Out}} \subseteq \mathbf{V}_{\mathcal{L}}$, disjoint subsets of intermediate variables $\mathbf{X}_{\mathcal{L}}^1, \ldots, \mathbf{X}_{\mathcal{L}}^k \subseteq \mathbf{V}_{\mathcal{L}} \setminus (\mathbf{X}_{\mathcal{L}}^{\text{In}} \cup \mathbf{X}_{\mathcal{L}}^{\text{Out}})$, and <em>base</em> input $\mathbf{b}$ and <em>source</em> inputs $\mathbf{s}_1, \ldots, \mathbf{s}_k \in \mathbf{X}_{\mathcal{L}}^{\text{In}}$.</p><p>We define an <strong>interchange intervention</strong> to be the partial setting that sets the input variables to the base input $\mathbf{b}$ and the intermediate variables $\mathbf{X}_{\mathcal{L}}^i$ to the values they would take on if the source input $\mathbf{s}_i$ were provided to $\mathcal{N}$:
$$
\begin{aligned}
\mathsf{IntInv}&(\mathcal{N}, \mathbf{b}, \langle \mathbf{s}_1, \ldots, \mathbf{s}_k \rangle, \langle \mathbf{X}_{\mathcal{L}}^1, \ldots, \mathbf{X}_{\mathcal{L}}^k \rangle)\
&:= \mathbf{b} \cup \mathsf{Proj}(\mathsf{Solve}(\mathcal{N}_{\mathbf{s}_1}), \mathbf{X}_{\mathcal{L}}^1) \cup \ldots \cup \mathsf{Proj}(\mathsf{Solve}(\mathcal{N}_{\mathbf{s}_k}), \mathbf{X}_{\mathcal{L}}^k)
\end{aligned}
$$</p></blockquote><h2 id=thinking-about-transformers>Thinking about transformers<a hidden class=anchor aria-hidden=true href=#thinking-about-transformers>#</a></h2><blockquote><p>[!info] Sources</p><ul><li><a href=https://transformer-circuits.pub/2021/framework/index.html>&ldquo;A Mathematical Framework for Transformer Circuits&rdquo; (Elhage et al., 2019)</a></li><li><a href=https://transformer-circuits.pub/2022/solu/index.html>&ldquo;Softmax Linear Units&rdquo; (Elhage et al., 2022)</a></li><li><a href=https://transformer-circuits.pub/2022/toy_model/index.html>&ldquo;Toy Models of Superposition&rdquo; (Elhage et al., 2022)</a></li></ul></blockquote><p>In the example I presented, we mapped our neural network into a causal model at the neuron level. But given a transformer language model (the setting which we actually care about) that is potentially very large, is this the right level of granularity to be performing interventions at?</p><p>Let&rsquo;s summarise our understanding of the transformer architecture first, which I believe has only seriously been tackled by <a href=https://transformer-circuits.pub/2021/framework/index.html>Elhage et al. (2019)</a>. They proposed some new conceptual ways that we should think about the transformer (when doing interpretability at least):</p><ul><li>There is a single communication channel spanning all layers called the <strong>residual stream</strong>. Each attention layer and MLP layer of the transformer takes linear transformers of the residual stream as input, and additively contributes its output back to the residual stream. (In the traditional view, the residual stream is the skip connections between layers, which people sometimes forget about!)</li><li>The residual stream <strong>does not have a privileged basis</strong>. &ldquo;we could rotate it by rotating all the matrices interacting with it, without changing model behavior&rdquo;.</li><li>Modules (attention heads/MLP layers) communicate via the residual stream: if one layer wants to pass information to another layer, it must write to some subspace of the residual stream that the later layer will read from. This makes the residual stream an <strong>information bottleneck</strong>.</li><li>Attention is the only mechanism in the transformer that can move information between token positions.</li><li>Attention heads operate <strong>entirely in parallel</strong>, and thus their inputs/outputs with respect to the residual stream can be examined independently.</li><li>Each attention head is composed of two independent circuits: the <strong>QK-circuit</strong> which computes attention probabilities, and the <strong>OV-circuit</strong> which actually applies the transformation of attention outputs to get attention outputs. The QK-circuit is basically a filter on what the OV-circuit computes.</li></ul><p><img loading=lazy src=/img/elhage.png alt title="Figure 4: The residual stream and how components interact with it, taken from Elhage et al. (2019)."></p><p>While all of this is important, the key point I wanted to introduce here is that <strong>the residual stream does not have a privileged basis</strong>. Since reads and writes with respect to the residual stream are linear transformations (e.g. QKV matrices in attention, the input transform in an MLP), the &lsquo;features&rsquo; in the representation space that we want to modify with our interventions may not be neuron-aligned. Thus, the best level of granularity with which to represent nodes in our causal models may not be a single neuron, but instead e.g. a whole transformer block and the residual stream, or each attention head and MLP and the residual stream.</p><p>This poses no problems for our definition of a causal model; we can simply define a structural function as having vector-valued inputs and outputs.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://transformer-circuits.pub/2023/interpretability-dreams/index.html>Chris Olah (2023)</a> says: &ldquo;The beauty of deep learning and scale is a kind of biological beauty.&rdquo; Another one I have heard and agree with is from Buck Shlegeris, who compares modern deep learning to alchemy; mechanistic interpretability researchers are striving towards turning this field into chemistry.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li><li><a href=http://localhost:1313/tags/causality/>Causality</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>