<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Some intuitions about transformers | Aryaman Arora</title>
<meta name=keywords content="NLP,transformers"><meta name=description content="I've been thinking about transformers a lot recently."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/2022-12-24-transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2022-12-24-transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="Some intuitions about transformers"><meta property="og:description" content="I've been thinking about transformers a lot recently."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2022-12-24-transformers/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-24T00:00:00+00:00"><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="Some intuitions about transformers"><meta name=twitter:description content="I've been thinking about transformers a lot recently."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Some intuitions about transformers","item":"http://localhost:1313/posts/2022-12-24-transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Some intuitions about transformers","name":"Some intuitions about transformers","description":"I've been thinking about transformers a lot recently.","keywords":["NLP","transformers"],"articleBody":"Unless you have been living under a rock for the last five years, you have definitely (if possibly unknowingly) somehow interacted with a machine learning model that uses the transformer architecture. I have spent a couple months poking at little transformer models like GPT-2 and the 19 million-parameter version of Pythia and yet after working at an interpretability startup for a week I realised that I actually don’t have a great understanding of how a transformer works. I was really only superficially familiar with the computations that happen within them and where weights are stored and so on. So this is a mishmash of some thoughts I have about how they work, some of which are hard to empirically verify and may be pure speculation. No claims of originality here either.\nAttention Attention is not really the best name for understanding how it works, at least in the way it is utilised in the modern transformer as a repeated and parallel operation involving multiple heads stacked in layers. I was actually surprised to see while writing this that Bahdanau didn’t really call the mechanism he designed “attention” except for briefly in one paragraph in the original paper, where he says:\nIntuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector.\nNeural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2014)\nSo it was first mentioned as an analogy rather than the all-encompassing name for it. I wonder how that ended up different.\nI think a better way of thinking about it in the context of the transformer is as information flow. Attention is the mechanism that transfers informations between fixed-memory token positions. A single attention head has the dual job of (1) figuring out from where to where information needs to be moved, and (2) actually doing that operation. So, a previous-token attention head (such as head 4.11 in GPT-2 small) is really moving information about a token to the next token. Each token now includes information about its predecessor, so heads in layer 5 can attend to just one token to know about bigrams.\nIf you think about attention as moving information around, it becomes really clear why phenomena like cross-layer cooperation between heads occur. A great example is how induction heads work: they promote generations like [A][B]...[A]([B]). It isn’t possible to do this with just one attention head in one layer (assuming the distance between [B]...[A] is not fixed). A head can learn to attend to copies of heads at the current position, but it can’t simultaneously move over information from one token over. Induction heads don’t show up in one layer models because a head in a layer lower needs to create a landmark saying “I am a [B] preceded immediately by an [A], look at me.”\nBasically, the role of attention is to figure out what information to move around and how to do so. If you buy that discrete linguistic features are encoded in separable directions in embedding space (I think this is mostly true given embedding arithmetic and the ability to remove information linear-algebraically, but subject to caveats like sparsity causing superposition of features), then attention should be able to pick specific features to copy over (not just token embeddings in their entirety) thanks to the QKV transforms.\nMLP The MLP layers are annoying as hell to make sense of. Unfortunately, the vast amount of computation (and this increases with scale) happens in the MLP layers. 80% of the FLOPs in OPT-175B! Yet, the key difference from attention is that MLPs do not move information around.\nHowever, consider the recursive layer structure of a transformer. The MLPs do operate on contextual information, using whatever stuff the attention heads below them have moved into that token position. And the two-layer structure (first multiplication followed by ReLU or another non-linearity, then second multiplication) makes the MLP similar to the KV-transform in attention.\nActually, thinking about it further, I think the MLPs are doing the same thing as the post-attention matrix multiplication by OV. The only difference is the addition of the ReLU which allows mass removal of information. So maybe the MLP not only serves as a sort of lookup for information to add without looking at other tokens (like it seems to encode knowledge), but it also cleans out unimportant information with the ReLU.\nLayerNorms The LayerNorms don’t, by definition, mess with the information contained in the language model (if we fix the mean and variance of activations as constants, then it’s just a linear transformation; this is called folding LayerNorm in) but they would rescale non-basis-aligned features in weird ways so it’s unclear what they do. I don’t think they add information by themselves (it’s just rotating and scaling the embedding information around), but the other mechanisms (attention and MLP) may strategically exploit the LayerNorm to offload e.g. getting rid of unimportant information.\nConclusion I read some hyperbole once on Twitter about how transformers are the modern Turing machine or whatever. I think that’s a silly comparison if you’re comparing their importance in the history of computing, since it’s very clear which one would not exist without the other. But on the architectural level it’s not a bad comparison. The tape of the transformer is the information contained at each token position after initial embedding. Attention is the instruction table and also moves the head around and registers states. MLPs and layernorms modify things inside a position in the tape without reference to other positions, but can still leverage information the attention put in there.\nThinking about transformers this way also makes a stronger argument for scaling, since the layer of a transformer obviously limits the attention heads’ chances to decide what information to move around. If you have 12 layers then you only have 12 chances to figure out what to move around and do that. Like, your max depth is 12 even if you have 144 heads total. Since we’re dealing with natural language, this is enough for most things but the long-tail and interesting phenomena get hurt. For example, this makes it pretty clear why addition would be hard for transformers, since you may have to carry over and store intermediate sums way more than 12 times when dealing with big numbers. I bet this also means adding breadth to the network (more parallel heads) will be less powerful than adding a new layer.\n","wordCount":"1108","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"2022-12-24T00:00:00Z","dateModified":"2022-12-24T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2022-12-24-transformers/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Some intuitions about transformers</h1><div class=post-description>I've been thinking about transformers a lot recently.</div><div class=post-meta><span title='2022-12-24 00:00:00 +0000 UTC'>December 24, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1108 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#attention>Attention</a></li><li><a href=#mlp>MLP</a></li><li><a href=#layernorms>LayerNorms</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Unless you have been living under a rock for the last five years, you
have definitely (if possibly unknowingly) somehow interacted with a
machine learning model that uses the transformer architecture. I have
spent a couple months poking at little transformer models like GPT-2 and
the 19 million-parameter version of
<a href=https://github.com/EleutherAI/pythia>Pythia</a> and yet after working at
an interpretability startup for a week I realised that I actually don&rsquo;t
have a great understanding of how a transformer works. I was really only
superficially familiar with the computations that happen within them and
where weights are stored and so on. So this is a mishmash of some
thoughts I have about how they work, some of which are hard to
empirically verify and may be pure speculation. No claims of originality
here either.</p><h2 id=attention>Attention<a hidden class=anchor aria-hidden=true href=#attention>#</a></h2><p>Attention is not really the best name for understanding how it works, at
least in the way it is utilised in the modern transformer as a repeated
and parallel operation involving multiple heads stacked in layers. I was
actually surprised to see while writing this that Bahdanau didn&rsquo;t really
call the mechanism he designed &ldquo;attention&rdquo; except for briefly in one
paragraph in the original paper, where he says:</p><blockquote><p>Intuitively, this implements a mechanism of attention in the decoder.
The decoder decides parts of the source sentence to pay attention to.
By letting the decoder have an attention mechanism, we relieve the
encoder from the burden of having to encode all information in the
source sentence into a fixedlength vector.</p><p><em><a href=https://arxiv.org/pdf/1409.0473.pdf>Neural Machine Translation by Jointly Learning to Align and
Translate</a> (Bahdanau et al.,
2014)</em></p></blockquote><p>So it was first mentioned as an analogy rather than the all-encompassing
name for it. I wonder how that ended up different.</p><p>I think a better way of thinking about it in the context of the
transformer is as information flow. Attention is the mechanism that
transfers informations between fixed-memory token positions. A single
attention head has the dual job of (1) figuring out from where to where
information needs to be moved, and (2) actually doing that operation.
So, a previous-token attention head (such as head 4.11 in GPT-2 small)
is really moving information about a token to the next token. Each token
now includes information about its predecessor, so heads in layer 5 can
attend to just one token to know about bigrams.</p><p>If you think about attention as moving information around, it becomes
really clear why phenomena like cross-layer cooperation between heads
occur. A great example is how <a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>induction
heads</a>
work: they promote generations like <code>[A][B]...[A]([B])</code>. It isn&rsquo;t
possible to do this with just one attention head in one layer (assuming
the distance between <code>[B]...[A]</code> is not fixed). A head can learn to
attend to copies of heads at the current position, but it can&rsquo;t
simultaneously move over information from one token over. Induction
heads don&rsquo;t show up in one layer models because a head in a layer lower
needs to create a landmark saying &ldquo;I am a <code>[B]</code> preceded immediately by
an <code>[A]</code>, look at me.&rdquo;</p><p>Basically, the role of attention is to figure out what information to
move around and how to do so. If you buy that discrete linguistic
features are encoded in separable directions in embedding space (I think
this is mostly true given embedding arithmetic and <a href=https://aclanthology.org/2020.acl-main.647/>the ability to
remove information
linear-algebraically</a>, but
subject to caveats like <a href=https://transformer-circuits.pub/2022/toy_model/index.html>sparsity causing superposition of
features</a>),
then attention should be able to pick specific features to copy over
(not just token embeddings in their entirety) thanks to the QKV
transforms.</p><h2 id=mlp>MLP<a hidden class=anchor aria-hidden=true href=#mlp>#</a></h2><p>The MLP layers are annoying as hell to make sense of. Unfortunately, the
vast amount of computation (and this increases with scale) <a href=https://twitter.com/stephenroller/status/1579993017234382849>happens in
the MLP
layers</a>.
80% of the FLOPs in OPT-175B! Yet, the key difference from attention is
that MLPs do not move information around.</p><p>However, consider the recursive layer structure of a transformer. The
MLPs do operate on contextual information, using whatever stuff the
attention heads below them have moved into that token position. And the
two-layer structure (first multiplication followed by ReLU or another
non-linearity, then second multiplication) makes the MLP similar to the
KV-transform in attention.</p><p>Actually, thinking about it further, I think the MLPs are doing the same
thing as the post-attention matrix multiplication by OV. The only
difference is the addition of the ReLU which allows mass removal of
information. So maybe the MLP not only serves as a sort of lookup for
information to add without looking at other tokens (like it <a href=https://arxiv.org/abs/2202.05262>seems to
encode knowledge</a>), but it also cleans
out unimportant information with the ReLU.</p><h2 id=layernorms>LayerNorms<a hidden class=anchor aria-hidden=true href=#layernorms>#</a></h2><p>The LayerNorms don&rsquo;t, by definition, mess with the information contained
in the language model (if we fix the mean and variance of activations as
constants, then it&rsquo;s just a linear transformation; this is called
<a href=https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln%22>folding
LayerNorm in</a>) but they would rescale non-basis-aligned features
in weird ways so it&rsquo;s unclear what they do. I don&rsquo;t think they add
information by themselves (it&rsquo;s just rotating and scaling the embedding
information around), but the other mechanisms (attention and MLP) may
strategically exploit the LayerNorm to offload e.g. getting rid of
unimportant information.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I read some hyperbole once on Twitter about how transformers are the
modern Turing machine or whatever. I think that&rsquo;s a silly comparison if
you&rsquo;re comparing their importance in the history of computing, since
it&rsquo;s very clear which one would not exist without the other. But on the
architectural level it&rsquo;s not a bad comparison. The tape of the
transformer is the information contained at each token position after
initial embedding. Attention is the instruction table and also moves the
head around and registers states. MLPs and layernorms modify things
inside a position in the tape without reference to other positions, but
can still leverage information the attention put in there.</p><p>Thinking about transformers this way also makes a stronger argument for
scaling, since the layer of a transformer obviously limits the attention
heads&rsquo; chances to decide what information to move around. If you have 12
layers then you only have 12 chances to figure out what to move around
and do that. Like, your max depth is 12 even if you have 144 heads
total. Since we&rsquo;re dealing with natural language, this is enough for
most things but the long-tail and interesting phenomena get hurt. For
example, this makes it pretty clear why addition would be hard for
transformers, since you may have to carry over and store intermediate
sums way more than 12 times when dealing with big numbers. I bet this
also means adding breadth to the network (more parallel heads) will be
less powerful than adding a new layer.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li><li><a href=http://localhost:1313/tags/transformers/>Transformers</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2023-03-28-research/><span class=title>« Prev</span><br><span>Me and Research</span>
</a><a class=next href=http://localhost:1313/posts/2022-10-08-ccg/><span class=title>Next »</span><br><span>Some CCG derivations in Hindi</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>