<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A machine-learned syntactic tagset? | Aryaman Arora</title>
<meta name=keywords content="NLP"><meta name=description content="Messing with the Kitaev et al. (2022) parser."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/2022-06-21-kitaev/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2022-06-21-kitaev/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="A machine-learned syntactic tagset?"><meta property="og:description" content="Messing with the Kitaev et al. (2022) parser."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2022-06-21-kitaev/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-21T00:00:00+00:00"><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="A machine-learned syntactic tagset?"><meta name=twitter:description content="Messing with the Kitaev et al. (2022) parser."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"A machine-learned syntactic tagset?","item":"http://localhost:1313/posts/2022-06-21-kitaev/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A machine-learned syntactic tagset?","name":"A machine-learned syntactic tagset?","description":"Messing with the Kitaev et al. (2022) parser.","keywords":["NLP"],"articleBody":"I have been fascinated by the ACL 2022 Best Paper, Learned Incremental Representations for Parsing by Nikita Kitaev, Thomas Lu, and Dan Klein. I had the good fortune, wandering around in Gather.town, to attend the poster session virtually before the award was announced. The big thing that the paper showed is the tractibility of human-like incremental parsing, which seems to have been pretty much a dormant problem in the field ever since full-sentence language models started dominating all the benchmarks. What’s especially crazy is the numbers Kitaev et al. produce with only left context for their parsing model beat state-of-the-art whole-sentence models from just a couple years ago.\nKitaev and Klein have done a ton of great work on syntactic parsing in the past, e.g. this paper which introduced attention to constituency parsing in 2018, or this one which Klein is an author on, achieving SOTA on constituency parsing with a neural chart-parsing model for (I think) the first time. Thomas Lu seems to have been a fourth-year undergrad at Berkeley at the time when the paper was submitted—insanely cool.\nWhat I found exciting about the paper was two things: (1) vector quantisation, a technique to induce discrete values in the model pipeline while still having the system be end-to-end differentiable; and (2) the unsupervised learning of a pretty small inventory of token labels, whose distribution is worth investigating. I’ve thought about a couple of tasks that would need learned discrete labels as an intermediate step, so learning about VQ was a huge deal for me (and led me down the rabbithole of variational autoencoders recently). But my main topic here today is the learned tagset itself.\nWhile checking out their code, I was excited to see that the model can be run in Colab extremely easily, so naturally I decided to poke around at it. The purpose of this post is to list some things I figured out about the learned 32-token tagset—I was curious how it compared to the usual part-of-speech tagsets that we use. Note that while they largely discuss the 256-token tagset with an F1 score of 94.49, the 32-token one is still really good with a score of 93.50.\nPrepositional phrases One of the first things I thought to poke at were prepositional phrase attachment, since it was mentioned in the paper. One thing became pretty clear: the thing this tagset is trying to do is very different from what conventional POS tagsets are. Part-of-speech tags intends to group lemmata with similar distributions together; e.g., the Cambridge Grammar of the English Language designs its POS tagset by making arguments about distribution, coordination scoping, and similar linguistic considerations—this is how it ends up grouping some conventional subordinators with prepositions, due to shared acceptance of some clausal dependents.\nThis tagset, on the other hand, is learned as the sole input in order to parse syntax, and that too with only the benefit of left-side context. A normal POS tagset, I wager, is not informative enough to generate accurate syntax trees by itself.\nA common feature is a distinction between leading (initial) and non-leading tags, sort of like the B and I tags of BIO. This makes sense; a leading tag signals the start of a new constituent, say an NP, and a non-leading tag signals its continuation. But things were not so simple as to let there be only two tags for indicating an NP. Below are some minimal pairs I found for PP-attachment tags.\n1 2 Ex. PP attachment (NP vs. VP) [24, 11, 0] [4, 16, 25] There is some cheese {from, in} the farm [24, 11, 0] [4, 11, 7] I saw the {man, plant} with a telescope [24, 11, 21] [4, 28, 21] I drew pictures {of, with} my kids Presence of preceding VP-internal constituent [17, 11, 7] [4, 28, 21] I drew {∅, pictures} with my kids [17, 28] [4, 28] He is {∅, truly} in fear [17, 28, 21] [4, 11, 21] He is {∅, truly} in the farm ? [17, 11] [17, 28] He is in {France, fear} [17, 11, 7] [17, 11, 21] I am on the {internet, edge} Table 1: Tags outputted for some PP attachment minimal pairs.\nSo we see that the tag on the preposition really does signal where it’s attaching: to the preceding NP (4), or to the VP (24). Also, there is another tag for attaching to the VP when there is no preceding NP in the way (17).\nThe nouns are sort of screwed up though. There is a clear distinction between leading and non-leading tokens, but it’s not just two tags involved. We see all of the following: [11, {0, 7, 21}], [28, 21], [16, 25]. I think this is signalling some kind of syntactic tendency embodied by these determiners/nouns, to use for potential dismabiguation as more tokens to the right are consumed by the encoder. I’m not sure what that tendency is though. Really interesting to find nevertheless!\nAttempts to describe each tag Because I am excellent at making use of my time, I tried to figure out what each tag was conveying syntactically. Some of these descriptions are very rough and were before I systematically tried out some of the PP-attachment stuff, so do not take them as authoritative. I also could not get it to produce all of the tags since I was only using it manually (it seemed like a fun challenge). An actual analysis should probably run the tagger on the original paper’s test set.\nNon-leading NP elements but nested in something. This contrasts with 7; if 0 is used, the PP attaches to the NP preceding, but with 7 it attaches to the VP as an adjunct.\nI saw a man with a book Commas.\nWhen I was young , I had no brains Fronted verb or auxiliary verb in WH-questions.\nWhat have you been doing How dare you ! Final punctuation.\nI hate commas . Prepositions, complement to VPs. Interestingly, to (in the infinitive) falls under this category as well!\nThere is some cheese [in the fridge]PP I did this [with the goal of saving you]PP I came here [with him]PP [on the run]PP I did this [to save you]S Non-leading elements of QPs.\nIt happened [almost 20]QP years ago Verbs in main clauses. Your least fancy verb in the sentence goes under this category. Interestingly, this includes auxiliary verbs in the clause too. This leads to chains of 6s.\nLuna smashed the pumpkin You run I will be drawing a picture of him He said that you are mean NP complements of PPs.\nThe power of the people is magnificent Inside a potato is a man I am a man of the city I saw the planet with a telescope Some kind of non-leading NP element (compounds?)\nWhat are the consequences of artificial intelligence ? Conjunctions.\n[You or me]NP will go Verbs in nominalised clauses.\nEating food {is, seems} epic Yelling at people is mean Kind of cursed, but also to when indicating a complement clause.\nI want to be the best Adjectives in predicate position.\nI am hungry Leading elements of non-subject NPs. This one is super interesting. So, if the object NP has a determiner, then that determiner is labelled 11. But, if there is no determiner in that NP then it’s just the nominal labelled that! This makes sense given only left context is available to the model; this signals the left-most boundary of an object NP.\nThe man eats the food The man eats food Also seems to include PP objects.\nA hungry man is in my kitchen Verbs in SINV (inverted clauses), as well as where the subject is a VP or NP with relative clause. Seems to be an indicator to head back up to the S constituent after a pretty complex subject.\nBehind every door is a fridge Eating food is epic The man who I saw smells funny Verbs in adjunct clauses.\n[When Luna smashed the pumpkin]SBAR , she hurt her foot Verbs in WH-subject clauses which cause movement.\nWho are you What [have you done]SQ What have you been doing ?\nLeading NP elements of subjects of adjunct and complement clauses.\nWhen the man was young he lived here He said that you are mean Leading elements of main-clause subject NPs. Interestingly, this behaves the exact same way as 11, indicating the leftmost token of a subject NP.\nThe man eats the food He eats the food Also includes subjects of SQ which end up post-verbal.\nHow dare you ! Preposition that heads a PP adjunct to the verb.\nI am [inside a potato]PP We live [for the delicious things in life]PP Yelling [at people]PP is mean Adverbs (spatial adjuncts, adjective modifiers).\nHe lives here Life is really weird Adverbs (verbal adjuncts in general).\nHe quickly handed me the potato Leading subjects of WH-questions and main clauses with preceding WH-adjunct.\nWhat have you been doing When the man was young he lived here Adverbial modifiers to S. These nest directly under S, no intermediate projections.\nHe threw up today Leading direct objects in those constructions where the indirect object isn’t in a PP.\nHe quickly handed me the potato Non-leading NP elements. These seem to all be non-leftmost elements of the (flat) NP that are nested under a VP or S directly. This contrasts with 25.\nThe man [eats [the food]NP ]VP [[A hungry man]NP has arrived]VP In copular clauses, inside the predicate NP following the determiner (labelled 28), the nominal is labelled 21 or 25. It seems to be 21 if inanimate (or tending towards inanimate?) but 25 if animate. Honestly, I have no clue what’s going on here. Maybe the sentence I am testing are two short relative to the training data so they confuse the parser, but this animacy thing seems to have a point to it!\nI am a {pumpkin, cheeseburger, telescope} Complementiser.\nHe said that you are mean WH-words. This is used regardless of where they are syntactically, in a question, in a relative clause, or as an adjunct clause (with when) etc.\nWhat have you done Who are you Why are you hitting yourself The man who I saw smells funny Prepositions.\nThe cat in [in the hat]PP strikes back 25 is used for a variety of non-leading NP elements inside the VP. One is the first NP with a PP complement. Another is the NP inside a PP following the object of the verb, which seems entirely counterintuitive.\nThere is [[a man]NP [in the fridge]PP ]NP We live for [[the delicious things]NP in life]NP There [is some cheese [in [the fridge]NP ]PP ]VP I have found yet another cursed contrast here though, see 21.\nI am a {man, woman, lady, baby, machine, device} Verbs in relative clauses.\nShe ate the pumpkin [that Luna smashed]SBAR [What you said]SBAR was wrong ?\nNP-leading predicate of a copular clause. Also in some PPs?\nWe are men (of the city) I am a man (of the city) I did this [with the goal of saving you]PP Honestly just a grab bag of weird NPs.\nI am [[four feet]NP tall]ADJP Non-leading NP element that is a modifier to an ADJP.\nI am [[four feet]NP tall]ADJP ?\nParticles.\nHe threw up everywhere The adverb ago.\n[6 years ago]ADVP he got here References Nikita Kitaev and Dan Klein. 2018. Constituency Parsing with a Self-Attentive Encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Association for Computational Linguistics. Nikita Kitaev, Thomas Lu, and Dan Klein. 2022. Learned Incremental Representations for Parsing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3086–3095, Dublin, Ireland. Association for Computational Linguistics. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Minimal Span-Based Neural Constituency Parser. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 818–827, Vancouver, Canada. Association for Computational Linguistics. ","wordCount":"2002","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"2022-06-21T00:00:00Z","dateModified":"2022-06-21T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2022-06-21-kitaev/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">A machine-learned syntactic tagset?</h1><div class=post-description>Messing with the Kitaev et al. (2022) parser.</div><div class=post-meta><span title='2022-06-21 00:00:00 +0000 UTC'>June 21, 2022</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2002 words&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#prepositional-phrases>Prepositional phrases</a></li><li><a href=#attempts-to-describe-each-tag>Attempts to describe each tag</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>I have been fascinated by the ACL 2022 Best Paper, <em><a href=https://aclanthology.org/2022.acl-long.220.pdf>Learned Incremental
Representations for
Parsing</a></em> by Nikita
Kitaev, Thomas Lu, and Dan Klein. I had the good fortune, wandering
around in Gather.town, to attend the poster session virtually before the
award was announced. The big thing that the paper showed is the
tractibility of human-like incremental parsing, which seems to have been
pretty much a dormant problem in the field ever since full-sentence
language models started dominating all the benchmarks. What&rsquo;s especially
crazy is the numbers Kitaev et al. produce <em>with only left context</em> for
their parsing model beat state-of-the-art <em>whole-sentence</em> models from
just a couple years ago.</p><p>Kitaev and Klein have done a ton of great work on syntactic parsing in
the past, e.g. <a href=https://aclanthology.org/P18-1249/>this paper</a> which
introduced attention to constituency parsing in 2018, or <a href=https://aclanthology.org/P17-1076.pdf>this
one</a> which Klein is an author on,
achieving SOTA on constituency parsing with a neural chart-parsing model
for (I think) the first time. Thomas Lu seems to have been a fourth-year
undergrad at Berkeley at the time when the paper was submitted—insanely
cool.</p><p>What I found exciting about the paper was two things: (1) <strong>vector
quantisation</strong>, a technique to induce discrete values in the model
pipeline while still having the system be end-to-end differentiable; and
(2) the unsupervised learning of a pretty small inventory of token
labels, whose distribution is worth investigating. I&rsquo;ve thought about a
couple of tasks that would need learned discrete labels as an
intermediate step, so learning about VQ was a huge deal for me (and led
me down the rabbithole of variational autoencoders recently). But my
main topic here today is the learned tagset itself.</p><p>While checking out their code, I was excited to see that the model can
be run <a href=https://colab.research.google.com/drive/1X99Vbz9pWv1-w-wZoCHzknYl_EU_GXZc>in
Colab</a>
extremely easily, so naturally I decided to poke around at it. The
purpose of this post is to list some things I figured out about the
learned 32-token tagset—I was curious how it compared to the usual
part-of-speech tagsets that we use. Note that while they largely discuss
the 256-token tagset with an <em>F1</em> score of 94.49, the
32-token one is still really good with a score of 93.50.</p><h2 id=prepositional-phrases>Prepositional phrases<a hidden class=anchor aria-hidden=true href=#prepositional-phrases>#</a></h2><p>One of the first things I thought to poke at were prepositional phrase
attachment, since it was mentioned in the paper. One thing became pretty
clear: the thing this tagset is trying to do is very different from what
conventional POS tagsets are. Part-of-speech tags intends to group
lemmata with similar distributions together; e.g., the <em>Cambridge
Grammar of the English Language</em> designs its POS tagset by making
arguments about distribution, coordination scoping, and similar
linguistic considerations—this is how it ends up grouping some
conventional subordinators with prepositions, due to shared acceptance
of some clausal dependents.</p><p>This tagset, on the other hand, is learned as the sole input in order to
parse syntax, and that too with only the benefit of left-side context. A
normal POS tagset, I wager, is not informative enough to generate
accurate syntax trees by itself.</p><p>A common feature is a distinction between <strong>leading</strong> (initial) and
<strong>non-leading</strong> tags, sort of like the B and I tags of BIO. This makes
sense; a leading tag signals the start of a new constituent, say an NP,
and a non-leading tag signals its continuation. But things were not so
simple as to let there be only two tags for indicating an NP. Below are
some minimal pairs I found for PP-attachment tags.</p><table><thead><tr><th>1</th><th>2</th><th>Ex.</th></tr></thead><tbody><tr><td>PP attachment (NP vs. VP)</td><td></td><td></td></tr><tr><td>[24, 11, 0]</td><td>[4, 16, 25]</td><td><em>There is some cheese <strong>{from, in} the farm</strong></em></td></tr><tr><td>[24, 11, 0]</td><td>[4, 11, 7]</td><td><em>I saw the {man, plant} <strong>with a telescope</strong></em></td></tr><tr><td>[24, 11, 21]</td><td>[4, 28, 21]</td><td><em>I drew pictures <strong>{of, with} my kids</strong></em></td></tr><tr><td>Presence of preceding VP-internal constituent</td><td></td><td></td></tr><tr><td>[17, 11, 7]</td><td>[4, 28, 21]</td><td><em>I drew {∅, pictures} <strong>with my kids</strong></em></td></tr><tr><td>[17, 28]</td><td>[4, 28]</td><td><em>He is {∅, truly} <strong>in fear</strong></em></td></tr><tr><td>[17, 28, 21]</td><td>[4, 11, 21]</td><td><em>He is {∅, truly} <strong>in the farm</strong></em></td></tr><tr><td>?</td><td></td><td></td></tr><tr><td>[17, 11]</td><td>[17, 28]</td><td><em>He is <strong>in {France, fear}</strong></em></td></tr><tr><td>[17, 11, 7]</td><td>[17, 11, 21]</td><td><em>I am <strong>on the {internet, edge}</strong></em></td></tr></tbody></table><p><strong>Table 1</strong>: Tags outputted for some PP attachment minimal pairs.</p><p>So we see that the tag on the preposition really does signal where it&rsquo;s
attaching: to the preceding NP (4), or to the VP (24). Also, there is
another tag for attaching to the VP when there is no preceding NP in the
way (17).</p><p>The nouns are sort of screwed up though. There is a clear distinction
between leading and non-leading tokens, but it&rsquo;s not just two tags
involved. We see all of the following: [11, {0, 7, 21}], [28, 21],
[16, 25]. I think this is signalling some kind of syntactic tendency
embodied by these determiners/nouns, to use for potential dismabiguation
as more tokens to the right are consumed by the encoder. I&rsquo;m not sure
what that tendency is though. Really interesting to find nevertheless!</p><h2 id=attempts-to-describe-each-tag>Attempts to describe each tag<a hidden class=anchor aria-hidden=true href=#attempts-to-describe-each-tag>#</a></h2><p>Because I am excellent at making use of my time, I tried to figure out
what each tag was conveying syntactically. Some of these descriptions
are very rough and were before I systematically tried out some of the
PP-attachment stuff, so do not take them as authoritative. I also could
not get it to produce all of the tags since I was only using it manually
(it seemed like a fun challenge). An actual analysis should probably run
the tagger on the original paper&rsquo;s test set.</p><ol><li><p>Non-leading NP elements but nested in something. This contrasts with
7; if 0 is used, the PP attaches to the NP preceding, but with 7 it
attaches to the VP as an adjunct.</p><ul><li>I saw a man with a <strong>book</strong></li></ul></li><li><p>Commas.</p><ul><li>When I was young <strong>,</strong> I had no brains</li></ul></li><li><p>Fronted verb or auxiliary verb in WH-questions.</p><ul><li>What <strong>have</strong> you been doing</li><li>How <strong>dare</strong> you !</li></ul></li><li><p>Final punctuation.</p><ul><li>I hate commas <strong>.</strong></li></ul></li><li><p>Prepositions, complement to VPs. Interestingly, <em>to</em> (in the
infinitive) falls under this category as well!</p><ul><li>There is some cheese [<strong>in</strong> the fridge]PP</li><li>I did this [<strong>with</strong> the goal of saving you]PP</li><li>I came here [<strong>with</strong> him]PP [<strong>on</strong> the
run]PP</li><li>I did this [<strong>to</strong> save you]S</li></ul></li><li><p>Non-leading elements of QPs.</p><ul><li>It happened [almost <strong>20</strong>]QP years ago</li></ul></li><li><p>Verbs in main clauses. Your least fancy verb in the sentence goes
under this category. Interestingly, this includes auxiliary verbs in
the clause too. This leads to chains of 6s.</p><ul><li>Luna <strong>smashed</strong> the pumpkin</li><li>You <strong>run</strong></li><li>I <strong>will be drawing</strong> a picture of him</li><li>He said that you <strong>are</strong> mean</li></ul></li><li><p>NP complements of PPs.</p><ul><li>The power of the <strong>people</strong> is magnificent</li><li>Inside a <strong>potato</strong> is a man</li><li>I am a man of the <strong>city</strong></li><li>I saw the planet with a <strong>telescope</strong></li></ul><p>Some kind of non-leading NP element (compounds?)</p><ul><li>What are the consequences of artificial <strong>intelligence</strong> ?</li></ul></li><li><p>Conjunctions.</p><ul><li>[You <strong>or</strong> me]NP will go</li></ul></li><li><p>Verbs in nominalised clauses.</p><ul><li><strong>Eating</strong> food {is, seems} epic</li><li><strong>Yelling</strong> at people is mean</li></ul><p>Kind of cursed, but also <em>to</em> when indicating a complement clause.</p><ul><li>I want <strong>to</strong> be the best</li></ul></li><li><p>Adjectives in predicate position.</p><ul><li>I am <strong>hungry</strong></li></ul></li><li><p>Leading elements of non-subject NPs. This one is super interesting.
So, if the object NP has a determiner, then that determiner is
labelled 11. But, if there is no determiner in that NP then it&rsquo;s
just the nominal labelled that! This makes sense given only left
context is available to the model; this signals the left-most
boundary of an object NP.</p><ul><li>The man eats <strong>the</strong> food</li><li>The man eats <strong>food</strong></li></ul><p>Also seems to include PP objects.</p><ul><li>A hungry man is in <strong>my</strong> kitchen</li></ul></li><li><p>Verbs in SINV (inverted clauses), as well as where the subject is a
VP or NP with relative clause. Seems to be an indicator to head back
up to the S constituent after a pretty complex subject.</p><ul><li>Behind every door <strong>is</strong> a fridge</li><li>Eating food <strong>is</strong> epic</li><li>The man who I saw <strong>smells</strong> funny</li></ul></li><li><p>Verbs in adjunct clauses.</p><ul><li>[When Luna <strong>smashed</strong> the pumpkin]SBAR , she hurt
her foot</li></ul><p>Verbs in WH-subject clauses which cause movement.</p><ul><li>Who <strong>are</strong> you</li><li>What [have you <strong>done</strong>]SQ</li><li>What have you <strong>been doing</strong></li></ul></li><li><p>?</p></li><li><p>Leading NP elements of subjects of adjunct and complement clauses.</p><ul><li>When <strong>the</strong> man was young he lived here</li><li>He said that <strong>you</strong> are mean</li></ul></li><li><p>Leading elements of main-clause subject NPs. Interestingly, this
behaves the exact same way as 11, indicating the leftmost token of a
subject NP.</p><ul><li><strong>The</strong> man eats the food</li><li><strong>He</strong> eats the food</li></ul><p>Also includes subjects of SQ which end up post-verbal.</p><ul><li>How dare <strong>you</strong> !</li></ul></li><li><p>Preposition that heads a PP adjunct to the verb.</p><ul><li>I am [<strong>inside</strong> a potato]PP</li><li>We live [<strong>for</strong> the delicious things in life]PP</li><li>Yelling [<strong>at</strong> people]PP is mean</li></ul></li><li><p>Adverbs (spatial adjuncts, adjective modifiers).</p><ul><li>He lives <strong>here</strong></li><li>Life is <strong>really</strong> weird</li></ul></li><li><p>Adverbs (verbal adjuncts in general).</p><ul><li>He <strong>quickly</strong> handed me the potato</li></ul></li><li><p>Leading subjects of WH-questions and main clauses with preceding
WH-adjunct.</p><ul><li>What have <strong>you</strong> been doing</li><li>When the man was young <strong>he</strong> lived here</li></ul><p>Adverbial modifiers to S. These nest directly under S, no
intermediate projections.</p><ul><li>He threw up <strong>today</strong></li></ul><p>Leading direct objects in those constructions where the indirect
object isn&rsquo;t in a PP.</p><ul><li>He quickly handed me <strong>the</strong> potato</li></ul></li><li><p>Non-leading NP elements. These seem to all be non-leftmost elements
of the (flat) NP that are nested under a VP or S directly. This
contrasts with 25.</p><ul><li>The <strong>man</strong> [eats [the <strong>food</strong>]NP ]VP</li><li>[[A <strong>hungry man</strong>]NP has arrived]VP</li></ul><p>In copular clauses, inside the predicate NP following the determiner
(labelled 28), the nominal is labelled 21 or 25. It seems to be 21
if inanimate (or tending towards inanimate?) but 25 if animate.
Honestly, I have no clue what&rsquo;s going on here. Maybe the sentence I
am testing are two short relative to the training data so they
confuse the parser, but this animacy thing seems to have a point to
it!</p><ul><li>I am a {<strong>pumpkin</strong>, <strong>cheeseburger</strong>, <strong>telescope</strong>}</li></ul></li><li><p>Complementiser.</p><ul><li>He said <strong>that</strong> you are mean</li></ul></li><li><p>WH-words. This is used regardless of where they are syntactically,
in a question, in a relative clause, or as an adjunct clause (with
<em>when</em>) etc.</p><ul><li><strong>What</strong> have you done</li><li><strong>Who</strong> are you</li><li><strong>Why</strong> are you hitting yourself</li><li>The man <strong>who</strong> I saw smells funny</li></ul></li><li><p>Prepositions.</p><ul><li>The cat in [<strong>in</strong> the hat]PP strikes back</li></ul></li><li><p>25 is used for a variety of non-leading NP elements inside the VP.
One is the first NP with a PP complement. Another is the NP inside a
PP following the object of the verb, which seems entirely
counterintuitive.</p><ul><li>There is [[a <strong>man</strong>]NP [in the
fridge]PP ]NP</li><li>We live for [[the <strong>delicious</strong> things]NP in
life]NP</li><li>There [is some cheese [in [the <strong>fridge</strong>]NP
]PP ]VP</li></ul><p>I have found yet another cursed contrast here though, see 21.</p><ul><li>I am a {<strong>man</strong>, <strong>woman</strong>, <strong>lady</strong>, <strong>baby</strong>, <strong>machine</strong>,
<strong>device</strong>}</li></ul></li><li><p>Verbs in relative clauses.</p><ul><li>She ate the pumpkin [that Luna <strong>smashed</strong>]SBAR</li><li>[What you <strong>said</strong>]SBAR was wrong</li></ul></li><li><p>?</p></li><li><p>NP-leading predicate of a copular clause. Also in some PPs?</p><ul><li>We are <strong>men</strong> (of the city)</li><li>I am <strong>a</strong> man (of the city)</li><li>I did this [with <strong>the</strong> goal of saving you]PP</li></ul><p>Honestly just a grab bag of weird NPs.</p><ul><li>I am [[<strong>four</strong> feet]NP tall]ADJP</li></ul></li><li><p>Non-leading NP element that is a modifier to an ADJP.</p><ul><li>I am [[four <strong>feet</strong>]NP tall]ADJP</li></ul></li><li><p>?</p></li><li><p>Particles.</p><ul><li>He threw <strong>up</strong> everywhere</li></ul><p>The adverb <em>ago</em>.</p><ul><li>[6 years <strong>ago</strong>]ADVP he got here</li></ul></li></ol><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Nikita Kitaev and Dan Klein. 2018. <a href=https://aclanthology.org/P18-1249/>Constituency Parsing with a
Self-Attentive Encoder</a>. In
<em>Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 2676–2686,
Melbourne, Australia. Association for Computational Linguistics.</li><li>Nikita Kitaev, Thomas Lu, and Dan Klein. 2022. <a href=https://aclanthology.org/2022.acl-long.220/>Learned Incremental
Representations for
Parsing</a>. In
<em>Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 3086–3095,
Dublin, Ireland. Association for Computational Linguistics.</li><li>Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. <a href=https://aclanthology.org/P17-1076/>A Minimal
Span-Based Neural Constituency
Parser</a>. In <em>Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers)</em>, pages 818–827, Vancouver, Canada.
Association for Computational Linguistics.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2022-08-04-naacl/><span class=title>« Prev</span><br><span>NAACL 2022</span>
</a><a class=next href=http://localhost:1313/posts/2022-05-23-tam/><span class=title>Next »</span><br><span>Secret verb forms in Hindi</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>