<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>EMNLP 2020 | Aryaman Arora</title>
<meta name=keywords content="NLP"><meta name=description content="Random stuff."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/2020-11-18-emnlp/><link crossorigin=anonymous href=/assets/css/stylesheet.2c337fb86f9536060b454f4a8b7f1d6740cceb72dc167ed453d119184019fa6a.css integrity="sha256-LDN/uG+VNgYLRU9Ki38dZ0DM63LcFn7UU9EZGEAZ+mo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon.png><link rel=apple-touch-icon href=http://localhost:1313/icon.png><link rel=mask-icon href=http://localhost:1313/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2020-11-18-emnlp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-W6HV8VE5SV"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W6HV8VE5SV",{anonymize_ip:!1})}</script><meta property="og:title" content="EMNLP 2020"><meta property="og:description" content="Random stuff."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2020-11-18-emnlp/"><meta property="og:image" content="http://localhost:1313/icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-11-18T00:00:00+00:00"><meta property="article:modified_time" content="2020-11-18T00:00:00+00:00"><meta property="og:site_name" content="Aryaman Arora"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/icon.png"><meta name=twitter:title content="EMNLP 2020"><meta name=twitter:description content="Random stuff."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"EMNLP 2020","item":"http://localhost:1313/posts/2020-11-18-emnlp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"EMNLP 2020","name":"EMNLP 2020","description":"Random stuff.","keywords":["NLP"],"articleBody":" Cool Papers Notes Claire Cardie: Information Extraction Nov 16, 10:00 AM\nClaire Cardie delivered a really cool keynote on information extraction from a historical perspective. (Surprising and uncomfortable how much NLP research started out with U.S. military applications.) My notes here don't make sense, it's just for me to note down things to read since I didn't know information extraction was a thing!\nOne thing to think about: Can we use information extraction techniques to build useful resources for low-resource languages? I'm thinking extracting data from the currently unstructured DSAL dictionaries for example, or the recent effort to do so from The Linguistic Survey of India.\nNER: Akbik et al. (2018, 2019) [CoNLL 03] relation extraction/classification: Soares et al. (2019) Miwa \u0026 Bansal (2016), Zhang et al. (2017), Wang et al. (2018), Luan et al. (2019), Wadden et al. (2019) event extraction (CNNs, RNNs) ACE same sentence tho Ralph Grishman BOAF: Semantics Nov 17, 4:00 PM The lexical semantics session at #emnlp2020 was super cool yet again. Some of the neat questions discussed:\n1. How much do we need explicit linguistic data (things like syntax trees) in our models? Seems like NLP is moving towards learning those implicitly.\n‚Äî ƒÄryaman Arora ëÄÜëÄ≠ëÅÜëÄ¨ëÄ´ëÄ¶ ëÄÖëÄ≠ëÅÑëÄ≠ëÄ∏ (@aryaman2020) November 17, 2020 Siva Reddy, Dipanjan Das, Ellie Pavlick, Matt Gardner, Chris Potts. Move to contextual representations is a better approximation of how linguistics thinks about language (Chris Potts), explicit linguistic structures [I suppose things like POS tags, dependencies, etc., \"explicit things like a parser\"] is going to be declining as we have better models that don't need that information and can learn it implicitly (Matt Gardner). [I am reminded of Ethan A. Chi's work on extracting syntactic representations from mBERT]. Nathan Schneider: Can NLP work towards helping linguistics too? (Seems like we only talk about the other way). Dipanjan Das: We have not yet fully explored the capabilities of transformers and other masked LMs, we should not discount them and say diminishing returns are inevitable. Example given of multi-digit arithmetic skills appearing with greater inputs. [but GPT-3 is huge! at what point do we draw the line? humans do more with far less.] Ellie Pavlick: We can't pick up world knowledge or even full level human-level language just from masked LMs (\"hints\" on the internet can be picked up), there's got to be a more efficient [multimodal?] way. But it is very possible that an NN can have human-level language skill, just not the current masked LM approach. multi-hop reasoning Why only text-in text-out? \"Fundamentally awkward\" way to think about language (Pavlick). Need embodying in the world. [What the heck even is language? Why are we using text as a proxy? I wonder why NLP work doesn't deal with, like, speech directly (outside ASR which is just a way to interpret into text).] \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. Linguistic vs. world knowledge, not a division actually reflected in masked language models which are learning both together. Lead to rethinking linguistics? Potts: Deep learning isn't gonna replace linguistics! Blackboxes Black-box explanation methods (LME, SHAP, Partial dependence) are when you don't have the training data, glass-box (EBM) when you do. Accuracy vs. intelligibility tradeoff? No longer the case necessarily, thanks to explainable boosting machines. Comparable to full-complexity models. EBMs are a type of generalised additive models (GAM) i.e. sums of functions or sums of functions of pairwise interactions. https://github.com/interpretml/interpret ","wordCount":"576","inLanguage":"en","image":"http://localhost:1313/icon.png","datePublished":"2020-11-18T00:00:00Z","dateModified":"2020-11-18T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2020-11-18-emnlp/"},"publisher":{"@type":"Organization","name":"Aryaman Arora","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Aryaman Arora (Alt + H)">Aryaman Arora</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/papers/ title=papers><span>papers</span></a></li><li><a href=http://localhost:1313/posts/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">EMNLP 2020</h1><div class=post-description>Random stuff.</div><div class=post-meta><span title='2020-11-18 00:00:00 +0000 UTC'>November 18, 2020</span>&nbsp;¬∑&nbsp;3 min&nbsp;¬∑&nbsp;576 words&nbsp;¬∑&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner></div></details></div><div class=post-content><h2>Cool Papers</h2><h2>Notes</h2><h3>Claire Cardie: Information Extraction</h3><p><em>Nov 16, 10:00 AM</em></p><p>Claire Cardie delivered a really cool keynote on information extraction from a historical perspective. (Surprising and uncomfortable how much NLP research started out with U.S. military applications.) My notes here don't make sense, it's just for me to note down things to read since I didn't know information extraction was a thing!</p><p>One thing to think about: <strong>Can we use information extraction techniques to build useful resources for low-resource languages?</strong> I'm thinking extracting data from the currently unstructured <a href=https://dsalsrv04.uchicago.edu/dictionaries/>DSAL dictionaries</a> for example, or the recent effort to do so from <a href=https://spraakbanken.gu.se/blogg/index.php/2020/09/01/griersons-linguistic-survey-of-india-as-open-access-digital-data-resource-for-studying-languages-of-south-asia/>The Linguistic Survey of India</a>.</p><ul><li>NER: Akbik et al. (2018, 2019) [CoNLL 03]</li><li>relation extraction/classification: Soares et al. (2019)</li><li>Miwa & Bansal (2016), Zhang et al. (2017), Wang et al. (2018), Luan et al. (2019), Wadden et al. (2019)</li><li>event extraction (CNNs, RNNs)<ul><li>ACE same sentence tho</li></ul></li><li>Ralph Grishman</li></ul><h3>BOAF: Semantics</h3><em>Nov 17, 4:00 PM</em><blockquote class=twitter-tweet data-theme=dark><p lang=en dir=ltr>The lexical semantics session at <a href="https://twitter.com/hashtag/emnlp2020?src=hash&amp;ref_src=twsrc%5Etfw">#emnlp2020</a> was super cool yet again. Some of the neat questions discussed:<br><br>1. How much do we need explicit linguistic data (things like syntax trees) in our models? Seems like NLP is moving towards learning those implicitly.</p>&mdash; ƒÄryaman Arora ëÄÜëÄ≠ëÅÜëÄ¨ëÄ´ëÄ¶ ëÄÖëÄ≠ëÅÑëÄ≠ëÄ∏ (@aryaman2020) <a href="https://twitter.com/aryaman2020/status/1328825182320996359?ref_src=twsrc%5Etfw">November 17, 2020</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><ul><li><a href=https://sivareddy.in/>Siva Reddy</a>, <a href=http://www.dipanjandas.com/>Dipanjan Das</a>, <a href=http://cs.brown.edu/people/epavlick/>Ellie Pavlick</a>, <a href=https://matt-gardner.github.io/>Matt Gardner</a>, <a href=https://web.stanford.edu/~cgpotts/>Chris Potts</a>.</li><li>Move to contextual representations is a better approximation of how linguistics thinks about language (Chris Potts), explicit linguistic structures [I suppose things like POS tags, dependencies, etc., "explicit things like a parser"] is going to be declining as we have better models that don't need that information and can learn it implicitly (Matt Gardner).</li><li>[I am reminded of <a href=http://ai.stanford.edu/blog/finding-crosslingual-syntax/>Ethan A. Chi's work on extracting syntactic representations from mBERT</a>].</li><li>Nathan Schneider: Can NLP work towards helping linguistics too? (Seems like we only talk about the other way).</li><li>Dipanjan Das: We have not yet fully explored the capabilities of transformers and other masked LMs, we should not discount them and say diminishing returns are inevitable. Example given of multi-digit arithmetic skills appearing with greater inputs. [but GPT-3 is huge! at what point do we draw the line? humans do more with far less.]</li><li>Ellie Pavlick: We can't pick up world knowledge or even full level human-level language just from masked LMs ("hints" on the internet can be picked up), there's got to be a more efficient [multimodal?] way. But it is very possible that an NN can have human-level language skill, just not the current masked LM approach.</li><li><a href=https://proceedings.neurips.cc/paper/2020/file/fc84ad56f9f547eb89c72b9bac209312-Paper.pdf>multi-hop reasoning</a></li><li>Why only text-in text-out? "Fundamentally awkward" way to think about language (Pavlick). Need embodying in the world. [What the heck even is language? Why are we using text as a proxy? I wonder why NLP work doesn't deal with, like, speech directly (outside ASR which is just a way to interpret into text).]</li><li><a href=https://arxiv.org/abs/2003.10555>"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators." Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.</a></li><li>Linguistic vs. world knowledge, not a division actually reflected in masked language models which are learning both together. Lead to rethinking linguistics?</li><li>Potts: Deep learning isn't gonna replace linguistics!</li></ul><h3>Blackboxes</h3><ul><li>Black-box explanation methods (LME, SHAP, Partial dependence) are when you don't have the training data, glass-box (EBM) when you do.</li><li>Accuracy vs. intelligibility tradeoff? No longer the case necessarily, thanks to <strong>explainable boosting machines</strong>. Comparable to full-complexity models.</li><li>EBMs are a type of generalised additive models (GAM) i.e. sums of functions or sums of functions of pairwise interactions.</li><li><a href=https://github.com/interpretml/interpret>https://github.com/interpretml/interpret</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2020-12-12-why-linguistics/><span class=title>¬´ Prev</span><br><span>Why Linguistics?</span>
</a><a class=next href=http://localhost:1313/posts/2020-07-10-acl/><span class=title>Next ¬ª</span><br><span>ACL 2020</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Aryaman Arora</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>