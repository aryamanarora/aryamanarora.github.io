[{"content":"Why DAS may fail Aryaman Arora\nJanuary 7, 2024\nRecently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:\na causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i.e. intervening along it should have no causal effect a dormant direction that achieves the desired causal effect but is not activated differently by the base and source Furthermore, they provide experimental evidence of DAS producing apparently large causal effects (according to their logit-difference metric) when applied to transformer MLP modules, which disappear when the DAS vector is projected onto the MLP output rowspace.\nA forthcoming response by Wu et al. (2024) shows that their definition of an interpretability illusion is flawed, even on the toy model they present to illustrate the definition\u0026mdash;it rules out the obviously non-illusory direction as having a significant illusory component. It\u0026rsquo;s also immediately apparent to us that the experimental setup they use in many instances is not one we would espouse for DAS: they overfit on one example for the fact-editing setup, they report FLDD and don\u0026rsquo;t note that the IIA metric is more robust to their claimed \u0026ldquo;illusion\u0026rdquo;, etc.\nHowever, I do not think this closes the matter. They were obviously wrong on many counts, but they are pointing to something concerning about DAS that neither of us really know how to explain but which we both recognise. It is experimentally true that DAS can \u0026ldquo;overfit\u0026rdquo;. In this brief post, I outline how this might be possible.\nToy examples The key interpretability problem we are trying to overcome with DAS, and others are trying to overcome with methods like LEACE (Belrose et al., 2023), is that we do not know the basis of the feature space that models operate on. We attempt to learn it by making a claim about counterfactual pairs of inputs and how they should be processed differently by the model; a method like DAS allows us to test such a hypothesis by trying to find a direction along which we can flip the model behaviour with regard to the counterfactual pair being tested.\nLet\u0026rsquo;s consider some simple scenarious about some toy feature spaces that can illustrate why 1D DAS may fail. Recall that 1D DAS learns a unit vector $\\mathbf{v}$ and applies the following patch along it:\n$$\\mathbf{act}{\\text{patched}} = \\mathbf{act}{\\text{base}} + (\\mathbf{v}^\\top\\mathbf{act}{\\text{src}} - \\mathbf{v}^\\top\\mathbf{act}{\\text{base}})\\mathbf{v}$$\n1D DAS on 1D feature The ideal scenario is that the feature we are trying to find with 1D DAS is indeed a 1D direction. This will obviously work perfectly, so I won\u0026rsquo;t walk through it.\n1D DAS on 2D feature Suppose that the feature we care about is actually represented as 2 orthogonal directions in activation space that take on values $\\in {0, 1}$, and the model\u0026rsquo;s output is some function of those. Can 1D DAS still learn a successful patch?\nWell, it is apparent that no matter what, 1D DAS cannot entirely patch the exact values along both directions. This is because the 1D DAS edit can only move the activation along the angle of the vector $\\mathbf{v}$, and a line is not sufficient to cover all possible 2D values.\nE.g., given $\\mathbf{act}{\\text{base}} = [0, 1]$, $\\mathbf{act}{\\text{src}} = [1, 0]$, and the basis directions being the two features, here are the results of the patch for some settings of DAS\u0026rsquo;s $\\mathbf{v}$:\n$\\mathbf{v}$ $\\mathbf{v}^\\top\\mathbf{act}_{\\text{base}}$ $\\mathbf{v}^\\top\\mathbf{act}_{\\text{src}}$ $\\mathbf{act}_{\\text{patched}}$ $[0, 1]$ $1$ $0$ $[0, 0]$ $[1, 0]$ $0$ $1$ $[1, 1]$ $\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]$ $\\frac{1}{\\sqrt{2}}$ $\\frac{1}{\\sqrt{2}}$ $[0, 1]$ $\\left[\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right]$ $\\frac{\\sqrt{3}}{2}$ $\\frac{1}{2}$ $[-0.183, 0.683]$ Also note that our 1D DAS projects from $\\mathbb{R}^2$ to $\\mathbb{R}^1$, so there will be collisions between activations that are an equal angle away from $\\mathbf{v}$, e.g. the third row in the table above.\nOkay, so 1D DAS does can\u0026rsquo;t learn to patch a 2D feature, which is great! We have successfully shown that illusions are impossible, and the blog post can end.\nNot quite. I will show in the next toy example how illusions may be possible.\n1D DAS to activate unrelated feature Let\u0026rsquo;s say our activation space is in $\\mathbb{R}^2$. Let\u0026rsquo;s say only the first dimension is causally relevant for our task, and all our inputs only vary along that: $[0, 0]$, $[1, 0]$. The second dimension is $0$ for all inputs.\nIf turning on the unrelated feature along the second dimension is somehow useful for the behaviour we want to induce, can 1D DAS activate it? Yes!\n$\\mathbf{v}$ Type Patch 1 Patch 2 $[1, 0]$ non-illusory $[1, 0] \\to [0, 0]$ $[0, 0] \\to [1, 0]$ $[0, 1]$ dormant (no effect) $[1, 0] \\to [1, 0]$ $[0, 0] \\to [0, 0]$ $\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]$ ??? $[1, 0] \\to [0.293, -0.707]$ $[0, 0] \\to [0.707, 0.707]$ In the last row above, we successfully modified the value of the second dimension such that it depends on the value along the first dimension. This isn\u0026rsquo;t new though; this is exactly what Makelov et al. (2023) claimed. Their definition was wrong, but this is the phenomenon they were talking about.\nHere is how I would describe this illusory behaviour as:\nGiven two sets of activations $X$ and $Y$:\nA feature that has (approximately) one value for $X$ and (approximately) a different value for $Y$ A feature, orthogonal and thus uncorrelated to the first, that has (approximately) the same value for both $X$ and $Y$ 1D DAS can induce a correlation between the two features, such that a patch can modify the value of (2) based on the value of (1). This results in patched activations where (2) takes on values never encountered in the input sets $X$, $Y$.\nThis is possible because the DAS vector $\\mathbf{v}$ is non-orthogonal to both features. Essentially, we give up some causal effect on feature 1 in order to create a causal effect on feature 2.\nNotice that we are only talking about features and not the rowspace/nullspace of downstream components. While Makelov et al. were on the right track, I think the downstream component subspace stuff is a distraction from the core point about DAS illusion.\nBut is this actually relevant beyond being a cute toy example? I will show how this idea lets us predict actual illusory behaviour by DAS!\nReal example A dataset for DAS on an autogressive language model includes pairs of counterfactual inputs and associated next-token predictions, such that the model should give one output for one input in a pair and a different output for the other input in the pair.\nWe will focus on interventions at the last block_output at the last token position. We will also only consider a single pair of inputs, i.e. the \u0026ldquo;factual recall\u0026rdquo; setting in Makelov et al. (2023) where DAS can overfit.\nConsider the activations at the last position at the last block_output for the two inputs. We will call those $\\mathbf{a}$ and $\\mathbf{b}$. I claim that DAS can learn an vector $\\mathbf{v}$ that perfectly turns $\\mathbf{a}$ into $\\mathbf{b}$ and vice versa.\nProof. Set $\\mathbf{v} := \\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})$, i.e. the unit vector along the difference between the two activations. If we run the intervention along this direction, we achieve the desired output:\n$$ \\begin{aligned} \\text{patch}(\\mathbf{b} \\to \\mathbf{a}) \u0026amp;= \\mathbf{b} + (\\mathbf{v}^\\top\\mathbf{a} - \\mathbf{v}^\\top\\mathbf{b})\\mathbf{v} \\ \u0026amp;= \\mathbf{b} + (\\mathbf{v}^\\top(\\mathbf{a} - \\mathbf{b}))\\mathbf{v}\\ \u0026amp;= \\mathbf{b} + \\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{a} - \\mathbf{b})\\right)\\mathbf{v}\\ \u0026amp;= \\mathbf{b} + \\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert} \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^2\\right)\\mathbf{v}\\ \u0026amp;= \\mathbf{b} + \\lVert \\mathbf{a} - \\mathbf{b} \\rVert\\mathbf{v}\\ \u0026amp;= \\mathbf{b} + \\lVert \\mathbf{a} - \\mathbf{b} \\rVert\\left(\\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})\\right)\\ \u0026amp;= \\mathbf{b} + \\mathbf{a} - \\mathbf{b} \\ \u0026amp;= \\mathbf{a} \\end{aligned} $$\nAnd the other way:\n$$ \\begin{aligned} \\text{patch}(\\mathbf{a} \\to \\mathbf{b}) \u0026amp;= \\mathbf{a} + (\\mathbf{v}^\\top\\mathbf{b} - \\mathbf{v}^\\top\\mathbf{a})\\mathbf{v} \\ \u0026amp;= \\mathbf{a} + (\\mathbf{v}^\\top(\\mathbf{b} - \\mathbf{a}))\\mathbf{v}\\ \u0026amp;= \\mathbf{a} - (\\mathbf{v}^\\top(\\mathbf{a} - \\mathbf{b}))\\mathbf{v}\\ \u0026amp;= \\mathbf{a} - (\\mathbf{a} - \\mathbf{b}) \\ \u0026amp;= \\mathbf{b} \\end{aligned} $$\nThus, setting $\\mathbf{v} := \\frac{1}{\\lVert \\mathbf{a} - \\mathbf{b} \\rVert}(\\mathbf{a} - \\mathbf{b})$ perfectly swaps the two inputs.\nOkay, so DAS should learn that, right? Actually, I will show that DAS will do something weird. First, note that the training objective of DAS (at least, as Zen and I have been using in our code) is to maximise the probability of the counterfactual next-token label using a cross entropy loss. The objective is not to swap the activations!\nThese activations (at last block_output at last position) will get unembeded into next-token logits. The logit and resulting probability for each token $i$ in the vocab is calculated as thus:\n$$\\text{logit}(\\mathbf{act}, \\text{vocab}_i) = \\mathbf{act}^\\top \\text{unembed}(\\text{vocab}_i)\\ \\text{prob}(\\mathbf{act}, \\text{vocab}_i) = \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j))}$$\nDAS wants to maximise the probability of the counterfactual output, which implies maximising the logit of the counterfactual output relative to all other logits, which implies maximising the dot product of the final activation with the counterfactual output unembedding relative to all other items in the vocabulary.\nWhen we add a vector $\\mathbf{u}$ to the final activation, how does that affect a particular logit?\n$$ \\begin{aligned} \\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i) \u0026amp;= (\\mathbf{act} + \\mathbf{u})^\\top \\text{unembed}(\\text{vocab}_i)\\ \u0026amp;= \\mathbf{act}^\\top \\text{unembed}(\\text{vocab}_i) + \\mathbf{u}^\\top \\text{unembed}(\\text{vocab}_i)\\ \u0026amp;= \\text{logit}(\\mathbf{act}, \\text{vocab}_i) + \\text{logit}(\\mathbf{u}, \\text{vocab}_i) \\end{aligned} $$\nAnd how about a particular probability?\n$$ \\begin{aligned} \\text{prob}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i) \u0026amp;= \\frac{\\exp(\\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act} + \\mathbf{u}, \\text{vocab}_j))}\\ \u0026amp;= \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i) + \\text{logit}(\\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j) + \\text{logit}(\\mathbf{u}, \\text{vocab}_j))}\\ \u0026amp;= \\frac{\\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_i))\\exp(\\text{logit}(\\mathbf{u}, \\text{vocab}_i))}{\\sum_j \\exp(\\text{logit}(\\mathbf{act}, \\text{vocab}_j))\\exp(\\text{logit}(\\mathbf{u}, \\text{vocab}_j))} \\end{aligned} $$\nSince, in this secnario, the training objective for DAS is just a constrained optimisation of an added vector $\\mathbf{u}$ to the activation, it seems that DAS should learn a patch that brings the activation as close to the next token label $\\text{vocab}_\\text{target}$, which is not necessarily the perfect patch for flipping the activation. Does this happen in practice?\nExamples ","permalink":"http://localhost:1313/drafts/2024-01-07-illusion/","summary":"Why DAS may fail Aryaman Arora\nJanuary 7, 2024\nRecently, Makelov et al. (2023) claimed that distributed alignment search (DAS; Geiger et al., 2023) can produce an interpretability illusion that does not accurately tell us about model-internal feature directions but nevertheless learns an intervention that achieves the desired causal effect. The crux of their argument is that a one-dimensional DAS intervention may intervene along both:\na causally disconnected direction that distinguishes between the source and base but is in the nullspace of all downstream model projections, i.","title":"Why DAS may fail"},{"content":"There is a new kind of interpretability being done these days: mechanistic interpretability. It is sometimes unclear whether mechanistic interpretability really is a departure from the methods that comprised \u0026ldquo;interpretability\u0026rdquo;, but if you are charitable, what sets it apart is that it is aims to \u0026ldquo;reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program\u0026rdquo; (Olah, 2022). This is a far more ambitious goal than previous research programmes in the realm of interpretability, whose goal is adequately summarised as \u0026ldquo;to provide explanations in understandable terms to a human\u0026rdquo; (Zhang et al., 2021). Mechanistic interpretability is model-first; it may not be a priori possible to develop faithful explanations for neural network behaviours based on what we currently know, and mechanistic approaches are willing to do the hard work of creating a new basis for understanding neural networks from the ground up.\nThis blog post is a comprehensive overview of one class of genuinely novel methods in (mechanistic) interpretability, which I call interventional interpretability. These approaches all share the broad idea of intervening on model internals to see how they affect model behaviour. Interventional experiments are similar in spirit to how biologists experiment on living organisms (except with finer-grained control and perfect observational accuracy).1\nThe key intuition behind interventional interpretability is that when we mess with things and they break, this tells us that the thing we messed with is important. Causal abstraction provides the framework to map our \u0026ldquo;mental models\u0026rdquo; about such processes to low-level implementation details of the thing we are messing with.\n[!warning]\nAlmost nothing in this blog post is original and I don\u0026rsquo;t claim to have invented this stuff\u0026mdash;think of it as my notes on existing papers that I have read. If you found it useful, feel free to cite it, but also make sure you cite the original works I reference in each section. This structure of this post is inspired by Lilian Weng\u0026rsquo;s blog.\n[!info] Sources\n\u0026ldquo;Causal Abstraction for Faithful Model Interpretation\u0026rdquo; (Geiger et al., 2023) \u0026ldquo;Causal Abstractions of Neural Networks\u0026rdquo; (Geiger et al., 2021) A causal view of neural networks The mathematical basis for understanding what interventions do to a neural network is often overlooked. A view of neural networks as causal models is, in my opinion, best laid out by Geiger et al. (2023). This section presents the definitions from that paper as a foundation for how we should think about interventional methods in interpretability. I will recast all existing interventional interpretability methods to fit in this framework.\nFirst, let\u0026rsquo;s define variables, which will constitute nodes in the computational graph which our neural network implements.\nDefinition 1: A variable $X$ has a range $\\mathsf{Val}(X)$ of possible values. In a set of variables $\\mathbf{V}$, no two variables can share the same value. Given a subset of variables $\\mathbf{X} \\subseteq \\mathbf{V}$, the values $\\mathbf{x} \\in \\mathsf{Val}(\\mathbf{X})$ is a partial setting. If $\\mathbf{v} \\in \\mathsf{Val}(\\mathbf{V})$ (i.e. all variables are set to something), then we have a total setting.\nNow, we define a causal model on a set of variables (nodes) and a set of structural functions (edges).\nDefinition 2: A causal model is a pair $\\mathcal{M} = (\\mathbf{V}, \\mathcal{F})$, where:\n$\\mathbf{V}$ is a set of variables, where each $X \\in \\mathbf{V}$ has a range $\\mathsf{Val}(X)$ of possible values. $\\mathcal{F} = \\{f_V\\}_{V \\in \\mathbf{V}}$ is a set of structural functions, with each $f_V: \\mathsf{Val}(\\mathbf{V}) \\to \\mathsf{Val}(V)$ assigns a value to variable $V \\in \\mathbf{V}$ as a function of the values of all the variables. To induce causal structure in this model, we note that, while each structural function $f_V$ takes in a setting of all the variables $\\mathbf{V}$ as input, its output may not necessarily depend on all of those variables. We say we can define a causal order between two variables $Y$ and $X$ where $Y \\prec X$ ($Y$ is a parent of $X$), if there is a setting $\\mathbf{w}$ of the variables $\\mathbf{W} = \\mathbf{V} \\setminus \\{X, Y\\}$ and settings $y, y\u0026rsquo;$ of $Y$ such that $f_X(\\mathbf{w}, y) \\neq f_X(\\mathbf{w}, y\u0026rsquo;)$, i.e. $Y$ has a direct effect on $X$.\nFinally, there are some special subsets of variables that we will need to refer to. One is input variables $\\mathbf{X}^{\\text{In}}_\\mathcal{M}$, which depend on no other variables (i.e. they have no parents). The other is output variables $\\mathbf{X}^{\\text{Out}}_\\mathcal{M}$, which no other variables depend on (i.e. they have no children). All remaining variables $\\mathbf{V}_{\\mathcal{M}} \\setminus (\\mathbf{X}_{\\mathcal{M}}^{\\text{In}} \\cup \\mathbf{X}_{\\mathcal{M}}^{\\text{Out}})$ are intermediate variables.\nOnce we have the variable ordering, we can easily make the computational graph that corresponds to this causal model. The nodes are the variables. The directed edges are the ordering relations that we found based on the structural functions.\nAlso note that for the purposes of this post, we only need to consider acyclic causal models.\nExample: Causal model Let\u0026rsquo;s consider a simple neural network that computes the identity function, and define a causal model where the variables are individual neurons. (We can define the model at a coarser-grained level too, e.g. layers, but we will keep it simple for this example). This example is taken from Makelov et al. (2023) because it will come up again later.\nThe network takes in an input $x \\in \\mathbb{R}$, computes a hidden state $\\mathbf{h} = x\\mathbf{w}_1$, and the output $y = \\mathbf{w}_2^T\\mathbf{h}$. We define the weights as:\n$$ \\begin{aligned} \\mathbf{w}_1 \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 1\\end{bmatrix} \\ \\mathbf{w}_2 \u0026amp;= \\begin{bmatrix} 0 \u0026amp; 2 \u0026amp; 1\\end{bmatrix} \\end{aligned} $$\nThus we have the variables and functions:\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ \u0026mdash; $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x$ $\\mathbf{h}_2$ $f_{\\mathbf{h}_2}(x) = 0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x$ $y$ $f_y(\\mathbf{h}_1, \\mathbf{h}_2, \\mathbf{h}_3) = 0 \\cdot \\mathbf{h}_1 + 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3$ But note that our definition for the causal order between variables in a model actually isn\u0026rsquo;t satisfied by all of these functions. We have to get rid of dead connections since they don\u0026rsquo;t mediate any causal effect:\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ \u0026mdash; $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3$ Thus, we get the ordering relations $x \\prec \\mathbf{h}_1$, $x \\prec \\mathbf{h}_3$, $\\mathbf{h}_2 \\prec y$, $\\mathbf{h}_3 \\prec y$. Per our earlier remarks, the input variable is $x$ and the output variable is $y$, and the rest are intermediate variables. Finally, we can make the computational graph representing our causal model:\nDefining Interventions Okay, now we have our causal model $\\mathcal{M}$. Let\u0026rsquo;s mess with it.\nFirst, we will define a partial setting of variables, which permits us to \u0026ldquo;override\u0026rdquo; the values of arbitrary subsets of variables.\nDefinition 3: Given a partial setting $\\mathbf{u}$ for a set of variables $\\mathbf{U} \\supseteq \\mathbf{X}$, we define $\\mathsf{Proj}(\\mathbf{u}, \\mathbf{X})$ to be the restriction of $\\mathbf{u}$ to values of the variables in the set $\\mathbf{X}$. Also, given a partial setting $\\mathbf{x}$ and set $\\mathbf{U} \\supseteq \\mathbf{X}$: $$\\mathsf{Proj}^{-1}(\\mathbf{x}, \\mathbf{U}) = \\{\\mathbf{u} \\in \\mathsf{Val}(\\mathbf{U}) : \\mathsf{Proj}(\\mathbf{u}, \\mathbf{X}) = \\mathbf{x}\\}$$ Basically, for all possible settings of the variables in $\\mathbf{U}$, this fixes the partial setting $\\mathbf{x}$ on $\\mathbf{X}$.\nNow, we use this to define an intervention!\nDefinition 4: Given a causal model $\\mathcal{M}$, an intervention is a partial setting of variables $\\mathbf{i} \\in \\mathsf{Val}(\\mathbf{I})$ for a subset $\\mathbf{I} \\subseteq \\mathbf{V}$. We define the intervened model $\\mathcal{M}_{\\mathbf{i}}$ as the same as $\\mathcal{M}$, except we replace $f_X$ with $\\mathbf{v} \\mapsto \\mathsf{Proj}(\\mathbf{x}, X)$ for each variable $X \\in \\mathbf{I}$.\nIn non-mathematical English, an intervention is just a partial setting of variables that we enforce on the causal model. We do this by \u0026ldquo;overriding\u0026rdquo; the structural functions associated with those variables to output the value we gave to that variable in our partial setting. Note that this severs the overriden variables from their parents.\nFinally, we define what a solution to a causal model is so that we can measure the effect of an intervention.\nDefinition 5: Given a causal model $\\mathcal{M} = \\langle \\mathbf{V}, \\mathcal{F} \\rangle$, the set of solutions called $\\textsf{Solve}(\\mathcal{M})$ is the set of all $\\mathbf{v} \\in \\mathsf{Val}(\\mathbf{V})$ such that all the structural equations $v = f_V(\\mathbf{v})$ are satisfied for each variable $v \\in \\mathbf{v}$. In an acyclic model, any intervention $\\mathbf{i}$ results in a model with a single solution and we use the notation $\\mathsf{Solve}(\\mathcal{M}_{\\mathbf{i}})$ to refer to that singleton solution as well as the one-member set of solutions.\nExample: Intervention We will now (1) run our example causal model $\\mathcal{M}$ without any interventions, and (2) intervene on it at one hidden neuron to see what happens.\nFirst, let\u0026rsquo;s compute the total setting of variables for input $x := 5$. Technically, this is an intervention where we do $\\mathbf{i} := \\{x: 5\\}$ and compute $\\mathsf{Solve}(\\mathcal{M}_\\mathbf{i})$.\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ $5$ $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x = 5$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $f_{\\mathbf{h}_3}(x) = x = 5$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3 = 0 + 5 = 5$ Wow, the output number is also $5$. Now let\u0026rsquo;s intervene on $\\mathbf{h}_3$ too and see what happens. We define $\\mathbf{i} := \\{x: 5;~\\mathbf{h}_3: 42\\}$ and compute $\\mathsf{Solve}(\\mathcal{M}_\\mathbf{i})$.\nVariable: $V \\in \\mathbf{V}$ Function: $f_V \\in \\mathcal{F}$ $x$ $5$ $\\mathbf{h}_1$ $f_{\\mathbf{h}_1}(x) = x = 5$ $\\mathbf{h}_2$ $0$ $\\mathbf{h}_3$ $42$ $y$ $f_y(\\mathbf{h}_2, \\mathbf{h}_3) = 2 \\cdot \\mathbf{h}_2 + \\mathbf{h}_3 = 0 + 42 = 42$ Woah, we change the output! If you look at our computation graph now, notice that we severed the edge between $x$ and $\\mathbf{h}_3$ when performing this intervention:\nSince our intervention did affect the output $y$, we also found a causally-relevant pathway in the graph! Modifying $\\mathbf{h}_3$ directly modifies $y$. This obviously taught us nothing new about this model because we already understand it completely. The power of interventional interpretability is that it allows us to test hypotheses about intermediate variables in arbitrarily complex models\u0026mdash;such as multi-billion-parameter transformers.\nInterchange intervention Definition 6: Consider a neural network $\\mathcal{N}$, with input and output variables $\\mathbf{X}_{\\mathcal{L}}^{\\text{In}}, \\mathbf{X}_{\\mathcal{L}}^{\\text{Out}} \\subseteq \\mathbf{V}_{\\mathcal{L}}$, disjoint subsets of intermediate variables $\\mathbf{X}_{\\mathcal{L}}^1, \\ldots, \\mathbf{X}_{\\mathcal{L}}^k \\subseteq \\mathbf{V}_{\\mathcal{L}} \\setminus (\\mathbf{X}_{\\mathcal{L}}^{\\text{In}} \\cup \\mathbf{X}_{\\mathcal{L}}^{\\text{Out}})$, and base input $\\mathbf{b}$ and source inputs $\\mathbf{s}_1, \\ldots, \\mathbf{s}_k \\in \\mathbf{X}_{\\mathcal{L}}^{\\text{In}}$.\nWe define an interchange intervention to be the partial setting that sets the input variables to the base input $\\mathbf{b}$ and the intermediate variables $\\mathbf{X}_{\\mathcal{L}}^i$ to the values they would take on if the source input $\\mathbf{s}_i$ were provided to $\\mathcal{N}$: $$ \\begin{aligned} \\mathsf{IntInv}\u0026amp;(\\mathcal{N}, \\mathbf{b}, \\langle \\mathbf{s}_1, \\ldots, \\mathbf{s}_k \\rangle, \\langle \\mathbf{X}_{\\mathcal{L}}^1, \\ldots, \\mathbf{X}_{\\mathcal{L}}^k \\rangle)\\ \u0026amp;:= \\mathbf{b} \\cup \\mathsf{Proj}(\\mathsf{Solve}(\\mathcal{N}_{\\mathbf{s}_1}), \\mathbf{X}_{\\mathcal{L}}^1) \\cup \\ldots \\cup \\mathsf{Proj}(\\mathsf{Solve}(\\mathcal{N}_{\\mathbf{s}_k}), \\mathbf{X}_{\\mathcal{L}}^k) \\end{aligned} $$\nThinking about transformers [!info] Sources\n\u0026ldquo;A Mathematical Framework for Transformer Circuits\u0026rdquo; (Elhage et al., 2019) \u0026ldquo;Softmax Linear Units\u0026rdquo; (Elhage et al., 2022) \u0026ldquo;Toy Models of Superposition\u0026rdquo; (Elhage et al., 2022) In the example I presented, we mapped our neural network into a causal model at the neuron level. But given a transformer language model (the setting which we actually care about) that is potentially very large, is this the right level of granularity to be performing interventions at?\nLet\u0026rsquo;s summarise our understanding of the transformer architecture first, which I believe has only seriously been tackled by Elhage et al. (2019). They proposed some new conceptual ways that we should think about the transformer (when doing interpretability at least):\nThere is a single communication channel spanning all layers called the residual stream. Each attention layer and MLP layer of the transformer takes linear transformers of the residual stream as input, and additively contributes its output back to the residual stream. (In the traditional view, the residual stream is the skip connections between layers, which people sometimes forget about!) The residual stream does not have a privileged basis. \u0026ldquo;we could rotate it by rotating all the matrices interacting with it, without changing model behavior\u0026rdquo;. Modules (attention heads/MLP layers) communicate via the residual stream: if one layer wants to pass information to another layer, it must write to some subspace of the residual stream that the later layer will read from. This makes the residual stream an information bottleneck. Attention is the only mechanism in the transformer that can move information between token positions. Attention heads operate entirely in parallel, and thus their inputs/outputs with respect to the residual stream can be examined independently. Each attention head is composed of two independent circuits: the QK-circuit which computes attention probabilities, and the OV-circuit which actually applies the transformation of attention outputs to get attention outputs. The QK-circuit is basically a filter on what the OV-circuit computes. While all of this is important, the key point I wanted to introduce here is that the residual stream does not have a privileged basis. Since reads and writes with respect to the residual stream are linear transformations (e.g. QKV matrices in attention, the input transform in an MLP), the \u0026lsquo;features\u0026rsquo; in the representation space that we want to modify with our interventions may not be neuron-aligned. Thus, the best level of granularity with which to represent nodes in our causal models may not be a single neuron, but instead e.g. a whole transformer block and the residual stream, or each attention head and MLP and the residual stream.\nThis poses no problems for our definition of a causal model; we can simply define a structural function as having vector-valued inputs and outputs.\nChris Olah (2023) says: \u0026ldquo;The beauty of deep learning and scale is a kind of biological beauty.\u0026rdquo; Another one I have heard and agree with is from Buck Shlegeris, who compares modern deep learning to alchemy; mechanistic interpretability researchers are striving towards turning this field into chemistry.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/drafts/2023-12-14-intervention/","summary":"There is a new kind of interpretability being done these days: mechanistic interpretability. It is sometimes unclear whether mechanistic interpretability really is a departure from the methods that comprised \u0026ldquo;interpretability\u0026rdquo;, but if you are charitable, what sets it apart is that it is aims to \u0026ldquo;reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program\u0026rdquo; (Olah, 2022). This is a far more ambitious goal than previous research programmes in the realm of interpretability, whose goal is adequately summarised as \u0026ldquo;to provide explanations in understandable terms to a human\u0026rdquo; (Zhang et al.","title":"Interventional interpretability I: Causal abstraction"},{"content":"You are a Hittite spy. You report directly to the great king Šuppiluliuma I, who rules from his magnificent capital at Ḫattuša. Recently, the Hittites have been on good terms with their neighbours the Hurrians. Still, you can never be too sure about even the best of allies, and therefore the king has sent you to infilitrate the Hurrian bureauracy so that you can pre-empt any conspiracies brewing within.1\nYou are so good at your job that you have totally compromised all incoming information channels into the Hurrian government apparatus\u0026mdash;you can mess with their weather reports, their intelligence dossiers, their tax records, etc. However, there is one big problem preventing you from completely compromising this adversary: all of their internal communication is encrypted and your cryptographers have made no progress at cracking the code. Furthermore, no one inside the government can be bribed to reveal how the encryption works, since the encryption method is so complex that no single person (let alone a random civil servant) can understand how it works.2 Basically, you can only mess with what\u0026rsquo;s coming in (all information channels, including internal ones) and observe what goes out (governmental decisions decided by the top brass).\nKnowing who is in charge of what will be useful for identifying high-value targets or figuring out what minimal set of information channels to manipulate to steer the government into doing what you want. So how can you figure out how the government is structured?\nThis is the interpretability problem.\nHere are some of the approaches the surprisingly sophisticated Hittite geopolitics experts have suggested over the years for cracking this problem:\nProbing: Access past internal encrypted communications and resulting government decisions and train machine learning models to try and guess what the government will do based on just the communications. You\u0026rsquo;re just a spy though\u0026mdash;you don\u0026rsquo;t understand machine learning and you don\u0026rsquo;t see how this tells you anything useful about who\u0026rsquo;s in charge of what. (Breaking out of character: this doesn\u0026rsquo;t establish causality between decisions and communications\u0026mdash;you only know X information could be used based on this activation, not whether the model actually does so. And interpreting a model with another model is\u0026hellip; an interesting choice. I feel like we can do better.)3 Interpret attention patterns: While you can\u0026rsquo;t tell what the content of each encrypted message is, you do know who is sending it to who and what information channels are being accessed. So you can establish who is whose superior, and what input information some decisions may be based on. But you are missing the actual meaningful part of the messages: their content! (Attention patterns are seductively easy to visualise, but again they don\u0026rsquo;t establish causality between parts of a model. The attention pattern doesn\u0026rsquo;t tell us how information is used or manipulated, only where it goes.)4 Zero ablation: Erase some set of internal messages and see what happens. This might not tell you much though since it may just cause confusion in the governmental ranks, probably making some superiors angry at their subordinates for not transmitting important information rather than meaningfully affecting governmental decisions. (Zero ablation may throw the model off-distribution in an important way, since you have no idea what a zero vector as an internal activation actually means\u0026mdash;e.g. maybe \u0026ldquo;no information\u0026rdquo; is encoded as a non-zero vector.)5 Mean ablation: Take all encrypted communications recorded over a particular channel in the past. In this analogy we can\u0026rsquo;t really take the mean over activations, but let\u0026rsquo;s say we produce a really boring and average communication (e.g. the mode). This means no important (i.e. anomalous, low-surprisal) information gets transferred over this channel. So e.g. you may be able to figure out who in the government handles disaster response if you replace their \u0026ldquo;oh no!\u0026rdquo; message into an everyday \u0026ldquo;all clear\u0026rdquo; message. (This might again be weirdly off distribution. In a real model, you might then be taking the mean of a non-normally distributed activation and producing some weird new stuff the model has never seen before.) Path patching or interchange intervention: Feed wrong information to only some people in the government or only along some chains of command. See how the response changes\u0026mdash;you can causally establish what people are responsible for delivering certain kinds of information up the chain of command.6 Resampling ablation: Pick a random encrypted message from the past. Substitute it in place of the one being transmitted today. Do this a bunch of times with different random messages and see what happens\u0026mdash;e.g. maybe suddenly the government is unable to feed its population, in which case you know you messed with someone responsible for agriculture. (Easier to automate than the above but hard to figure out how to set up your experiments.)7 Okay now I\u0026rsquo;ll break out of character.\nWhat is the best method for figuring out how a neural network computes something? It\u0026rsquo;s barely been a couple years and so many different approaches have proliferated. I haven\u0026rsquo;t even listed all the possible approaches because some of them are pretty hard to analogise\u0026mdash;e.g., iterative nullspace projections attempt to remove concepts in the activation space by (iteratively) projecting activations onto the nullspace of linear probes trained to predict that concept (see Ravfogel et al., 2020). There\u0026rsquo;s a bunch of work on fact-editing in transformers too that is relevant to interpretability as well. I also don\u0026rsquo;t even mention explainability methods (which sometimes aren\u0026rsquo;t conceptually different from probing).\nBroadly, I think methods that establish causality are the most promising and well-founded. The argument for causality stems from the deficiency in probing that I think is best articulated in Geiger et al. (2023):\nFrom an information-theoretic point of view, we can observe that using arbitrarily powerful probes is equivalent to measuring the mutual information between the concept and the neural representation (Hewitt and Liang, 2019; Pimentel et al., 2020). If we restrict the class of probing models based on their complexity, we can measure how usable the information is (Xu et al., 2020; Hewitt et al., 2021). Regardless of what probe models are used, successfully probing a neural representation does not guarantee that the representation plays a causal role in model behavior (Ravichander et al., 2020; Elazar et al., 2020; Geiger et al., 2020, 2021).\nWhat we want to know, when interpreting how a model works, is how information is used to produce the output. What probing tells is whether that information could be used to produce a desired output. To illustrate the point above about probes measuring mutual information: imagine your transformer model internally one-hot encodes your vocabulary. You want to probe whether the model knows if a word is a noun or not. With an MLP probe on the embeddings (with sufficient layers), you would think yes\u0026mdash;because an NN with nonlinearities can approximate any function, and any function on your vocabulary can be encoded on one-hot vector inputs! Your probe would basically just have to learn a lookup table, never mind whether or how the model does. This is formalised in the data processing inequality and first pointed out in NLP literature by Pimentel et al. (2020).\nWe don\u0026rsquo;t actually care about whether information could be extracted from a representation. We care about whether it is by the model we\u0026rsquo;re studying. Causal abstraction methods, which test counterfactual inputs to parts of the model that let uss modify specific information, can tell us this.\nI think NLP should employ more whacky analogies (see e.g. Leslie Lamport\u0026rsquo;s work on distributed systems). I am sorry in advance if this makes no sense!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI know it\u0026rsquo;s not a perfect analogy so you can poke holes in it if you like. Or just assume every government worker is completely loyal so you can\u0026rsquo;t just manipulate people to get what you want.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI am a real scholar so I will try to cite some papers. But there\u0026rsquo;s way too much probing literature to sift through. Start with ctrl-F \u0026ldquo;probing\u0026rdquo; on Rogers et al. (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nClark et al. (2019)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFrom here on out, we are in the Wild West of mechanistic interpretability, where much interesting work is very recent and confined to blogposts that can be kind of hard to understand, rather than arXiv papers (which tbh are often still hard to understand). The earliest use of zero ablation is probably nostalgebraist (2020), who zero-ablated internal model layers to get a sort of early peak into what activations may mean in terms of the vocabulary. You see the method often used by LessWrong-ers. Also, note that zero ablation should be pretty useless on models trained with dropout (since that applies zero ablation to increase model robustness), as pointed out by Neel Nanda.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPath patching comes from Wang et al. (2022) which was the first work to find a circuit accomplishing a specific task in GPT-2 Small. The method is expounded upon in Goldkowsky-Dill et al. (2023) [which I am an author on]. Interchange interventions come from this line of work: Geiger et al. (2023), Geiger et al. (2021). The basic idea between the two lines of work is the same, but I have yet to fully understand the latter series to be confident that the implementation is the same too.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nResampling ablations are used in Goldkowsky-Dill et al. (2023). A promising kind of work is automated circuit interpretability as implemented by Conmy et al. (2023).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2023-08-10-causality/","summary":"You are a Hittite spy. You report directly to the great king Šuppiluliuma I, who rules from his magnificent capital at Ḫattuša. Recently, the Hittites have been on good terms with their neighbours the Hurrians. Still, you can never be too sure about even the best of allies, and therefore the king has sent you to infilitrate the Hurrian bureauracy so that you can pre-empt any conspiracies brewing within.1\nYou are so good at your job that you have totally compromised all incoming information channels into the Hurrian government apparatus\u0026mdash;you can mess with their weather reports, their intelligence dossiers, their tax records, etc.","title":"The Hittite problem"},{"content":"I started doing research in the summer before my senior year of high school, the summer of 2019. I\u0026rsquo;m closing in on 4 years now. The time I have been doing research subsumes the COVID-19 pandemic as well as my entire undergraduate education at Georgetown. I was 16 when I started doing research and I am 20 now. If I have an expected lifespan of ~80 years, then I hope I get to have 60 more years of research left in me.\nTo be completely honest though, I got into research because I wanted to maximise my chances of getting into MIT. Those days in high school were not so long ago, when dreams had a timeline extending to college and no further, when everything seemed so immediate and important. Fun fact: I did not get into MIT. Thankfully, research has stuck with me though.\nIt infuriates me now how cynical my original motivations were. My attitude towards major life decisions as a high school student bordered on careerist. I wanted to do things for the sole reason that they would look good on my application. I had very little respect for what I learned in school beyond the final grade or AP score, even though I did pretty well. I also had an inflated sense of my own abilities\u0026mdash;it got better after I moved to D.C., a bigger city where people had greater aspirations and the impressive accomplishments to back them up. But I really did expect to get into any college I wanted.\nI don\u0026rsquo;t think such an attitude is uncommon among many high school student in the United States today. The nature of college admissions has a big role to play in this; people optimise for what is rewarded, and when there are a lot of people and very few spots the optimisation gradient blows up. It does kind of suck though.\nWhat even is research? I don\u0026rsquo;t think I have a great answer. I think the name for it carries a lot of baggage. Maybe at its core, research is just about asking questions. Just a fancy name for applying one\u0026rsquo;s innate sense of curiosity. Unlike much of what we deal with in everyday life, the search space in research is infinite. Everything I did in life before research had a finite search space, in contrast\u0026mdash;there are $n$ prestigious colleges that are worth going to, there are $x$ acceptable fields of study (and they are all STEM), there are $y$ places worth working at and the metric to optimise is a 1-dimensional scalar called \u0026ldquo;salary\u0026rdquo;. Such a big world with so little that we understand, and we carve out tiny domains to fit our whole lives into.\nAt the urging of my mom,1 I cold-emailed a lot of people in the summer of 2019 to try and get to work on a research project. I had some vague idea that I wanted to do computer science stuff, but also with linguistics if possible. I emailed a professor working on compilers, some lab in D.C., maybe a couple others I don\u0026rsquo;t remember. Funnily enough, only my first email got a response, the one I was most hoping to hear from\u0026mdash;Nathan Schneider at Georgetown University, who worked (and still works) on computational linguistics. About a week passed between the email and the response, and then I responded immediately on July 2nd. I didn\u0026rsquo;t hear back for 3 weeks, and got pretty worried because I was starting to get excited about it! Fortunately it worked out, and I got to meet Dr. Schneider and his Ph.D. student Luke Gessler on August 5th, 2020.\nMy first research project was on computationally modelling schwa deletion in Hindi, i.e. throwing machine learning methods at the problem and hoping it works.2 This was my first time doing a real machine learning project. I had played around with neural networks once before, for my final project for the online class CS50x where I first learned programming. It was a really dumb neural network for playing Connect 4, and since I didn\u0026rsquo;t know what backprop was I implemented \u0026ldquo;genetic algorithms\u0026rdquo;, a fancy way of saying I tried a bunch of random initialisations for the network weights and had them play each other. It was really fun. But I digress; now, I got to apply neural networks to a real problem.\nThe project was pretty successful if you look at the metrics. We beat out a lot of previous hand-crafted methods in the literature and I got to write my first paper\u0026mdash;well, it was more like \u0026ldquo;learned to write a paper from the example of Luke and Nathan and wrote a few things here and there myself\u0026rdquo;\u0026mdash;which ended up getting accepted at ACL. But I would not say I fell in love with research at that point. I liked the work and I definitely preferred it to schoolwork. I guess I would say that I liked working on hard problems, and this was a hard problem. Solving it was satisfying.\nThe first research project I did where I truly felt driven by my own curiousity about it was extending SNACS to Hindi. I spent the summer of 2020 working on SNACS annotation of the English version of The Little Prince with some of the grad students at Georgetown (I had gotten accepted there and committed for undergrad). I got the idea in May to try and extend this to Hindi, mostly out of curiosity as to whether it would really work, since Hindi\u0026rsquo;s case markers were way more complex than English prepositions. I wrote up a basic annotation guidelines document, did a bunch of example sentence annotations myself. Research is basically self-nerd-sniping and I had nerd-sniped myself for the first time. In fact, I did not realise that this is what happened until I sat down to write this. Unlike most things I had done before then, I really just was curious about this and had no ulterior motives driving my desire to work on the problem.\nFrom that point onwards, everything that I have worked on has been motivated by my own interest and love for the problem. Sure, there are frustrating moments\u0026mdash;sometimes there\u0026rsquo;s a seemingly unsolvable bug in the code, sometimes revising papers and dealing with collaborators is a pain, sometimes the problem doesn\u0026rsquo;t feel exciting. But, like any loving and healthy relationship, if it is worth doing you slog through the tough parts and are rewarded for it. It seems to me that the motivations for working on research should be a lot of curiosity (\u0026ldquo;Why?\u0026rdquo;), a belief in the importance of the problem, and a little bit of dissatisfaction with the status quo.\nI basically just started research and don\u0026rsquo;t consider myself particularly good at it. I am still developing my research taste. I don\u0026rsquo;t think I\u0026rsquo;ve worked on particularly exciting problems thus far. However, I find this fact very exciting because it means the ceiling is so high I can\u0026rsquo;t see it, and I have room to grow and much cooler things left to work on. But even beginner researchers get to see amazing things\u0026mdash;if you spend even a little time with information theory you get to grasp a little of the beauty of math, if you read the transformers paper and go through the implementation yourself the insights behind it leave you in awe. Especially working on problems in AI or adjacent areas right now, you get a glimpse of the future and it feels like there are too many exciting problems to choose from right now.\nThe rest of my thoughts on research were already better articulated by Richard Hamming.\nI think it is very definitely worth the struggle to try and do first-class work because the truth is, the value is in the struggle more than it is in the result. The struggle to make something of yourself seems to be worthwhile in itself. The success and fame are sort of dividends, in my opinion.\nI truly do not know now why I didn\u0026rsquo;t really want to cold-email people for research opportunities then. Maybe I thought it would be embarrassing? Cold-emailing is great.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI guess many of the everyday things we work on in NLP are of this nature.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2023-03-28-research/","summary":"I started doing research in the summer before my senior year of high school, the summer of 2019. I\u0026rsquo;m closing in on 4 years now. The time I have been doing research subsumes the COVID-19 pandemic as well as my entire undergraduate education at Georgetown. I was 16 when I started doing research and I am 20 now. If I have an expected lifespan of ~80 years, then I hope I get to have 60 more years of research left in me.","title":"Me and Research"},{"content":"Unless you have been living under a rock for the last five years, you have definitely (if possibly unknowingly) somehow interacted with a machine learning model that uses the transformer architecture. I have spent a couple months poking at little transformer models like GPT-2 and the 19 million-parameter version of Pythia and yet after working at an interpretability startup for a week I realised that I actually don\u0026rsquo;t have a great understanding of how a transformer works. I was really only superficially familiar with the computations that happen within them and where weights are stored and so on. So this is a mishmash of some thoughts I have about how they work, some of which are hard to empirically verify and may be pure speculation. No claims of originality here either.\nAttention Attention is not really the best name for understanding how it works, at least in the way it is utilised in the modern transformer as a repeated and parallel operation involving multiple heads stacked in layers. I was actually surprised to see while writing this that Bahdanau didn\u0026rsquo;t really call the mechanism he designed \u0026ldquo;attention\u0026rdquo; except for briefly in one paragraph in the original paper, where he says:\nIntuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector.\nNeural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2014)\nSo it was first mentioned as an analogy rather than the all-encompassing name for it. I wonder how that ended up different.\nI think a better way of thinking about it in the context of the transformer is as information flow. Attention is the mechanism that transfers informations between fixed-memory token positions. A single attention head has the dual job of (1) figuring out from where to where information needs to be moved, and (2) actually doing that operation. So, a previous-token attention head (such as head 4.11 in GPT-2 small) is really moving information about a token to the next token. Each token now includes information about its predecessor, so heads in layer 5 can attend to just one token to know about bigrams.\nIf you think about attention as moving information around, it becomes really clear why phenomena like cross-layer cooperation between heads occur. A great example is how induction heads work: they promote generations like [A][B]...[A]([B]). It isn\u0026rsquo;t possible to do this with just one attention head in one layer (assuming the distance between [B]...[A] is not fixed). A head can learn to attend to copies of heads at the current position, but it can\u0026rsquo;t simultaneously move over information from one token over. Induction heads don\u0026rsquo;t show up in one layer models because a head in a layer lower needs to create a landmark saying \u0026ldquo;I am a [B] preceded immediately by an [A], look at me.\u0026rdquo;\nBasically, the role of attention is to figure out what information to move around and how to do so. If you buy that discrete linguistic features are encoded in separable directions in embedding space (I think this is mostly true given embedding arithmetic and the ability to remove information linear-algebraically, but subject to caveats like sparsity causing superposition of features), then attention should be able to pick specific features to copy over (not just token embeddings in their entirety) thanks to the QKV transforms.\nMLP The MLP layers are annoying as hell to make sense of. Unfortunately, the vast amount of computation (and this increases with scale) happens in the MLP layers. 80% of the FLOPs in OPT-175B! Yet, the key difference from attention is that MLPs do not move information around.\nHowever, consider the recursive layer structure of a transformer. The MLPs do operate on contextual information, using whatever stuff the attention heads below them have moved into that token position. And the two-layer structure (first multiplication followed by ReLU or another non-linearity, then second multiplication) makes the MLP similar to the KV-transform in attention.\nActually, thinking about it further, I think the MLPs are doing the same thing as the post-attention matrix multiplication by OV. The only difference is the addition of the ReLU which allows mass removal of information. So maybe the MLP not only serves as a sort of lookup for information to add without looking at other tokens (like it seems to encode knowledge), but it also cleans out unimportant information with the ReLU.\nLayerNorms The LayerNorms don\u0026rsquo;t, by definition, mess with the information contained in the language model (if we fix the mean and variance of activations as constants, then it\u0026rsquo;s just a linear transformation; this is called folding LayerNorm in) but they would rescale non-basis-aligned features in weird ways so it\u0026rsquo;s unclear what they do. I don\u0026rsquo;t think they add information by themselves (it\u0026rsquo;s just rotating and scaling the embedding information around), but the other mechanisms (attention and MLP) may strategically exploit the LayerNorm to offload e.g. getting rid of unimportant information.\nConclusion I read some hyperbole once on Twitter about how transformers are the modern Turing machine or whatever. I think that\u0026rsquo;s a silly comparison if you\u0026rsquo;re comparing their importance in the history of computing, since it\u0026rsquo;s very clear which one would not exist without the other. But on the architectural level it\u0026rsquo;s not a bad comparison. The tape of the transformer is the information contained at each token position after initial embedding. Attention is the instruction table and also moves the head around and registers states. MLPs and layernorms modify things inside a position in the tape without reference to other positions, but can still leverage information the attention put in there.\nThinking about transformers this way also makes a stronger argument for scaling, since the layer of a transformer obviously limits the attention heads\u0026rsquo; chances to decide what information to move around. If you have 12 layers then you only have 12 chances to figure out what to move around and do that. Like, your max depth is 12 even if you have 144 heads total. Since we\u0026rsquo;re dealing with natural language, this is enough for most things but the long-tail and interesting phenomena get hurt. For example, this makes it pretty clear why addition would be hard for transformers, since you may have to carry over and store intermediate sums way more than 12 times when dealing with big numbers. I bet this also means adding breadth to the network (more parallel heads) will be less powerful than adding a new layer.\n","permalink":"http://localhost:1313/posts/2022-12-24-transformers/","summary":"Unless you have been living under a rock for the last five years, you have definitely (if possibly unknowingly) somehow interacted with a machine learning model that uses the transformer architecture. I have spent a couple months poking at little transformer models like GPT-2 and the 19 million-parameter version of Pythia and yet after working at an interpretability startup for a week I realised that I actually don\u0026rsquo;t have a great understanding of how a transformer works.","title":"Some intuitions about transformers"},{"content":"Combinatory Categorial Grammar (CCG) is one of the many, many (far too many) syntactic formalisms posited by linguists in the Chomskyian era. CCG has been outlined in work by Mark Steedman, the most recent guide being his 2001 book which I have to read for a class.\nUnlike most other syntactic theories, I find the mechanics of CCG very elegant. There is no difference between the distributional categorisation of a lexeme and the argument structure that it has; both are neatly contained in the CCG type system, and the connections to type theory just work out very nicely (with the caveat that this is in my limited understanding of type theory).\nNaturally, I was thinking about how CCG would be applied to Hindi syntax. This isn\u0026rsquo;t anything novel (see this 2017 paper on exactly that) but for my own edification I wanted to see how flexible CCG is across languages. Here\u0026rsquo;s CCG analysis of some simply clauses in Hindi; as you can see, all the derivations proceed backwards, which makes sense since Hindi is pretty left-branching.\nSo, those were pretty straightforward; note that I also included a weak functional representation of the semantics in the same lambda calculus of Steedman. You can see that the familiar (S\\NP)/NP of English is replaced with (S\\NP)\\NP, corresponding to SOV structure in Hindi (both verbal arguments are to the left). I do wonder how a language where the object is further than the subject would work though.\nHere\u0026rsquo;s a more complex clause, with a ditransitive verb as well as another example with non-canonical word ordering. The preferred way to analyse non-SOV orderings (which are canonical in Hindi) is with type-raising and then forward composition (a rare rightwards derivation). I am reminded of how CCG handles relative clauses in English, which have screwed-up word order due to an argument being moved out of the clause.\nOne thing I would like to explore is ergativity. Ergativity has been studied to death by Hindi linguists, unfortunately. But I like how you can easily express the verbal ergative agreement using a feature on the subject, which even works well where a serial verb overrides the agreement of a main verb. Also, check out all the nice left derivations!\nHere\u0026rsquo;s how lenā and cuknā are handled as serial verbs; the former takes the ergative and the latter does not. The backward composition operation overrides the unspecified subject type of the bare verb kar.\nDative subjects:\n","permalink":"http://localhost:1313/posts/2022-10-08-ccg/","summary":"Combinatory Categorial Grammar (CCG) is one of the many, many (far too many) syntactic formalisms posited by linguists in the Chomskyian era. CCG has been outlined in work by Mark Steedman, the most recent guide being his 2001 book which I have to read for a class.\nUnlike most other syntactic theories, I find the mechanics of CCG very elegant. There is no difference between the distributional categorisation of a lexeme and the argument structure that it has; both are neatly contained in the CCG type system, and the connections to type theory just work out very nicely (with the caveat that this is in my limited understanding of type theory).","title":"Some CCG derivations in Hindi"},{"content":"I entered the NAACL conference venue, just a couple blocks away from my place in Seattle, with the absolute lowest of expectations. I have attended two other conferences this year entirely online: ACL and LREC. At both, I had minimal interactions with other human beings beyond \u0026ldquo;standing\u0026rdquo; at my poster, and most of that time no one was even there to ask about my work. Hence, zero expectations.\nThe Multimodal ML tutorial.\nNAACL was the best conference experience I\u0026rsquo;ve ever had, no doubt about it. The posters and talks were fascinating, I learned a lot from the tutorials, the socials were both fun and a great way to meet people, and there was great food. I can easily say that I met more people at NAACL than I did in all my previous conferences combined. More than the paper talks or presentations, the value of a conference is in being able to easily meet new people who offer a diversity of perspectives. It was also great to experience how approachable even the biggest names in NLP are.\nA big theme of the conference seemed to be the uncertain role of symbolic systems, often inspired by linguistics, in an overwhelmingly neural field. There was a somewhat terse back-and-forth at the panel between Emily Bender and Chris Manning which struck at some of the fundamental points of debate in the field: what are we trying to do as computational linguists by training big language models, and could those things be done in a different way? We\u0026rsquo;ve seen a big convergence in AI in the past couple years on the topic of model architecture; every field has benefited from training transformer models on massive amounts of unlabelled data and finetuning them on (or prompting them about) data-limited and specific tasks. But maybe this convergence comes at the cost of dismissing alternatives. (I\u0026rsquo;m not sure I explained that very clearly.)\nProbably the most important thing I took away is that NLP is much bigger than computational linguistics. I consider myself a linguist; I find language absolutely fascinating and its existence and development pretty much insane. Understanding how the complex and intricate system that makes up language works is satisfying. But, linguistics is a very confused field—the more I learn about, the less I can get what it\u0026rsquo;s trying to say or whether its story of language is coherent. It\u0026rsquo;s hard to say what its place is at the cutting edge of NLP, where unsupervised systems reign supreme now. Seeing the wealth of development in e.g. multimodal ML, which was barely a thing two years ago in my view, has convinced me that looking beyond linguistics is exciting too.\nAnyway, I don\u0026rsquo;t think my narrative of this conference is going to form a coherent whole either, so I\u0026rsquo;ll just note down random experiences that I remember from the conference.\nEating pho with Justus Mattern (differential privacy), one of the limited number of undergrads at NAACL, before the conference. Us and Rohan Pandey (semantic parsing/linguistics in NLP) ended up attending much of the conference and socials together. Gasworks. One of my favourite views of Seattle.\nWalking aimlessly around Seattle with Machel Reid (low-resource MT, LM things), also before the conference. Talking to Dzmitry Bahdanau (first author on the paper that introduced attention; a real fanboy moment) at the MoPOP social. I am proud of maintaining surprisingly high-level conversation which I learned a lot from—one thing I\u0026rsquo;ll keep thinking about is his mention of the \u0026ldquo;high-risk, high-reward\u0026rdquo; challenge of beating transformers. The Museum of Pop Culture.\nTalking to Venelin Kovatchev about Bulgarian for half an hour, and randomly more throughout the conference. Having lunch at a Sichuanese restaurant with Eleanor Jiang and some others. This was funny to me because I ended up meeting more ETH-affliated people at NAACL than I did while actually at ETH, due to the work-from-home policy last summer there. Learning about embeddings in hyperbolic spaces from Jun Takeuchi\u0026rsquo;s excellent poster at the SRW. This is an idea I had could have never even known to look for, and the presentation was very well done. Meeting Matyáš Boháček, a 10th-grader (!!) who does research in NLP and ML broadly. As someone who got into research early too but not that early, this kind of thing is very humbling. Talking to Paul Smolensky (the inventor of optimality theory; again a fanboy moment) about the panel and randomly chatting like normal people throughout the conference. Talking to Shaily Bhatt during the workshops. In general, it\u0026rsquo;s cool to see the exciting new work and opportunities going on in India, especially on NLP for Indian langauges. Also was really great to finally meet Monojit Choudhury in-person at the social. Seeing Ruibo Liu\u0026rsquo;s paper on AI alignment. I have thoughts™ on alignment as a research programme, but anyway this made me think a lot about how we evaluate generative language models on free-text generation. Visiting the AI2 social (which we somehow got into despite not being able to RSVP) and joining random circles of people and talking about everything. I really enjoyed talking to Ashish Sabharwal and Zhijing Jin. AI2 also had a great selection of cheeses. Playing some game whose name I do not know (it was a bit like curling on a table?) at the DADC rooftop social, with great views of Seattle. I talked to Martijn Bartelds (Dutch dialectology) and a bunch of UCSC people. A view of Seattle from the DADC rooftop.\nThe talk for \u0026ldquo;Do Prompt-Based Models Really Understand the Meaning of their Prompts?\u0026rdquo; by Albert Webson and Ellie Pavlick. I had seen an earlier version of this at MASC-SLL. This affirms, to a degree, the intuitive suspicions that arise about the effectiveness of prompting. But of course, we have work pointing the other way too now, with chain-of-thought prompting. In general, the conference left me with a lot of new ideas and uncertainties. I was definitely affirmed about my choice of sticking towards research. It was exciting to see all the new things happening at NAACL! There is a unique wonder of being at the cutting edge of a rapidly-maturing field. There is a dizzing array of directions to work towards, in uncharted territory, even for the least experienced researchers. At the same time, the results of new work are immediately apparent due to the inherent scalability of software. To me, this is the best of both worlds.\nOn the other hand, I am increasingly disappointed in the place of linguistics in this new work. Language is a beautiful and complex system, grounded in social interaction, but it does not feel like the direction that NLP is going in really is benefitting from the work of linguists. I think my disillusionment is not with NLP though. NLP is successful. Computational linguistics is successfully using the engineering breakthroughs of NLP to better understand language. But the way in which this is progressing is totally divorced from the field of linguistics, which makes linguistics feel like a bit of a dead end.\nAnyway, NAACL was great, and these are exciting times in the field.\n","permalink":"http://localhost:1313/posts/2022-08-04-naacl/","summary":"I entered the NAACL conference venue, just a couple blocks away from my place in Seattle, with the absolute lowest of expectations. I have attended two other conferences this year entirely online: ACL and LREC. At both, I had minimal interactions with other human beings beyond \u0026ldquo;standing\u0026rdquo; at my poster, and most of that time no one was even there to ask about my work. Hence, zero expectations.\nThe Multimodal ML tutorial.","title":"NAACL 2022"},{"content":"I have been fascinated by the ACL 2022 Best Paper, Learned Incremental Representations for Parsing by Nikita Kitaev, Thomas Lu, and Dan Klein. I had the good fortune, wandering around in Gather.town, to attend the poster session virtually before the award was announced. The big thing that the paper showed is the tractibility of human-like incremental parsing, which seems to have been pretty much a dormant problem in the field ever since full-sentence language models started dominating all the benchmarks. What\u0026rsquo;s especially crazy is the numbers Kitaev et al. produce with only left context for their parsing model beat state-of-the-art whole-sentence models from just a couple years ago.\nKitaev and Klein have done a ton of great work on syntactic parsing in the past, e.g. this paper which introduced attention to constituency parsing in 2018, or this one which Klein is an author on, achieving SOTA on constituency parsing with a neural chart-parsing model for (I think) the first time. Thomas Lu seems to have been a fourth-year undergrad at Berkeley at the time when the paper was submitted—insanely cool.\nWhat I found exciting about the paper was two things: (1) vector quantisation, a technique to induce discrete values in the model pipeline while still having the system be end-to-end differentiable; and (2) the unsupervised learning of a pretty small inventory of token labels, whose distribution is worth investigating. I\u0026rsquo;ve thought about a couple of tasks that would need learned discrete labels as an intermediate step, so learning about VQ was a huge deal for me (and led me down the rabbithole of variational autoencoders recently). But my main topic here today is the learned tagset itself.\nWhile checking out their code, I was excited to see that the model can be run in Colab extremely easily, so naturally I decided to poke around at it. The purpose of this post is to list some things I figured out about the learned 32-token tagset—I was curious how it compared to the usual part-of-speech tagsets that we use. Note that while they largely discuss the 256-token tagset with an F1 score of 94.49, the 32-token one is still really good with a score of 93.50.\nPrepositional phrases One of the first things I thought to poke at were prepositional phrase attachment, since it was mentioned in the paper. One thing became pretty clear: the thing this tagset is trying to do is very different from what conventional POS tagsets are. Part-of-speech tags intends to group lemmata with similar distributions together; e.g., the Cambridge Grammar of the English Language designs its POS tagset by making arguments about distribution, coordination scoping, and similar linguistic considerations—this is how it ends up grouping some conventional subordinators with prepositions, due to shared acceptance of some clausal dependents.\nThis tagset, on the other hand, is learned as the sole input in order to parse syntax, and that too with only the benefit of left-side context. A normal POS tagset, I wager, is not informative enough to generate accurate syntax trees by itself.\nA common feature is a distinction between leading (initial) and non-leading tags, sort of like the B and I tags of BIO. This makes sense; a leading tag signals the start of a new constituent, say an NP, and a non-leading tag signals its continuation. But things were not so simple as to let there be only two tags for indicating an NP. Below are some minimal pairs I found for PP-attachment tags.\n1 2 Ex. PP attachment (NP vs. VP) [24, 11, 0] [4, 16, 25] There is some cheese {from, in} the farm [24, 11, 0] [4, 11, 7] I saw the {man, plant} with a telescope [24, 11, 21] [4, 28, 21] I drew pictures {of, with} my kids Presence of preceding VP-internal constituent [17, 11, 7] [4, 28, 21] I drew {∅, pictures} with my kids [17, 28] [4, 28] He is {∅, truly} in fear [17, 28, 21] [4, 11, 21] He is {∅, truly} in the farm ? [17, 11] [17, 28] He is in {France, fear} [17, 11, 7] [17, 11, 21] I am on the {internet, edge} Table 1: Tags outputted for some PP attachment minimal pairs.\nSo we see that the tag on the preposition really does signal where it\u0026rsquo;s attaching: to the preceding NP (4), or to the VP (24). Also, there is another tag for attaching to the VP when there is no preceding NP in the way (17).\nThe nouns are sort of screwed up though. There is a clear distinction between leading and non-leading tokens, but it\u0026rsquo;s not just two tags involved. We see all of the following: [11, {0, 7, 21}], [28, 21], [16, 25]. I think this is signalling some kind of syntactic tendency embodied by these determiners/nouns, to use for potential dismabiguation as more tokens to the right are consumed by the encoder. I\u0026rsquo;m not sure what that tendency is though. Really interesting to find nevertheless!\nAttempts to describe each tag Because I am excellent at making use of my time, I tried to figure out what each tag was conveying syntactically. Some of these descriptions are very rough and were before I systematically tried out some of the PP-attachment stuff, so do not take them as authoritative. I also could not get it to produce all of the tags since I was only using it manually (it seemed like a fun challenge). An actual analysis should probably run the tagger on the original paper\u0026rsquo;s test set.\nNon-leading NP elements but nested in something. This contrasts with 7; if 0 is used, the PP attaches to the NP preceding, but with 7 it attaches to the VP as an adjunct.\nI saw a man with a book Commas.\nWhen I was young , I had no brains Fronted verb or auxiliary verb in WH-questions.\nWhat have you been doing How dare you ! Final punctuation.\nI hate commas . Prepositions, complement to VPs. Interestingly, to (in the infinitive) falls under this category as well!\nThere is some cheese [in the fridge]PP I did this [with the goal of saving you]PP I came here [with him]PP [on the run]PP I did this [to save you]S Non-leading elements of QPs.\nIt happened [almost 20]QP years ago Verbs in main clauses. Your least fancy verb in the sentence goes under this category. Interestingly, this includes auxiliary verbs in the clause too. This leads to chains of 6s.\nLuna smashed the pumpkin You run I will be drawing a picture of him He said that you are mean NP complements of PPs.\nThe power of the people is magnificent Inside a potato is a man I am a man of the city I saw the planet with a telescope Some kind of non-leading NP element (compounds?)\nWhat are the consequences of artificial intelligence ? Conjunctions.\n[You or me]NP will go Verbs in nominalised clauses.\nEating food {is, seems} epic Yelling at people is mean Kind of cursed, but also to when indicating a complement clause.\nI want to be the best Adjectives in predicate position.\nI am hungry Leading elements of non-subject NPs. This one is super interesting. So, if the object NP has a determiner, then that determiner is labelled 11. But, if there is no determiner in that NP then it\u0026rsquo;s just the nominal labelled that! This makes sense given only left context is available to the model; this signals the left-most boundary of an object NP.\nThe man eats the food The man eats food Also seems to include PP objects.\nA hungry man is in my kitchen Verbs in SINV (inverted clauses), as well as where the subject is a VP or NP with relative clause. Seems to be an indicator to head back up to the S constituent after a pretty complex subject.\nBehind every door is a fridge Eating food is epic The man who I saw smells funny Verbs in adjunct clauses.\n[When Luna smashed the pumpkin]SBAR , she hurt her foot Verbs in WH-subject clauses which cause movement.\nWho are you What [have you done]SQ What have you been doing ?\nLeading NP elements of subjects of adjunct and complement clauses.\nWhen the man was young he lived here He said that you are mean Leading elements of main-clause subject NPs. Interestingly, this behaves the exact same way as 11, indicating the leftmost token of a subject NP.\nThe man eats the food He eats the food Also includes subjects of SQ which end up post-verbal.\nHow dare you ! Preposition that heads a PP adjunct to the verb.\nI am [inside a potato]PP We live [for the delicious things in life]PP Yelling [at people]PP is mean Adverbs (spatial adjuncts, adjective modifiers).\nHe lives here Life is really weird Adverbs (verbal adjuncts in general).\nHe quickly handed me the potato Leading subjects of WH-questions and main clauses with preceding WH-adjunct.\nWhat have you been doing When the man was young he lived here Adverbial modifiers to S. These nest directly under S, no intermediate projections.\nHe threw up today Leading direct objects in those constructions where the indirect object isn\u0026rsquo;t in a PP.\nHe quickly handed me the potato Non-leading NP elements. These seem to all be non-leftmost elements of the (flat) NP that are nested under a VP or S directly. This contrasts with 25.\nThe man [eats [the food]NP ]VP [[A hungry man]NP has arrived]VP In copular clauses, inside the predicate NP following the determiner (labelled 28), the nominal is labelled 21 or 25. It seems to be 21 if inanimate (or tending towards inanimate?) but 25 if animate. Honestly, I have no clue what\u0026rsquo;s going on here. Maybe the sentence I am testing are two short relative to the training data so they confuse the parser, but this animacy thing seems to have a point to it!\nI am a {pumpkin, cheeseburger, telescope} Complementiser.\nHe said that you are mean WH-words. This is used regardless of where they are syntactically, in a question, in a relative clause, or as an adjunct clause (with when) etc.\nWhat have you done Who are you Why are you hitting yourself The man who I saw smells funny Prepositions.\nThe cat in [in the hat]PP strikes back 25 is used for a variety of non-leading NP elements inside the VP. One is the first NP with a PP complement. Another is the NP inside a PP following the object of the verb, which seems entirely counterintuitive.\nThere is [[a man]NP [in the fridge]PP ]NP We live for [[the delicious things]NP in life]NP There [is some cheese [in [the fridge]NP ]PP ]VP I have found yet another cursed contrast here though, see 21.\nI am a {man, woman, lady, baby, machine, device} Verbs in relative clauses.\nShe ate the pumpkin [that Luna smashed]SBAR [What you said]SBAR was wrong ?\nNP-leading predicate of a copular clause. Also in some PPs?\nWe are men (of the city) I am a man (of the city) I did this [with the goal of saving you]PP Honestly just a grab bag of weird NPs.\nI am [[four feet]NP tall]ADJP Non-leading NP element that is a modifier to an ADJP.\nI am [[four feet]NP tall]ADJP ?\nParticles.\nHe threw up everywhere The adverb ago.\n[6 years ago]ADVP he got here References Nikita Kitaev and Dan Klein. 2018. Constituency Parsing with a Self-Attentive Encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Association for Computational Linguistics. Nikita Kitaev, Thomas Lu, and Dan Klein. 2022. Learned Incremental Representations for Parsing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3086–3095, Dublin, Ireland. Association for Computational Linguistics. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Minimal Span-Based Neural Constituency Parser. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 818–827, Vancouver, Canada. Association for Computational Linguistics. ","permalink":"http://localhost:1313/posts/2022-06-21-kitaev/","summary":"I have been fascinated by the ACL 2022 Best Paper, Learned Incremental Representations for Parsing by Nikita Kitaev, Thomas Lu, and Dan Klein. I had the good fortune, wandering around in Gather.town, to attend the poster session virtually before the award was announced. The big thing that the paper showed is the tractibility of human-like incremental parsing, which seems to have been pretty much a dormant problem in the field ever since full-sentence language models started dominating all the benchmarks.","title":"A machine-learned syntactic tagset?"},{"content":"Hindi is the best-studied language in South Asia. It would not be the worst thing if every linguist working on Hindi decided to take a break and pick any other language of the region to study. Nonetheless, Hindi does not set a relatively high bar for linguistic investigation when compared to other languages of the world; there is plenty that simply hasn't been described in any work by a linguist, let alone analysed or explained.\nI've gone through several grammars of Hindi in my decade-long quest to recover something close to native-speaker status in my L1. (This is a not uncommon frustration among Indian-Americans.) My favourite is Kachru (2006), which covers many aspects of the language in a fairly accessible format. It was the first linguistic grammar I ever read and probably the only one I've ever gone through cover-to-cover. But surprisingly, there are many strange properties of Hindi morphosyntax that I have not seen described in any comprehensive grammar, and none of the papers I've read. This is an attempt to document some of these verb forms. Imperatives are also attested.\nV[pfv] karnā This construction is used to express a very habitual action. I have largely seen it in the past tense; it seems weird to use it with other tenses, and web searches informally appear to only attest the past tense.\npūr-e din mẽ ḍeṛʰ rupa-e k-ā ek vaṛā-pāv kʰā-y-ā kar-t-ā tʰ-ā\nwhole-M.OBL day LOC 1½ rupee-OBL GEN-M.SG one vada.pav eat-PFV-M.SG do-IPFV-M.SG be.PST-M.SG\n“In the whole day, he used to eat a single 1½-rupee vada pav.” (Dai̯nik Bʰāskar)\nhāy mar jā-ẽg-e / ham to luṭṭ jā-ẽge / ai̯s-ī bāt-ẽ kiy-ā nā kar-o\nsigh die go-FUT.1PL-M.PL / 1PL TOP be.looted go-FUT.1PL / such-F.PL talk-PL do.PFV-M.SG NEG do-IMP.2PL\n“Oh, I\u0026rsquo;ll die / I\u0026rsquo;ll be made worthless / Don\u0026rsquo;t keep saying such things!” (Āj jāne kī zidd nā karo, Farīdā Xānum)\nWhat is weird is that the main verb takes the perfective while the light verb takes the imperfective, which makes no sense at all since that assigns conflicting aspectual values to the event! That violates one of the usual properties of serial verb constructions universally: shared aspectual features on all verbs in the SVC.\nThe usual way to express a habitual action in Hindi is the imperfective (see below).\nvo roz jā-t-ā tʰ-ā\n3SG every.day go-IPFV-M.SG be.PST-M.SG\n“He used to go every day.”\nThe perfective, however, is ambiguously the habitual or just a single event, without extra information. In the sentence above, the temporal adjunct roz makes it clear that the event repeats. Compare this:\nvo jā-t-ā h-ai̯\n3SG go-PFV-M.SG be-3SG\n“He goes (once/habitually).”\nPresumably, with the need to talk about an unambiguous habitual event, this kind of construction developed to fill the gap.\nV[ipfv] rahnā The history of rahnā has been studied in the linguistic literature, as a transparent grammaticalisation of \u0026ldquo;to remain\u0026rdquo; into a continuous aspect marker. It's gaining in popularity in other languages due to the influence of Hindi\u0026ndash;Urdu; see Slade (2013). What's interesting here is that it can be used with not only a bare stem, but also the imperfective-aspect main verb!\njī-te rah-o beṭ-e\nlive-IPFV stay-IMP.2PL child-VOC\n“Bless you, child.” (said when one\u0026rsquo;s child touches one\u0026rsquo;s feet)\nYou may ask again, how can one serial verb construction take two different (and contradictory?) aspectual features? I say, the weird verb morphosyntax I've been discussing makes an absolute mess of the TAM system of Hindi. But note that this grammaticalisation is identical to that of stay V-ing in African-American Vernacular English.\nV[cnv] rahnā This is transparently a grammaticaled converbial construction, where what was previously literally two separate events (V-ing and then staying put) is now analysed as a strong will towards performing the event V.\nāj kah-kar rah-ū̃g-ā\ntoday say-CONV stay-FUT.1SG-M.SG\n“Today I will say it no matter what.”\nThe grammaticalisation of converbs in Indo-Aryan languages is pretty interesting. Converbs are prototypically supposed to indicate a separate event from that of the main verb, but there are many instances in Hindi and other IA languages of a converb describing the same event as the main verb, which probably diachronically developed from a more literal reading. For example, paṛʰ-ke sunānā \u0026ldquo;to read then tell\u0026rdquo; \u0026gt; \u0026ldquo;to read out loud\u0026rdquo;.\nReferences Yamuna Kachru. 2006. Hindi. Amsterdam/Philadelphia: John Benjamins Publishing Company. Benjamin Slade. 2013. The diachrony of light and auxiliary verbs in Indo-Aryan. Diachronica 30(4): 531\u0026ndash;578. ","permalink":"http://localhost:1313/posts/2022-05-23-tam/","summary":"Hindi is the best-studied language in South Asia. It would not be the worst thing if every linguist working on Hindi decided to take a break and pick any other language of the region to study. Nonetheless, Hindi does not set a relatively high bar for linguistic investigation when compared to other languages of the world; there is plenty that simply hasn't been described in any work by a linguist, let alone analysed or explained.","title":"Secret verb forms in Hindi"},{"content":"After the fragmentation of Sanskrit, one of the innovative features that developed across the Indo-Aryan language family are the \u0026quot;pleonastic\u0026quot; suffixes, including (but not limited to) -kk-, -ḍ-, -r-, -l(l)-, and nominal diminutives -ka- (m.) and -ikā- (f.). Pleonastic means serving no semantic purpose; basically, the consensus has been that these suffixes merely served as phonological extensions to distinguish words after the collapse of many phonotactic distinctions from Sanskrit to Middle Indo-Aryan. But, it is worth re-examining that (on the surface, reasonable) claim. Even if these suffixes were meaningless at first, it is probable that they ended up serving some purpose by the chance nature of which lexemes got which specific suffix.\nSpecifically, I decided to look at the distribution of the -kk- extension in New Indo-Aryan languages. It seems to follow a pattern of creating denominal verbs from onomatopoeic nouns, which show up a bit in later Sanskrit and very commonly in MIA and afterwards. I am not the first to make this observation; that would be Emeneau (1969)1. I just want to qualify this observation with some more detailed analysis, specifically noting cases where non-onomatopoeic stems get this suffix.\nThis is in-progress.\nHindi Movement-related verbs:\naṭaknā \u0026quot;to be stopped\u0026quot; ucaknā \u0026quot;to be lifted up; jump up\u0026quot; ujʰaknā \u0026quot;to raise oneself up\u0026quot; uṛʰaknā \u0026quot;to overturn, tilt\u0026quot; kacaknā \u0026quot;to be jerked, sprained\u0026quot; kudaknā \u0026quot;to leap up, jump\u0026quot; : kūdnā ← Skt. kūrdati kʰisaknā \u0026quot;to move away, slip away\u0026quot; : kʰasaknā cipaknā \u0026quot;to stick, adhere\u0026quot; : capaknā cubhaknā \u0026quot;to plunge, duck\u0026quot; : cubhnā cʰaṭaknā \u0026quot;to slip from the grasp\u0026quot; cʰapaknā \u0026quot;to be splashed\u0026quot; cʰalaknā \u0026quot;to be spilt, splashed; overflow\u0026quot; cʰiṭaknā \u0026quot;to be scattered, sprinkled, diffused\u0026quot; Stimuli:\nkaraknā \u0026quot;to pain\u0026quot; kasaknā \u0026quot;to ache, pain naggingly\u0026quot; gaṭaknā \u0026quot;to gulp down (v.t.)\u0026quot; : guṭaknā gapaknā \u0026quot;to gulp down (v.t.)\u0026quot; gamaknā \u0026quot;to be fragrant\u0026quot; gahaknā \u0026quot;to feel a strong desire\u0026quot; gʰuṭaknā \u0026quot;to gulp down (v.t.)\u0026quot; : gʰaṭaknā gʰuṛaknā \u0026quot;to frighten, scold (v.t.)\u0026quot; ← Skt. ghurati canaknā \u0026quot;to burst, crack; be irritable\u0026quot; cabaknā \u0026quot;to throb or shoot with pain\u0026quot; casaknā \u0026quot;to shoot with pain, throb\u0026quot; ciraknā \u0026quot;to pass excreta\u0026quot; camaknā \u0026quot;to shine, sparkle; prosper; be enraged, strongly emotional\u0026quot; cilaknā \u0026quot;to glitter, sparkle; shoot with pain\u0026quot; cucuknā \u0026quot;to dry up, be parched, wither\u0026quot; cauṁknā \u0026quot;to be startled\u0026quot; cʰaknā \u0026quot;to be satiated, satisfied\u0026quot; cʰaknā \u0026quot;to be astonished\u0026quot; Verbs probably derived from onomatopoeia:\nubaknā \u0026quot;to vomit\u0026quot; oknā \u0026quot;to vomit\u0026quot; kaṛaknā \u0026quot;to crackle\u0026quot; kilaknā \u0026quot;to shout in delight\u0026quot; : kilkārnā, killānā, kilkilānā kīknā \u0026quot;to scream\u0026quot; kuṛaknā \u0026quot;to cackle, cluck; to munch (v.t.)\u0026quot; kuhuknā \u0026quot;to cry (a bird)\u0026quot; kūknā \u0026quot;to utter a shrill cry (e.g. a cuckoo)\u0026quot; ← Pkt. kukkaï kʰaṭaknā \u0026quot;to sound, rattle\u0026quot; kʰaṛaknā \u0026quot;to sound, rattle\u0026quot; kʰanaknā \u0026quot;to jingle\u0026quot; kʰuṭaknā \u0026quot;to peck, break a shell, nibble (v.t.)\u0026quot; guṭaknā \u0026quot;to coo (a dove)\u0026quot; caṭaknā \u0026quot;to make a snap or crackling sound; break with a crack, burst\u0026quot; : ciṭaknā cahaknā \u0026quot;to warble, sing (birds)\u0026quot; cuṭaknā \u0026quot;to nip, pinch, break, snap (fingers)\u0026quot; : cuṭkī \u0026quot;pinch; snapping\u0026quot; Emeneau, Murray B. \u0026ldquo;Onomatopoetics in the Indian linguistic area.\u0026rdquo; Language (1969): 274-299.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2022-05-03-kk/","summary":"After the fragmentation of Sanskrit, one of the innovative features that developed across the Indo-Aryan language family are the \u0026quot;pleonastic\u0026quot; suffixes, including (but not limited to) -kk-, -ḍ-, -r-, -l(l)-, and nominal diminutives -ka- (m.) and -ikā- (f.). Pleonastic means serving no semantic purpose; basically, the consensus has been that these suffixes merely served as phonological extensions to distinguish words after the collapse of many phonotactic distinctions from Sanskrit to Middle Indo-Aryan.","title":"The -kk- verbal extension in Indo-Aryan"},{"content":"ṭ ~ ṭh ~ l ~ ll aṅkōṭa, aṅkōṭha, aṅkōla, aṅkōlla113 \u0026rsquo;the small tree Alangium hexapetalum' l ~ ll *avala, *avalla819 \u0026lsquo;contrary\u0026rsquo; avalīyatē, *ullīyatē833 \u0026lsquo;stoops; hides oneself; sticks to\u0026rsquo; (r)dr ~ ll ārdrá, *ālla1340 \u0026lsquo;wet\u0026rsquo; ārdraka, *āllaka1341 \u0026lsquo;ginger\u0026rsquo; *āllabhr̥ṣṭa1408 \u0026lsquo;moist crop of maize\u0026rsquo; Other *allaḍa724 \u0026lsquo;childish\u0026rsquo; *allā725 \u0026rsquo;name of a tree or plant' cullī *apa-cullī420a \u0026lsquo;side or secondary stove\u0026rsquo; *ā-cullī1075 \u0026lsquo;small or secondary stove\u0026rsquo; *lulla *ā-lulla1391 \u0026lsquo;maimed\u0026rsquo; vallī amlavallī583 \u0026rsquo;the plant Pythonium bulbiferum Schott' ","permalink":"http://localhost:1313/posts/2022-02-21-ll/","summary":"ṭ ~ ṭh ~ l ~ ll aṅkōṭa, aṅkōṭha, aṅkōla, aṅkōlla113 \u0026rsquo;the small tree Alangium hexapetalum' l ~ ll *avala, *avalla819 \u0026lsquo;contrary\u0026rsquo; avalīyatē, *ullīyatē833 \u0026lsquo;stoops; hides oneself; sticks to\u0026rsquo; (r)dr ~ ll ārdrá, *ālla1340 \u0026lsquo;wet\u0026rsquo; ārdraka, *āllaka1341 \u0026lsquo;ginger\u0026rsquo; *āllabhr̥ṣṭa1408 \u0026lsquo;moist crop of maize\u0026rsquo; Other *allaḍa724 \u0026lsquo;childish\u0026rsquo; *allā725 \u0026rsquo;name of a tree or plant' cullī *apa-cullī420a \u0026lsquo;side or secondary stove\u0026rsquo; *ā-cullī1075 \u0026lsquo;small or secondary stove\u0026rsquo; *lulla *ā-lulla1391 \u0026lsquo;maimed\u0026rsquo; vallī amlavallī583 \u0026rsquo;the plant Pythonium bulbiferum Schott' ","title":"*ll in Indo-Aryan"},{"content":"One of the first unfamiliar distinctions that a learner of Sanskrit will encounter is parasmaipada vs. ātmanepada verbs. They have two different sets of morphological endings—a pain on top of the different endings for the 10 root classes—but often no obvious difference in meaning. Does it actually matter whether I use parasmaipada or ātmanepada forms? Why not stick to one to halve the number of endings I need to learn?\nThe learner will be taught that prescriptively, parasmaipada (or active-voice) verbs refer to actions done for the benefit of others, while ātmanepada (or middle-voice1) verbs are for actions done for oneself. Take the root पच् pac “cook”: if I say पचे pacē you better get your own meal because this is ātmanepada, cooking for myself. If I say पचामि pacāmi then feel free to take this food. But in reality, Sanskrit texts don’t strictly make this distinction. Many verbs will prefer one or the other for no apparent reason, and there is diachronic change in the preference towards one or the other with not clear driving factor. Perhaps this distinction was not bearing enough functional load to remain necessary over time, but awareness of the two forms, especially after Sanskrit became a formally learned language rather than an uncodified mothertongue, meant that they continued competing throughout Sanskrit’s history.\nBeyond the situation in Sanskrit, the similarity of the forms between the two voices meant that by Middle Indo-Aryan, the distinction had collapsed phonologically and did not survive (to my knowledge) to the present day in any Indo-Aryan language.\nRecently though, I noted a similar feature in Hindi, which I don’t think is really a purely morphological distinction yet like in Sanskrit, but the parallel is too interesting to dismiss. I’ll call it the reflexive causative.\nटीका लगवाओ ṭīkā lagvāo With the continuing spread of coronavirus in India as in much of the world, the government has been pushing people to get vaccinated. One tool to do that is advertising campaigns. There was a nice video from this kind of drive that I saw the other day:\nटीका लगवाया क्या?\nṭīkā lag-vā-yā kyā?\nvaccine.M.SG apply-CAUS.IND-M.SG what?\n“Did you get vaccinated?”\nOne of the things that struck me in that title2 is that the verb, despite being an indirect causative (or double causative) that can take 4 core arguments, has only 1 argument, ṭīkā “vaccine/vaccination”, indicated explicitly at all.\nA digression into how Hindi’s causatives work… Hindi, like other Indo-Aryan languages, has up to a four-way morphological gradation of transitivity/causativity on verbs. Basically, for every verb, we have up to four forms that vary in terms of how many entities are involved in that action. The obvious participants are the subject (thing doing the action) and object (action being done to it), but there are some less obvious ones that show up in this gradation. An example will show this better.\nदिख- dikʰ- “[Subj] is seen”\nदेख- dekʰ- “[Subj] sees [Obj]”\nदिखा- dikʰā- “[Subj] shows [Obj] to [IObj]”\nदिखवा- dikʰvā- “[Subj] makes [Inst] show [Obj] to [IObj]”\nThe main action in all of these is SEE, but who and what is involved, who causes the seeing, etc. varies based on what transitivity/causativity class we choose. The names of the classes, respectively, are intransitive, transitive, direct causative, and indirect causative. In the maximal case, the indirect causative, we can make up sentences like this:\nमैंने दुकानवाले से अपने दोस्त को मिठाइयाँ दिखवाईं।\nma͠i-ne dukān-vāl-e se apn-e dost ko miṭʰāi-yā̃ dikʰ-vā-ī̃\n1SG.ERG store-NMZ-OBL.SG by own-OBL.SG friend DAT sweet-NOM.PL see-IND.CAUS-F.PL\n“I made the storekeeper show my friend the sweets.”\nBack to the point. What did I find weird here? Well, we have the kind of verb that takes the maximal number of arguments, an indirect causative with 4 arguments! But here there’s just one argument. The other arguments must be implied. Well, let’s think about it. Clearly, you take the initiative in getting yourself vaccinated, someone else (like a nurse) applies the vaccine, and the vaccine is the thing applied. So we can fill the following slots:\nSubject: you initiate the applying it\nObject: vaccine gets applied\nIndirect object: you get applied to\nInstrument: nurse, doctor, health worker, etc. is made to apply it\nWith this verb, when we don’t specify the arguments besides the object (the vaccine), the subject and indirect object are implied to be the same! Meanwhile, the instrument is unspecified, there is no restriction on who it could be.\nNow, the importance of the start of this post should be clearer. This is sort of like a Sanskrit ātmanepada verb. When we don’t say the arguments, it is implied that we are applying this action to ourselves. It is easy to find examples of टीका लगवाना ṭīkā lagvānā being used this way.\n१८२ नागरिकों ने स्थानीय केन्द्रों पर पहुंचकर टीका लगवाया।\n“182 citizens reached the local centres and got [themselves] vaccinated.”\nबूथ पर लोगों ने रजिस्ट्रेशन कराने के बाद टीका लगवाया।\n“After getting registered at the booth, people got [themselves] vaccinated.”\nSo is this a new kind of language change going on in Hindi? Are we reviving the ātmanepada category? I don’t actually think so. I’m having a hard time thinking of other verbs that automatically assume the subject = indirect object in this way. If you say मैंने खाना खिलवाया “I made [someone] feed [someone else]” that kind of implication does not take place. So I’m curious what’s going on here.3\nMaybe it’s dialectal? This is not the first time I’ve thought about reflexive causatives in Hindi. There was a conversation I had on a Wiktionary talk page with Itsmeyash31, who was working with me in cleaning up the code for generating Hindi conjugation tables there. He claimed his dialect of Hindi, from the “eastern” area (so somewhere in Uttar Pradesh) frequently used reflexive direct causatives. So verbs like मरा- marā- “to get killed” could imply the subject being the object.\nI don’t think the examples he presented totally sounded grammatical for my Delhi Hindi dialect.\nतुम बेचाओगे। (tum becāoge.) — You will get (yourself) sold.\nवो चीज़ें आज ही बेचाएँगी। (vo cīzẽ āj hī becāeŋgī.) — Those things will get sold today itself.\nतुम पलटाओगे। (tum palṭāoge.) — You will get (yourself) flipped over. / You will get (someone/something) flipped over.\nतुम पिटाओगे। (tum piṭāoge.) — You will get (yourself) beaten.\nI can think of sentences like तू लुटवाएगा “you’ll get us scammed”, तू मरवाएगा “you’ll get us killed” which imply the object, but it’s not the same, since these are with indirect causative, which just sounds more natural here to me. Ultimately, I think the issue comes down we don’t have data about this kind of usage so we can’t really say whether it’s dialectal, restricted to particular verbs only, restricted to spoken colloquial language, etc.\nBut anyways, I hope you found that interesting and learned a bit about Hindi verbs behaving weirdly.\nThere are also passive-voice verbs, but they do not neatly fit in this paradigm since they’re derived with the suffix -ya- and then the person/number endings, and they are not really in competition like the active- and middle-voice forms are.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs a linguist, it’s inevitably a really trivial thing that seems interesting to me.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf I knew, I’d probably just write a paper.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2022-01-12-reflexive/","summary":"One of the first unfamiliar distinctions that a learner of Sanskrit will encounter is parasmaipada vs. ātmanepada verbs. They have two different sets of morphological endings—a pain on top of the different endings for the 10 root classes—but often no obvious difference in meaning. Does it actually matter whether I use parasmaipada or ātmanepada forms? Why not stick to one to halve the number of endings I need to learn?","title":"Reflexive causatives in Hindi"},{"content":" uh-ē sabʰa hal-y-ā v-y-ā rāṇā\n3SG.DIST-PL all walk-PFV-M.PL go-PFV-M.PL ?\nThey all left, sons of the soil\ncʰaɗ-ē gʰar pahinj-ā dāṇā\nabandon-SUBJ.3.SG home.M REFL.GEN-M.PL ?\nLeaving their ancestral land\nvakʰar-a vikʰ-a visār-ē sār-ā\nall? step.M-PL forget-SUBJ.3.SG all-M.PL\nForgetting all the footprints, never looking back\nsakʰara sukʰ-a sār-ē pyārā\nSukkur comfort.M-PL all-M.PL dear\nLeaving behind Sukkur, comforts, and their homeland\nmānī mānu=vār-ī macʰī j-e sā̃ palō\nbread.F honour=relating-F fish GEN-? with pallo\nBread of honour, Pallo fish in side\nkōkī kō pacā-e kō lasī sāṇu lōlō\nkoki some cook-3SG some lassi with lolo\nSome used to eat koki, some ate bread with lassi\npiṇu tā jantā uhē yādu sār-ā\nalso ? ? 3SG remember all-M.PL\nWe remember all those\nsānjʰi j-ī mahafil j-ā suru ūē pyālā\nevening GEN-F.SG gathering GEN-M.PL music.note ? ?\nmusical evenings\njamālo jatanu sā̃\nevening GEN-F.SG gathering GEN-M.PL music.note ? ?\nmusical evenings\nA website for learning Sindhi: learnsindhi.com. 1sg 2sg 3sg 1pl 2pl 3pl pres.m kʰā̃ tʰō kʰāē̃ tʰō kʰāē tʰō kʰāū̃ tʰā kʰāō tʰā kʰāē̃ tʰā pres.f kʰā̃ tʰī kʰāē̃ tʰī kʰāē tʰī kʰāū̃ tʰīū̃ kʰāō tʰīū̃ kʰāē̃ tʰīū̃ pres.cont.m kʰā̃ payō kʰāī̃ payō kʰāē payō kʰāū̃ payā kʰāō payā kʰāī̃ payā pres.cont.f kʰā̃ payī kʰāī̃ payī kʰāē payī kʰāū̃ payū̃ kʰāō payū̃ kʰāī̃ payū̃ pres.pfv.m halyō āhiyā̃ halyō āhī̃ halyō āhē halyā āhiyū̃ halyā āhiyō halyā āhan pres.pfv.f halī āhiyā̃ halī āhī̃ halī āhē halyū̃ āhiyū̃ halyū̃ āhiyō halyū̃ āhan m.sg f.sg m.pl f.pl pres.pfv kʰādʰo āhe kʰādʰī āhe kʰādʰā āhan kʰādʰīū̃ āhin ","permalink":"http://localhost:1313/posts/2021-09-07-sikk/","summary":"uh-ē sabʰa hal-y-ā v-y-ā rāṇā\n3SG.DIST-PL all walk-PFV-M.PL go-PFV-M.PL ?\nThey all left, sons of the soil\ncʰaɗ-ē gʰar pahinj-ā dāṇā\nabandon-SUBJ.3.SG home.M REFL.GEN-M.PL ?\nLeaving their ancestral land\nvakʰar-a vikʰ-a visār-ē sār-ā\nall? step.M-PL forget-SUBJ.3.SG all-M.PL\nForgetting all the footprints, never looking back\nsakʰara sukʰ-a sār-ē pyārā\nSukkur comfort.M-PL all-M.PL dear\nLeaving behind Sukkur, comforts, and their homeland\nmānī mānu=vār-ī macʰī j-e sā̃ palō\nbread.F honour=relating-F fish GEN-? with pallo","title":"Sikk glossed"},{"content":"The basic idea of Greater Magadha is that it was a culture separate from Vedic/Brahmanical Hinduism focused around the Magadha region, roughly around present-day Bihar and neighbouring areas. This culture did not hold the supremacy needed for it to be documented as well as Brahmanical Hinduism has been, but we can find clues of its existence through its interactions with Brahmanical Hinduism and traces in the other religious traditions, such as Jainism and Ājīvikism.\nBased on the known philosophies of Ājīvikism and early Jainism, we can ascertain that there was a religious belief in inevitable karmic retribution for all of one\u0026rsquo;s deeds, and the only solution to this suffering was the total cessation of action (expressed in both through voluntary starvation). One of the examples of this philosophy is the Bhagavad Gītā, which shares with Ājīvikism the idea that the body will act out its dharma (in the Gītā only once self-realisation is attained, but inevitably so in Ājīvikism) and is separate from the soul.\nBuddhism actually is not a continuation of this strain of thought since it says that karmic retribution is given only for deeds acted upon with desire (tṛṣṇā) and liberation from this desire is the path to freedom from the cycle of rebirth. Other features of Greater Magadhan culture are:\nthe use of round sepulchral mounds in their funerary practices a medical tradition based on diet and preparation of medicines with herbs etc. not unlike Āyurveda (contended to be a non-Vedic tradition, since it is unlike the usual Brahmanical treatments with chants and amulets found in the Vedas) Kapila (an ascetic god and probably an Asura) is worshipped cyclical time (tied to the idea of the cycle of rebirths) “It will be clear from the above that there was such a thing as Vedic asceticism during the late-Vedic and early post-Vedic period, and perhaps already before these two. This asceticism pursued different aims from the asceticism practised in Greater Magadha, and has to be distinguished from the latter.\u0026quot; (p. 84)\nPatañjali saw the two—Brahmins on the one hand, all those covered by the term Śramaṇa on the other—as two groups of people who were at loggerheads. This is of course precisely what we would expect, given the cultural division of northern India at his time. (p. 85)\nThe Āpastamba Dharma Sūtra (c. 600\u0026ndash;300 BCE) interestingly disparages the parivrāja (wandering ascetic, who thinks on the nature of the self) path of life (among four options for a Brahmin), favouring the life of a gṛhastha. Its description of the vānaprastha is very much like early Jainism (inaction and silence, reduction of consumption to nothing). And it gives a second type of vānaprastha who is like a Vedic ascetic. Megasthenes\u0026rsquo;s accounts also confirm this situation: one Vedic ascetic and two non-Vedic.\n\u0026ldquo;In an enlightened one there is obtainment of peace.\u0026rdquo;\nThat is opposed to the scriptures.\nIf there were obtainment of peace in an enlightened person, he would not experience pain even in this world.\nĀpastamba Dharma Sūtra 2.21.14\u0026ndash;16\nWhere the Jainas believed that the suffering engendered by a radical immobilization of body and mind would destroy the traces of deeds carried out earlier, the Ājīvikas did not accept this as a possibility. For them there was no shortcut to liberation; the full karmic burden of past deeds had to exhaust itself by bringing about results, and this gave rise to a long series of innumerable lives, at the end of which the person would reach liberation. (p. 105)\n","permalink":"http://localhost:1313/posts/2021-05-13-greater-magadha/","summary":"The basic idea of Greater Magadha is that it was a culture separate from Vedic/Brahmanical Hinduism focused around the Magadha region, roughly around present-day Bihar and neighbouring areas. This culture did not hold the supremacy needed for it to be documented as well as Brahmanical Hinduism has been, but we can find clues of its existence through its interactions with Brahmanical Hinduism and traces in the other religious traditions, such as Jainism and Ājīvikism.","title":"Greater Magadha"},{"content":"There is a weird developing cultural trend in some circles of treating Hindi as a sole inheritor of Sanskrit. Obviously, this is tied to the political centralisation around certain religious ideologies, which seem to have an obsession for a singular national language (invariably Hindi). But a lot of the arguments for this special status of Hindi are grounded in linguistic nonsense. I responded to one instance of this on Twitter, but it was a long thread that isn\u0026rsquo;t really nice to view on there so this blog post is a form of that.\nFirst of all, I will say that I love Hindi. All of my research work is tied to it right now and I expect much of it will continue to be. I had a phase like many children of immigrants do where I was too shy/embarrassed to speak in Hindi and so my language skills atrophied. But I basically reversed that process in high school. So, I don\u0026rsquo;t have any agenda against Hindi, just annoyed by the misinformation.\nSecond, I\u0026rsquo;m only touching on phonetic and phonological differences. This is one tiny facet of a language and its speech community; a full overview of the differences is probably necessary but I thought to star with sounds since those are basic and also subject to a lot of misinformation (\u0026ldquo;Hindi has the most scientific alphabet\u0026rdquo;, \u0026ldquo;Hindi is written exactly like it sounds\u0026rdquo;, \u0026ldquo;Hindi sounds are identical to Sanskrit\u0026rdquo;, etc.) Now to the actual discussion.\nVowels First off, the vowels. Hindi has gotten rid of Sanskrit\u0026rsquo;s phonemic length and replaced it with quality distinctions in monophthongs. Here\u0026rsquo;s the basic vowels compared between Hindi and Sanskrit.\nLetter Hindi Sanskrit अ /ɐ/ /ɐ/ आ /ä/ /äː/ उ /ʊ/ /u/ ऊ /u/ /uː/ इ /ɪ/ /i/ ई /i/ /iː/ Also note schwa deletion in Hindi. Where we Hindi-speakers pronounce भारत, आनंद, etc. as bʰārat, ānand, in Sanskrit all of the orthographic schwas were always spoken (=\u0026gt; bʰārata, ānanda) unless there was a virāma (=halant). Eastern IA is more conservative in keeping these.\nThe guṇa vowels ए and ओ were diphthongs in Vedic Sanskrit /ɐi̯ ɐu̯/, but were monophthongised by Classical Sanskrit /eː oː/. The latter (minus length distinction!) is how they are in Hindi. Hardly very conservative though, not like the Vedic system at all. The vr̥ddʰi vowels ऐ and औ were always diphthongs /äi̯ äu̯/ in Sanskrit. In Hindi we have reduced them to new monophthongs /ɛ ɔ/ that never existed in Sanskrit. The diphthong realisation is only retained before glides in MSH (e.g. भैया, कौवा). Bhojpuri keeps the diphthongs!\nNow to the \u0026ldquo;problem\u0026rdquo; vowels: ऋ ॠ ऌ ॡ. Only the first is common in Sanskrit, but due to the historical r ~ l alternation a couple verb roots have ऌ in their paradigms. In Sanskrit these were syllabic consonants /r̩ r̩ː l̩ l̩ː/. Think of how Americans say their \u0026ldquo;r\u0026quot;s in \u0026ldquo;butter\u0026rdquo;, but without the tongue curled so far back. Eastern European languages also have similar sounds. Basically no Indian language retains this sound; Hindi calls them /ɾɪ, ɾi, lɾɪ, lɾi/, Gujarati has /ɾu \u0026hellip;/, etc.\nTo sum up, the vowel system is quite changed from Sanskrit to Hindi, even if the way the vowel system is structured is the same in writing. Always keep in mind that writing != pronunciation! For Sanskrit, Devanagari corresponded 1-to-1 to speech, but it does not in Hindi.\nConsonants Now for consonants. There are fewer differences here. First, the lost nasal consonants ङ ञ ण. In Sanskrit these first two were nasal sounds at the place of articulation of the corresponding row in Devanagari, /ŋ ɲ/. We don\u0026rsquo;t do that in Hindi. In Hindi, we do not have the first one really; a few words like वाङमय \u0026rsquo;literature\u0026rsquo; do have it though, and we invariably insert the /g/ there: /ʋäŋgmɐj/.1 Skt would call it /ʋäːŋmɐjɐ/. The second one we call /nj/. In Skt it\u0026rsquo;s a single nasal, touching the roof of the mouth. Finally, the third one is usually just the same as न. Some people do have a different sound (especially priests when chanting Sanskrit etc.) which is like ड़ but more nasal, transcribed /ɽ̃/. In Sanskrit the flapping quality of ड़ was not there, it was a curled nasal /ɳ/.\nNow to the affricates च छ ज झ. Sanskrit might have had these as actual stops (fully blocking airflow) /c cʰ ɟ ɟʱ/ or affricates (with air) /t͡ɕ t͡ɕʰ d͡ʑ d͡ʑʱ/. Either way, these are palatal, with the body of the tongue touching the roof of the mouth. In Hindi they are totally different. It\u0026rsquo;s more of the tip area of the tongue, touch the part of the mouth right above the alveolar ridge. I transcribe these as /c͡ʃ c͡ʃʰ ɟ͡ʒ ɟ͡ʒʱ/ following Masicaa; they are less \u0026ldquo;pointy\u0026rdquo; than the English ch, j.\nNow let\u0026rsquo;s look at फ. In Modern Standard Hindi of Delhi, many people have taken to pronouncing it as /f/ (like the f of English). And indeed, this is probably due to post-independence influence of English. Go to a village, they will say /pʰ/, प with a puff of air.\nOur last single consonants to discuss are श and ष. In Sanskrit the former is palatal /ɕ/ (like च, in terms of how the tongue is shaped in the mouth) and the later is retroflex /ʂ/ (like ट). In Hindi, we have merged these to be both closer to the front of the mouth: /ʃ/.\nNow for the conjunct ज्ञ. The form in Sanskrit is supposed to be /ɟɲ/ or /ɟ͡ʑɲ/, or in Devanagari ज + virāma + ञ. In Hindi we somehow got /gj/ ग + virāma + य! There is a lot of variation in how Indian languages pronounce this; the closest is in Dravidian langs.\nFinally, क्ष is /kʂ/ in Sanskrit (क + virāma + ष). In Hindi people tend to say just श at the start of a word and क्श in the middle. We don\u0026rsquo;t have the tongue curled back like in ट. It\u0026rsquo;s extremely different from Sanskrit.\nOther letters The anusvāra (bindī) and candrabindu. In Hindi the latter nasalised vowels. The former is a nasal that assimilated to the following consonant. This is the same case as Sanskrit, except it gets weird with sonorants (sounds that allow air through). Basically, in words like संविधान or संस्कृत (व and स let air through unlike क or ड), Sanskrit would just have a nasal vowel, no actual \u0026ldquo;n\u0026rdquo; or \u0026ldquo;m\u0026rdquo; sound. Hindi behaves differently, calling them samvidhān and sanskrit.\nFinally (bear with me), the visarga in words like दुःख and सामान्यतः was /h/ in Sanskrit. (The full situation is complicated so I won\u0026rsquo;t go into it.) This is different from ह because there is no vibration of the vocal cords. But in Hindi, both are the same /ɦ/.\nConclusion To summarise, some Skt ~ Hindi examples.\nWord Sanskrit Hindi क्षत्रिय /kʂɐt̪ɾijɐ/ /ʃɐt̪ɾij(ɐ)/ ज्ञानी /ɟɲäːniː/ /gjäni/ संस्कृत /sɐ̃skr̩t̪/ /sɐnskɾɪt̪/ दुःख /d̪uhkʰɐ/ /d̪ʊkʰ/ मेष /mɐi̯ʂɐ/ /meʃ/ I won\u0026rsquo;t get much into other aspects of Sanskrit loaning in Hindi. I\u0026rsquo;ll just say that Hindi does not follow all of the rules of Sanskrit in this regard, specifically noting that we do not do proper sandhi (words like अंतर्रष्ट्रिय are misspellings).\nThe important points here are:\nHindi is not \u0026ldquo;identical\u0026rdquo; to Sanskrit in any way. There are so many intervening developments that have totally changed all Indian languages since the time of Sanskrit. No use of Sanskrit words or styles can turn back the clock on this. It is silly to compare X language with Sanskrit and say it is closer than Y language. It is even sillier to say that this is a reason for X language to be prioritised. Languages serve as lingua franca when they are easy to use by a wide range of people not by how \u0026ldquo;close\u0026rdquo; they are to an imagined ideal. There are features that Bhojpuri conserves that Hindi doesn\u0026rsquo;t. I do not see anyone arguing to make Bhojpuri the national language. Same goes for Bengali (way more Skt vocab, less schwa deletion), Punjabi (proper ण), etc. Footnotes It seems that some people do pronounce it [ʋäːŋmɐjɐ]. Either way, /ŋ/ is noenxistent as a phoneme in inherited Indo-Aryan vocabulary and marginal in other parts of the Hindi lexicon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2021-03-08-hindi-is-not-sanskrit/","summary":"There is a weird developing cultural trend in some circles of treating Hindi as a sole inheritor of Sanskrit. Obviously, this is tied to the political centralisation around certain religious ideologies, which seem to have an obsession for a singular national language (invariably Hindi). But a lot of the arguments for this special status of Hindi are grounded in linguistic nonsense. I responded to one instance of this on Twitter, but it was a long thread that isn\u0026rsquo;t really nice to view on there so this blog post is a form of that.","title":"Hindi is not Sanskrit: Phonetics and Phonology"},{"content":"Recently, I’ve been working on mapping the languages of South Asia by organising the scholarly work that has been done on individual varieties—this means sociolinguistic surveys, grammatical descriptions of standardised dialects, and any other fieldwork-based study of language. At first the goal was to organise the works that I read and reference in a way that is (1) useful to others, and (2) easy to browse through, but I came to realise that it can be useful beyond that, for more general-purpose language mapping. By plotting the locations of individual studies and dividing the map into zones around each point through some sort of nearest-neighbour interpolation1 we can create language regions, which approximate the geographical spread of languages. The result of this is Bhāṣācitra.\nA view of Bhāṣācitra.\nYou can hover to see zones and click on dots to see sublocations and references. The colour scheme is automatically generated (string to hex hashing function) so it’s a little wonky. On the tech side, this was all done through some JSON files and D3.js—I do not have the money to host a webapp for the foreseeable future and I don’t actually like coding web stuff! The data collection that went into the map was not a solitary effort: Adam Farris, Samopriya Basu, and Gopalakrishnan Ramamurthy provided a lot of the sources for Insular IA, Dardic and Pahari, and Dravidian (respectively). Some highlights:\nShina is the most studied Dardic language. Seems there is a lot of dialectal variation that makes it suitable for linguistic analysis, and there are plenty of speakers to access in both India and Pakistan. Sindhi and Lahnda are relatively understudied. The whole frontier region of Southern Pakistan is certainly not uninteresting, but there’s not much work being done there. This is surprising, given the sub-nationalist movements that have been so important to Pakistan’s history in that region—we’d expect linguistic work to occur there as these languages assert their unique identities. The Hindi region is understudied too! Cannot find sources for Chattisgarhi, Bagheli, etc. at least in English. Same for many of the Pahari lects. This is probably due to the political assertion of Hindi as the only language of the Indo-Aryan heartland, which really stifles linguistic work. The obvious issue in this approach is that linguistic work is not being done at a sufficiently fine level in South Asia to make really small zones/a very fine-grained distribution of points. (That’s not to say the there is any lack of linguistic work being done in South Asia—one of the things I’ve realised in mapping all these references is that there is plenty of work, it’s just hidden away in unpublished theses that we can only now access through Shodhganga, and papers in obscure non-digitised journals.)\nThe kind of data this map ideally would be built on can only really be collected in expansive dialect surveys, which are rare in South Asia. The only recent examples that come to mind are the SIL sociolinguistic surveys, which have only focused on a few regions of South Asia, and the Survey of the Dialects of the Marathi Language.\nStill, it’s a useful gathering of sources that I don’t think has been done for this important linguistic zone. One of the uses may be mapping areal features: here’s a prototype looking at some phonological features: the presence of contrastive breathy-voicing and historical post-nasal voicing. Other things that may be interesting to map:\nWriting systems Syntactic features, e.g. alignment systems, the use of numeral classifiers, branching Lexical-semantic features, e.g. case marking Lexicons, e.g. etymological sources for Swadesh list words or other glottochronological data There has been similar work on other regions, like Henrik Liljegren’s work on mapping and analysing the areal features and typology of Hindukush-Karakoram region languages and Erik Anonby and Mortaza Taheri-Ardali’s Atlas of the Languages of Iran mapping language distributions at the village level.\nOne of the data integrations I have thought of is extracting data from an etymological dictionary (like Turner’s Comparative Dictionary of the Indo-Aryan Languages), and, at least for phonological features, mapping out the likelihoods of individual sound changes or even a regex-based querying system for sound change. The initial extraction of data from CDIAL wasn’t so hard (it’s already digitised), but now I’m stuck on finding or making a good alignment algorithm, especially to cover things like metathesis. The usual dynamic-programming edit-distance algorithms can’t handle that very well; plus, we need to make decisions about how to measure phoneme distance (e.g. what are the distances between /pʰ p b/? /pʰ~p/ = /p~b/ or not?). I’m sure there’s work on this, but I’m totally new to it so these are just some musings. Another approach that comes to mind is something neural, sort of like the LSTM encoder-decoder in Cathcart and Rama (2020), but for alignment instead of prediction. One idea is to train contextual phoneme embeddings, and then run something like Sabet et al. (2020)’s Simalign algorithm. Maybe pretrain on manual alignments of tricky cases first.\nAnyways, there’s plenty of neat stuff to be done in this system. It’s a fun side project for now (apparently, I like making datasets), maybe will result in something publishable in a more formal format someday.\nSo far, I’ve only tried a geographic version of the Voronoi algorithm because it is easy to implement in D3.js with the d3-geo-voronoi library.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/2021-02-10-bhasacitra/","summary":"Recently, I’ve been working on mapping the languages of South Asia by organising the scholarly work that has been done on individual varieties—this means sociolinguistic surveys, grammatical descriptions of standardised dialects, and any other fieldwork-based study of language. At first the goal was to organise the works that I read and reference in a way that is (1) useful to others, and (2) easy to browse through, but I came to realise that it can be useful beyond that, for more general-purpose language mapping.","title":"Bhāṣācitra"},{"content":" Certain *ārtá: MIA āṭā \u0026lsquo;flour\u0026rsquo;, Hindi āṭā, Punjabi āṭṭā, Romani (j)aro \u0026hellip; (Turner: 1338)\nPIIr. *HārHtás \u0026rsquo;that which is ground\u0026rsquo;, vriddhi-form of *Hr̥H-tás \u0026lsquo;ground\u0026rsquo; \u0026lt; PIE *h₂elh₁- \u0026rsquo;to grind\u0026rsquo;; cf. MIA āṭā, Persian ārd \u0026lsquo;flour\u0026rsquo;, Avestan aṣ̌a \u0026lsquo;ground\u0026rsquo;, Old Armenian ałam \u0026rsquo;to grind\u0026rsquo;.\nDoesn\u0026rsquo;t seem that the r in the cluster has been preserved in any NIA language (Romani (j)aro has the usual t → r / V_V change), but lack of MIA dental attā ~ ātā supports the presence of r which causes retroflexion.\n*targá: Hindi tagṛā \u0026lsquo;robust; strong\u0026rsquo;, Bhadrawahi ṭ͡ḷagṛo, Gujarati trāgũ, Lahnda trakṛā (Turner: 5718)\n*targá \u0026lsquo;strong\u0026rsquo; + pleonastic -ḍ- \u0026lt; PIE *tergʷ-ós; cf. Ancient Greek tarbéō \u0026rsquo;to be afraid\u0026rsquo;, Sanskrit tárjati \u0026rsquo;to threaten\u0026rsquo;. All the non-Hindi cognates point to *tragá however, which may be due to metathesis.\n*bʰrūrá: Hindi bʰūrā \u0026lsquo;_brown\u0026rsquo;, Gujarati _bʰūrũ \u0026lsquo;brown, white\u0026rsquo;, Kashmiri bura \u0026lsquo;coarse white sugar\u0026rsquo;, Shina buro \u0026lsquo;whitish\u0026rsquo; (Turner: 9690)\nPIE *bʰruH-ró-s from the root *bʰerH- \u0026lsquo;brown\u0026rsquo;; cf. Sanskrit bábʰru (from diff. derivation), Persian būr. Well-documented in IA.\n*vari \u0026lsquo;speech; language\u0026rsquo;: Kalasha var, Indus Kohistani vārī̀ (Turner: 11327, Zoller: 98)\nPIE *werh₁-ís? from the root *werh₁- \u0026rsquo;to speak; say\u0026rsquo;; cf. Nuristani, Ashkun wērī, Kati werí, Prasun werī. Zoller also mentions the language name Khowar.\nPossible *stóra \u0026lsquo;pack animal\u0026rsquo;: Kumaoni tʰoro \u0026ldquo;young buffalo bull\u0026rdquo;, Nepali tʰore \u0026ldquo;full-grown buffalo heifer\u0026rdquo; (Turner: 13780)\nPIE *(s)táwros, related to Proto-Semitic *ṯawr- (Wanderwort). Cognates attested in Iranic (Av. staora), Hellenic, Italic, Balto-Slavic, Germanic, and Albanian.\nReferences Turner, Ralph Lilley (1962\u0026mdash;1966). A comparative dictionary of Indo-Aryan languages. London: Oxford University Press. https://dsalsrv04.uchicago.edu/dictionaries/soas/\nZoller, Claus Peter (2016). Outer and Inner Indo-Aryan, and northern India as an ancient linguistic area, Acta Orientalia 77, pp. 71\u0026mdash;132.\n","permalink":"http://localhost:1313/posts/2020-12-14-nia-conservatisms/","summary":"Certain *ārtá: MIA āṭā \u0026lsquo;flour\u0026rsquo;, Hindi āṭā, Punjabi āṭṭā, Romani (j)aro \u0026hellip; (Turner: 1338)\nPIIr. *HārHtás \u0026rsquo;that which is ground\u0026rsquo;, vriddhi-form of *Hr̥H-tás \u0026lsquo;ground\u0026rsquo; \u0026lt; PIE *h₂elh₁- \u0026rsquo;to grind\u0026rsquo;; cf. MIA āṭā, Persian ārd \u0026lsquo;flour\u0026rsquo;, Avestan aṣ̌a \u0026lsquo;ground\u0026rsquo;, Old Armenian ałam \u0026rsquo;to grind\u0026rsquo;.\nDoesn\u0026rsquo;t seem that the r in the cluster has been preserved in any NIA language (Romani (j)aro has the usual t → r / V_V change), but lack of MIA dental attā ~ ātā supports the presence of r which causes retroflexion.","title":"New Indo-Aryan conservatisms"},{"content":" This is a question I’ve been dealing with ever since I went down this academic path, and one that I think I will continue to grapple with for a long time: what is the point of studying linguistics?\nPart of the reason I feel like I have to justify this is that my culture, reflected in my upbringing, favours “hard” sciences as noble fields—medicine, physics—or fields with obvious practical applications—software engineering, finance. But this question also arises in my mind because of my own misgivings about linguistics, which seems as impractical a field as can be, divorced from the suffering of people or the problems of the world. What good can I do as a linguist?\nI’d like to point out that studying languages is different from studying language. The study of individual languages needs a different set of justifications than the ones I’m thinking of here, and those will depend on the languages in question too. Also, studying the idea of language is different from studying the use of language; the latter, I presume, means rhetoric or literary criticism, which are on another level of abstraction than what I am talking about: the human capacity for language and the systems that underlies all human languages which are the subject matter of linguistics.\nI don’t have any concrete answers yet nor have I put in a lifetime of thought into this issue, so I don’t discount the experiences of other, wiser linguists. I’m only speaking for myself.\nInherent value of language Language is a system that is an important part of our existence as human beings. Everything we do is predicated on a systematic means of communication (and, note, that doesn’t have to be a specific language or dialect—there are many forms that our communication takes). I think it is not unreasonable to say that there is some kind of inherent intellectual value in studying the important systems that underlie human existence.\nIt’s like mathematics. Math is a totally human-devised construct, and we expand on it while being constrained by its rules. It is, at its foundation, an abstract system that has no basis in reality—but this set of rules turns out to be incredibly useful in fields that no one would expect it to have value in, from physics to chemistry to economics and social science (and, indeed, linguistics).\nLanguage, too, also a human-devised abstract construct, except it seems to have arisen in a far less conscious, but no less systematic, manner. And yet, no matter who you are, rich or poor, young or old, online or offline, you deal with language almost every moment of your entire life, even just to think to yourself. It’s so pervasive—why shouldn’t we study language? Seems like we’d be ignoring a huge part of our lives.\nWhat are the ends of that intellectual value inherent in the study of language? I suppose better understanding our world, whether that is towards understanding the mind as a computation device (cognitive science), the creation of human-like computation devices (artificial intelligence and computer science), or understanding how our experience is shaped by language (psychology). Pushing into the social sciences, language, and languages, are an important part of the narratives investigated by historical studies and underpin how communities form and interact. And things like grammar, information encoding, etc. all underlie theories of computation and logic which are neat mathematical ideas in themselves.\nFor a physicist, a mathematician, or a computer scientist, this is what makes language interesting.\nLanguage and culture As an Indian immigrant to the United States, the value of language as a part of my cultural heritage is immense. It was probably the main reason I was drawn to linguistics: my desire to really learn Hindi in the later part of my childhood was the first time I came to appreciate a language as something bigger than a class at school. Whole literatures unsealed, millennia-long traditions, a family that I was suddenly much closer to—all outcomes of my learning Hindi.\nFor a linguist, studying the immense diversity of the world’s languages is a really wonderful experience. The ways that we express ourselves are so different and yet there is some universality of meaning in every language. There is, given enough time and patience by the listener, nothing one can say in English that one also cannot say in Hindi, in Kashmiri, in Zhuang, or in Cree. But, the ways we say things in different languages are inextricably tied to the culture of that language—languages do not grow in a vacuum. We’ll find meaningful differences for the same meanings across languages. Isn’t that crazy to think about? Crazy enough to be worth studying I think.\nBut that is from the perspective of the typologist, investigating diversity; not necessarily something you need to be seeped in a culture to do. From the perspective of a field linguist or a language activist though, there is some inherent value in the language at hand. Maybe it’s not spoken much anymore. Maybe there is a long oral tradition at risk of dying out or one that is worth disseminating to the rest of the world. Maybe there are wondrous unique qualities of the language that will help us better understand or shape models of the human capacity of language. Every language has some kind of value to someone, or we’d all be speaking a single language by now.\nI’ve had this sort of experience doing a small fieldwork project on Kholosi, a very small Indo-Aryan language of southwest Iran. It started out as an interesting project, but talking to my language contact made me realise how much language means to its users. There are songs and stories that would lose their lustre or nuance if he had to translate them to Persian or English.\nFor a humanist or philosopher, perhaps this is the most compelling reason to study language.\nWhat now? If you’re interested in majoring in linguistics as an undergrad, I really suggest looking at the department websites of the universities you’re applying to in order to get a sense of what they work on. Many departments have a “why linguistics?” page. There’s also Linguistic Society of America’s pamphlet on the topic.\nWhen I was going through the admissions process, I had narrowed my options down to Georgetown and Duke. Duke has a much stronger engineering department, which was good for my ambitions in computer science (my other major). But their Linguistics department was not as fleshed out as Georgetown’s, and computational linguistics research was happening in their Electrical Engineering department, meaning things like parsing unstructured data, machine learning algorithms, etc. were of more importance to them than understanding language for itself (which is what compling at Georgetown worked towards). Ultimately, my intellectual interests were toward understanding how language fundamentally works, not how to make use of language. That’s why I picked Georgetown.\n","permalink":"http://localhost:1313/posts/2020-12-12-why-linguistics/","summary":"This is a question I’ve been dealing with ever since I went down this academic path, and one that I think I will continue to grapple with for a long time: what is the point of studying linguistics?\nPart of the reason I feel like I have to justify this is that my culture, reflected in my upbringing, favours “hard” sciences as noble fields—medicine, physics—or fields with obvious practical applications—software engineering, finance.","title":"Why Linguistics?"},{"content":" Cool Papers Notes Claire Cardie: Information Extraction Nov 16, 10:00 AM\nClaire Cardie delivered a really cool keynote on information extraction from a historical perspective. (Surprising and uncomfortable how much NLP research started out with U.S. military applications.) My notes here don't make sense, it's just for me to note down things to read since I didn't know information extraction was a thing!\nOne thing to think about: Can we use information extraction techniques to build useful resources for low-resource languages? I'm thinking extracting data from the currently unstructured DSAL dictionaries for example, or the recent effort to do so from The Linguistic Survey of India.\nNER: Akbik et al. (2018, 2019) [CoNLL 03] relation extraction/classification: Soares et al. (2019) Miwa \u0026 Bansal (2016), Zhang et al. (2017), Wang et al. (2018), Luan et al. (2019), Wadden et al. (2019) event extraction (CNNs, RNNs) ACE same sentence tho Ralph Grishman BOAF: Semantics Nov 17, 4:00 PM The lexical semantics session at #emnlp2020 was super cool yet again. Some of the neat questions discussed:\n1. How much do we need explicit linguistic data (things like syntax trees) in our models? Seems like NLP is moving towards learning those implicitly.\n\u0026mdash; Āryaman Arora 𑀆𑀭𑁆𑀬𑀫𑀦 𑀅𑀭𑁄𑀭𑀸 (@aryaman2020) November 17, 2020 Siva Reddy, Dipanjan Das, Ellie Pavlick, Matt Gardner, Chris Potts. Move to contextual representations is a better approximation of how linguistics thinks about language (Chris Potts), explicit linguistic structures [I suppose things like POS tags, dependencies, etc., \"explicit things like a parser\"] is going to be declining as we have better models that don't need that information and can learn it implicitly (Matt Gardner). [I am reminded of Ethan A. Chi's work on extracting syntactic representations from mBERT]. Nathan Schneider: Can NLP work towards helping linguistics too? (Seems like we only talk about the other way). Dipanjan Das: We have not yet fully explored the capabilities of transformers and other masked LMs, we should not discount them and say diminishing returns are inevitable. Example given of multi-digit arithmetic skills appearing with greater inputs. [but GPT-3 is huge! at what point do we draw the line? humans do more with far less.] Ellie Pavlick: We can't pick up world knowledge or even full level human-level language just from masked LMs (\"hints\" on the internet can be picked up), there's got to be a more efficient [multimodal?] way. But it is very possible that an NN can have human-level language skill, just not the current masked LM approach. multi-hop reasoning Why only text-in text-out? \"Fundamentally awkward\" way to think about language (Pavlick). Need embodying in the world. [What the heck even is language? Why are we using text as a proxy? I wonder why NLP work doesn't deal with, like, speech directly (outside ASR which is just a way to interpret into text).] \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. Linguistic vs. world knowledge, not a division actually reflected in masked language models which are learning both together. Lead to rethinking linguistics? Potts: Deep learning isn't gonna replace linguistics! Blackboxes Black-box explanation methods (LME, SHAP, Partial dependence) are when you don't have the training data, glass-box (EBM) when you do. Accuracy vs. intelligibility tradeoff? No longer the case necessarily, thanks to explainable boosting machines. Comparable to full-complexity models. EBMs are a type of generalised additive models (GAM) i.e. sums of functions or sums of functions of pairwise interactions. https://github.com/interpretml/interpret ","permalink":"http://localhost:1313/posts/2020-11-18-emnlp/","summary":"Cool Papers Notes Claire Cardie: Information Extraction Nov 16, 10:00 AM\nClaire Cardie delivered a really cool keynote on information extraction from a historical perspective. (Surprising and uncomfortable how much NLP research started out with U.S. military applications.) My notes here don't make sense, it's just for me to note down things to read since I didn't know information extraction was a thing!\nOne thing to think about: Can we use information extraction techniques to build useful resources for low-resource languages?","title":"EMNLP 2020"},{"content":"ACL 2020 was the most exciting event in my life so far as a proto-computational linguist. This was my first conference, and my first published research paper. I never thought I would be doing something like this when I started out with computer science (something that was fun and exciting), and when I started out with linguistics (something that became a hobby as I sought to improve my Hindi). I learned, I grew, all while meeting some fantastic people who gave me advice that I will pay heed to as I continue forward.\nI had a great time livetweeting in Hindi and English as well as joining the myriad extra-conference sessions.\nWhere the conference would have been :(\nAnd despite the conference being virtual and all the hassles that entailed, I felt I was able to get to know a lot of people and felt welcomed by the *CL community.\nI decided, in order to protect the important memories that my brain will discard eventually, to write up some of the things I have learned as well as my thoughts on the conference itself. Future me will certainly find it useful, but I hope others do too.\nMy Paper, Getting There Our paper (my coauthors were Luke Gessler and Dr. Nathan Schneider) at the conference was \u0026ldquo;Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi\u0026rdquo;. Basically, we focused in on the problem of schwa deletion in Hindi and Punjabi. That is the primary issue in text-to-speech for those languages, and we proposed a high successful machine learning solution.\nI\u0026rsquo;ve been very luck to get to attend a conference of this magnitude and quality even before my freshman year of college. The reality is that lots of things aligned for me being where I am. I\u0026rsquo;m especially thankful to have a great mentor in Dr. Nathan Schneider, who has made me super excited to be in this field and been very willing to teach me, an absolute beginner, as well as great people to work with like Luke.\nSo when I got the chance to see the brightest people in the field gathered together sharing all their ground-breaking work? Of course I wanted to make the most of it!\nPapers, Talks My original idea about the conference was that the paper talks would be the most important part of it. I don\u0026rsquo;t think that\u0026rsquo;s entirely true anymore—socializing and talking to interesting people is the real benefit. Regardless, I learned immeasurably from the paper talks. There was no talk that did not teach me something new, and often I learned about whole subfields that I had not heard of before. (I admittedly gave up some family Netflix time to binge-watch the talks.) It was honestly the most exciting thing to me.\nFor the talks I enjoyed the most I tried to livetweet them in Hindi under #acl2020hi. (I was assigned the Lexical Semantics track for that but I livetweeted pretty much anything interesting.) This was beneficial in a couple ways:\nIt forced me to pay attention to what was going on and take notes. Surprisingly, I\u0026rsquo;d don\u0026rsquo;t like saying stupidly wrong things publicly! It let me practice my Hindi in an engaging way. I\u0026rsquo;m somewhere between a native and heritage speaker so I always take the opportunity to get better. I also got to come up with some cool words like अंतःस्थापन \u0026ldquo;embedding\u0026rdquo;. The combined mental effort of both of these meant I could get the most out of every talk and I didn\u0026rsquo;t descend into mindless NLP Netflix. On the other hand, it made me very tired and I\u0026rsquo;m not sure it would be as feasible at a physical conference.\nNevertheless, I would advise future me to always take notes at conferences. I think it\u0026rsquo;s important to maximize how much I get out of events like these, especially ACL because it\u0026rsquo;s only a couple days of the year. And obviously, it\u0026rsquo;s super exciting to watch (hopefully \u0026ldquo;attend\u0026rdquo; in future years) the talks but remembering what was exciting is pretty great too.\nAs for the contents of the talks themselves\u0026hellip; they were just so cool! I hope to be as excited for future conferences as I was for this one. Some of my favourite papers were:\nDavid Wilmot, Frank Keller. “Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation.” Mario Giulianelli, Marco Del Tredici, Raquel Fernández. “Analysing Lexical Semantic Change with Contextualised Word Representations.” Kenneth Joseph, Jonathan Morgan. “When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?” Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro, Animesh Mukherjee. “Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection.” I generally like the way the virtual site was organised. It was very easy to navigate and the attached RocketChat channels made asking questions very easy. The only real disappointment of the conference system was the Q\u0026amp;A Zoom session format, which seemed redundant and clunky especially relative to the RocketChat. It\u0026rsquo;s still unclear what an hour-long Q\u0026amp;A was supposed to do. The ones I went in were quite sparse in audience, and no one showed up to one of mine. Maybe some kind of live presentation would have been more interesting?\nMeetings, People The people I met at ACL were awesome and very friendly. I can\u0026rsquo;t think of any community I\u0026rsquo;ve been in that is as welcoming, and *CL is certainly what I would want any community that I am a part of to emulate. Everyone I\u0026rsquo;ve talked to has been genuinely interested; even in the RocketChat, people would drop by and say nice things about our paper (as well as ask great questions). We even got livetweeted about in Nepali and Hindi!\nरिसर्च शीर्षक - Supervised grapheme-to-phoneme conversion of orthographic schwas in Hindi and Punjabi\nअनुसन्धान कर्ता - @aryaman2020 @LukeGessler @complingy\nयहाँ पुरा पढनुहोस् - https://t.co/NpnqO5iSC1#acl2020nlp #acl2020ne @aclmeeting\n— Oyashi (@oya163) July 9, 2020\n#acl2020nlp #acl2020hi अरोरा, आर्यमन : Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi\nशोध पत्र : https://t.co/y6Y2T4nXYy#acl2020nlp #acl2020hi (1/n)\n— Rahul Mittal (@rahul14mittal) July 6, 2020\nThe Birds of a Feather session that I was able to attend was on Lexical Semantics, organized by Tiago Torrent. It was quite interesting to see how meaning and structure cooperate and all the research questions that arise from it (as well as meet the people who work on it), and I\u0026rsquo;m sorry I wasn\u0026rsquo;t able to attend more of those sessions. I also enjoyed meeting the extended academic family of Noah Smith, wonderful people at the University of Chicago and McGill University at a meeting organized by Jackie C. K. Cheung, the very cool people who attended the ACL Undergraduate Panel led by Sasha Rush, Suchin Gururangan, and Sabrina J. Mielke (although I was too shy to ask questions in that one, it was still very informative), other people interested in Low-Resource NLP at the session organized by Ibrahim Sharaf, and Kyle Gorman at SIGMORPHON. It was nice to see all the varied experiences and interests in the community.\nSome of the people who gave me really great advice were Rishi Bommasani, Nelson Liu, Tiago Torrent, and Luca Soldaini. I had many great short conversations with several other people as well. I learned a lot from them and these conversations will without a doubt inform the future steps in my career. It really made me happy to be part of a community with these kinds of people.\nSo, to future me, I say talk to people and don\u0026rsquo;t be afraid to start the conversation. It\u0026rsquo;s definitely the most rewarding part of a conference, to be surrounded by people with the same interests yet with different (read: more) experiences and knowledge to give.\nFuture, Present Where do I stand now? What do I take away from this? If there\u0026rsquo;s been any consistent thread tying all my conversations, all the advice I got, all that I learned here, it\u0026rsquo;s that trying new things is important. Diversity of experience in the present moment is the most valuable thing I can have. It\u0026rsquo;s too early to worry about what form my career will take in a decade, what subfield I\u0026rsquo;ll specialize in, or even what will happen after my undergrad. I have four formative years in front of me in which I can figure out what is exciting for me and what I want to do in the years that follow, so it makes sense that I explore all my options. I\u0026rsquo;m sure research is the path I will stay on, but that doesn\u0026rsquo;t preclude other fields like software engineering, data science, historical linguistics, and (of course!) all the non-academic portions of a college education.\nOne thing to consider is how much I want to remain working on South Asian languages. It seems the cutting edge of the field is squarely in the courtyard of English, which is concerning. But I foresee my work being the most important when it deals with languages like my own, because they sorely need it. This is probably something I will continue having to figure out.\nAnd what about the field? It seems, as someone very new, that there is a reckoning arriving in NLP/CL. People are frustrated with black box models, increasingly massive data that powers it, and just a general feeling of not knowing what is actually going on in the field. Sure, the metrics are up and there is money to be made from those performance gains. But the fundamental questions of linguistics have not been addressed at all by NLP/CL. How do we, as human beings, generalize the rules of language so well? How do we understand languages? How does language even exist? And, most unnervingly, what are computational systems even learning that makes these metrics go up? For a new researcher, this was a great time to have a Theme track at ACL that examines this from above. I think it\u0026rsquo;s an exciting time to be in the field, to be grappling with these questions that don\u0026rsquo;t have any clear answers awaiting us.\nAnd so, I say ACL 2020 was a success, not only in how it occurred but in what I personally got from it. I really look forward to attending more conferences in the field (and out of the field!) and to growing as a researcher. I believe that the future is generally bright. I think good things are happening in the field. And I\u0026rsquo;m excited beyond measure at the prospect of having something to contribute in it.\n","permalink":"http://localhost:1313/posts/2020-07-10-acl/","summary":"ACL 2020 was the most exciting event in my life so far as a proto-computational linguist. This was my first conference, and my first published research paper. I never thought I would be doing something like this when I started out with computer science (something that was fun and exciting), and when I started out with linguistics (something that became a hobby as I sought to improve my Hindi). I learned, I grew, all while meeting some fantastic people who gave me advice that I will pay heed to as I continue forward.","title":"ACL 2020"},{"content":" These are just some notes about Hindi SNACS that I wanted to be publicly available. I would use Google Docs, but they don't have good support for interlinear glosses. I would use Xposition, but I'm not really sure what the best place for it is there.\nTheme Optional को-marking Use Theme with appropriate construals as needed. The postposition को optionally marks a Theme only when there is some prototypical Agent involved in the action. That Agent isn't necessarily even named in the sentence. There is also evidence that the optional को marks more salient or definite objects (e.g. it is not optional on animate objects). Magier (1987) has a great analysis of what this kind of को means so I will not repeat it. मैंने {उसकोTheme} मारा।\n1SG-ERG 3SG-DAT hit-PRF\nI hit him.\nमैंने {उसकोStimulus--Theme} देखा।\n1SG-ERG 3SG-DAT see-PRF\nI saw her.\nAgent I have decided to apply Agent as the function (or Causer for inanimate agents) to all uses of ने, को, and से to mark an agentive subject regardless of volition. The issues in this are that (1) all three denote different levels of volition, and (2) they are used in very different constructions. ने is almost always the ergative subject, को used for the subject in modal sentences indicating necessity or obligation, and से used for subjects in passive constructions. The last two are a little tough to decide on, but I've done my best in detailing my decisions below. Impelled agents marked by से se Use Agent--Instrument (or any of the other relevant ने scene roles). The impelled agent (मध्यस्थ कर्ता) is a mediator forced to do some action by a full Agent (marked with ने). They lack the volition of the full agent. This construction involves a second causative verb. मैंने बाई से बच्चे को सुलवाया।\nI-ERG maid INS child DAT make-sleep-PRF\nI made the maid put the child to sleep.\nFirst of all, the function is undoubtedly Instrument since it is an entity applying causal force. Now what reasoning is there for the scene role not being the same? Well, if an actual Instrument is used to perform some action, the second causative verb is ungrammatical. A first causative verb is used when the intermediary has no agency at all. (Example adapted from Begum and Sharma, 2010). मैंने चाबी से ताला खोला।\nI-ERG key INS lock open-PRF\nI opened the lock with the key.\nमैंने मालिक से ताला खुलवाया।\nI-ERG owner INS lock make-open-PRF\nI made the owner open the lock.\nSo it's not Instrument--Instrument. That is why I have settled on using Agent--Instrument as the construal, since it captures both the impelling and the agency encoded in the construction. The problem is that instrumental agent does not perform the action with volition. However, I don't think volition should be considered in the formulation of SNACS as it stands. My thoughts below on modal and passive constructions will support that opinion. Modal constructions with को ko Treat the same as ने. को is an exact parallel to ने in non-modal constructions. Also, this parallelism is easily maintained without considering volition in other scene roles like Theme. Consider: मुझे रोना पड़ा।\n1SG-ERG cry-INF obligated-PRF\nI had to cry.\nमैं रोया।\n1SG cry-PRF\nI cried.\nBoth subjects get Theme. Ultimately, modality shouldn't matter in Agents; it should be, if encoded at all, should be separate of the scene role. Passive constructions with से se Treat the same as ने. से is used to mark the agent in passive constructions formed with the light verb जाना \"to go\". Building on the same logic used to construe को as Agent in modal constructions, we should stay parallel to the active construction, even though in passive constructions agentive force is dampened. मुझसे काम नहीं किया गया।\nI-INS work not do-PRF go-PRF\nThe work was not able to be done by me.\nNon-agentive uses of ने ne Use Theme--Theme. De Hoop and Narasimhan (2005) do an excellent job of analyzing the case markings of Hindi subjects. A good example for ने marking increased volition in bodily-emission verbs (\"to cry\", \"to urinate\", etc.) is given by them: राम {नेAgent} चीख़ा।\nRam ERG scream-PRF\nRam screamed (purposefully).\n{राम Theme} चीख़ा।\nRam-NOM scream-PRF\nRam screamed.\nHowever, they go on to conclude \"Ergative case thus expresses the volitionality of the agent argument in the intransitive examples above, but volitionality is not necessarily realized as ergative case.\" This solidifies my decision to include the less volitional uses of को and से as Agents. They go on to talk about how case markers on the subject actually have very little to do with agentivity and more to do with the semantics of the verb. One problem regarding the agentivity of ने arises in certain light verb constructions, e.g.: राम ने बहुत मार खायी।\nRam ERG much beating eat-PRF\nRam got severely beaten up. [lit. Ram ate a lot of beating.]\nराम ने उससे मार खायी।\nRam ERG he-INS beating eat-PRF\nRam got severely beaten up by him. [lit. Ram ate a lot of beating from him.]\nउसने राम को मारा।\nhe-ERG Ram DAT hit-PRF\nHe beat up Ram.\nThere is absolutely nothing agentive about Ram being beaten up, but the semantics of खाना force him to take an ergative marking anyways. Ram is unambiguously Theme. This affirms the utility of SNACS to Hindi, since case markers in isolation don't actually help in figuring out what role the marked object takes on so a computer won't be able to guess the semantic role off of it. Experiencer Dative subject Use Experiencer--Recipient, or a construal Theme--Recipient if the verb deal with external body events. I have been labelling को in this sense as Experiencer--Theme for quite a while now but looking deeper into the semantic role of experiencers in South Asian languages as a whole compels me to reclassify its function. It doesn't easily fit into subject (Agent) or object (Theme), which are the first functions that would come to mind for it. मुझको दुःख हुआ।\nI-DAT sadness be-PRF\nI felt sad.\nIn this example, we see that \"sadness\" is in the unmarked nominative case, so we expect it to be the subject intuitively. However, crediting Kachru (1990), we can use the test of reflexifization to find out what the subject is. If the reflexive genetive अपना is inserted, it refers back to the dative-marked noun, so that is the subject for sure. Now this seems straightforward. Just mark it Experiencer right? Well what about dative subjects such as: मुझको चोट लगी।\nI-DAT injury feel-PRF\nI got hurt.\nमुझको पहुँचने में एक घंटा लगा।\nI-DAT reach-INF-OBL LOC one hour feel-PRF\nIt took me an hour to arrive.\nThese aren't really Experiencer. The first one seems to be more of a Theme, but in Hindi body events that are outside the recipient's volition are treated just like cognitive or emotional events. The second one really has nothing to do with what we think of as an Experiencer in SNACS. This is an obvious Theme right? It's harder to make the case for it to be treated the same as the emotion example above. This illustrates that it is difficult to draw the line between Theme and Experiencer in Hindi (as well as, I suspect, other languages with such dative subjects). Kachru does look at semantic roles and suggests for the dative subject a hyperrole \"by combining Experiencer, Recipient, Goal, and Patient.\" The only commonality between these is that (1) something happens to them, and (2) they are not in control of what happens. But wait: मुझको ख़ुशी मिली।\n1SG-DAT happiness receive-PRF\nI felt happy.\nमुझको तोहफ़ा मिला।\n1SG-DAT gift receive-PRF\nI got a present.\nIndeed, मिलना's dative subject usage covers both Recipient and Experiencer, serving as the missing link to the indirect object functions of को discussed below. Another example: मुझे दिखा।\n1SG-DAT be-seen-PRF\nI saw it.\nमुझको दिखाई दिया।\n1SG-DAT sight give-PRF\nI saw it.\nIn the second one को serves as both the indirect object of receiving (देना) and a dative subject, much like मिलना. Ultimately, this overlap leads me to join the two use cases. In the two ambiguous cases I gave much earlier, Theme--Recipient is what should be used. Reading through Butt, Grimm, and Ahmed (2006) as well as seeing the Hindi example (if only I had seen it earlier!) in Hwang, et al. (2018) was immensely helpful. Recipient को-marked indirect objects of ditransitive verbs Use a separate function Recipient with appropriate construals. मैंने {उसकोRecipient} तोहफ़ा दिया।\n1SG-ERG 3SG-DAT gift give-PRF\nI gave him the gift.\nमैंने {उसकोExperiencer--Recipient} तोहफ़ा दिखाया।\n1SG-ERG 3SG-DAT gift show-PRF\nI showed him the gift.\nI was originally treating this as Goal but it has dawned on me that in this usage, the indirect object is always animate. Semantically, it is not the endpoint of the action but rather the one receiving the outcome. The distinction is fine enough that it was not immediately obvious to me. Notably, this means Hindi has little in terms of a prototypical Goal marker; the Locus postpositions (में, पर, पे) fill this gap. Conclusion Ultimately, the case-marking postpositional system of Hindi really deals with the intentionality and volition of the action at hand rather than semantic roles. The agentivity and volition of a participant is some function of both the case marker and the semantics of the verb that modify it. (This accounts for high-agentivity ने combined with negative-agency खाना \"to eat\" giving a Theme.)\nThe hierarchy I propose is: Postposition Case Meaning ने ergative High agency over the action. Generally volitional unless employed by a strongly unvolitional light verb construction (e.g. with खाना \"to eat\"). का genitive Neutral in terms of agency and volition. Can be used to describe any of the possible participant roles: उसका मरना \"his death\", उसका दुःख \"his sadness\", उसका तोड़ना \"his breaking (of x)\" से instrumental Less volitional or intentional but still can be used by full agents. Evidenced by impelled agents in causative constructions as well as passive agents. को dative Least volitional. Besides marking a protypical Theme, in the subject it denotes obligation in modal constructions and other generally non-volitional participants. References Begum, Rafiya, and Dipti Misra Sharma. \"A preliminary work on causative verbs in Hindi.\" Proceedings of the 8th Workshop on Asian Language Resources. 2010. Butt, Miriam, Scott Grimm, and Tafseer Ahmed. \"Dative subjects.\" NWO/DFG Workshop on Optimal Sentence Processing. 2006. De Hoop, Helen, and Bhuvana Narasimhan. \"Differential case-marking in Hindi.\" Competition and Variation in Natural Languages. Elsevier, 2005. 321-345. Hwang, Jena D., et al. \"Double trouble: the problem of construal in semantic annotation of adpositions.\" Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017). 2017. Kachru, Yamuna. \"Experiencer and other oblique subjects in Hindi.\" Experiencer subjects in South Asian languages. Stanford CA: CSLI Publications, 1990. 59-75. Magier, David. \"The transitivity prototype: evidence from Hindi.\" Word 38.3, 1987. 187-199. Narasimhan, Bhuvana. \"A lexical semantic explanation for ‘quirky’ case marking in Hindi.\" Studia Linguistica 52.1, 1998. 48-76. ","permalink":"http://localhost:1313/posts/2020-06-02-snacs/","summary":"These are just some notes about Hindi SNACS that I wanted to be publicly available. I would use Google Docs, but they don't have good support for interlinear glosses. I would use Xposition, but I'm not really sure what the best place for it is there.\nTheme Optional को-marking Use Theme with appropriate construals as needed. The postposition को optionally marks a Theme only when there is some prototypical Agent involved in the action.","title":"Hindi adpositions"},{"content":"I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.\nI want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I\u0026rsquo;ve been thinking about recently:\nHow can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc. In a self-supervised learning world, can linguistics still be useful in guiding how we do interpretability on language models? Can interpretability provide actionable findings that help us make better models? Machine learning is still a kind of alchemy. We should turn it into a science. To that end, I am inspired by work in NLP, causal inference, information theory, and psycholinguistics.\nOh, and besides doing research, I enjoy eating spicy food, (attempting to) play basketball and climb, and listening to rap. And if you handed me a violin, I would probably be able to make some sounds that are not too unpleasant.\nIf you want to chat about research or life, feel free to send me an email (aryamana [at] stanford [dot] edu)!\nBrief history I was born in New Delhi, India, raised in Savannah, Georgia (the U.S. state), and I think of home as Washington, D.C.\u0026mdash;where I spent part of high school and my undergrad. Still, I\u0026rsquo;ve wanted to move to the Bay Area for a long time, and I\u0026rsquo;m glad I made it here!\nBefore coming to Stanford to start my Ph.D. in 2023, I completed my B.S. in Computer Science and Linguistics at Georgetown University. There, I was mentored by Nathan Schneider as a member of his research group NERT. In those days, I primarily worked on computational linguistics and did a lot of linguistic annotation for Indian languages. Regardless of what I currently work on, my research style is probably largely copied from Nathan\u0026rsquo;s.\nSince 2021, I have also been closely working with Ryan Cotterell at ETH Zürich on information theory, and I visited Switzerland in Summer 2021 and 2023. From working with Ryan, I learned to be a little less scared of doing math.\nIn 2022, I spent the summer at Apple in Seattle with Robert Daland working on evaluating robustness on a ton of languages for Siri, and winter at Redwood Research in Berkeley working on mechanistic interpretability.\nMy research interests pivoted significantly in late 2022 towards interpretability, but I still have a love for language(s).\n","permalink":"http://localhost:1313/about/","summary":"I am a first-year Ph.D. student at Stanford NLP advised by Dan Jurafsky and Christopher Potts. My research is focused on interpretability.\nI want to understand how neural networks (like language models) work. I believe that this is a tractable goal that can be accomplished in my lifetime. Some things I\u0026rsquo;ve been thinking about recently:\nHow can we ensure that explanations of model behaviour are actually faithful? New methods grounded in causal inferences are promising, but we still need more theory, benchmarks, metrics, etc.","title":"About"},{"content":"Being acknowledged is way cooler than being cited, so here\u0026rsquo;s a list of things I am acknowledged in.\nI would like to acknowledge Chenglei Si for giving me the idea to do this.\nList Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang. 2024. Design2Code: How far are we from automating front-end engineering?. arXiv:2403.03163. Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush. 2024. ColorSwap: A color and word order dataset for multimodal evaluation. arXiv:2402.04492. Tristan Thrush, Jared Moore, Miguel Monares, Christopher Potts, Douwe Kiela. 2024. I am a Strange Dataset: Metalinguistic tests for language models. arXiv:2401.05300. Shira Wein. 2023. Human raters cannot distinguish English translations from original English texts. In EMNLP. Maitrey Mehta, Vivek Srikumar. 2023. Verifying annotation agreement without multiple experts: A case study with Gujarati SNACS. In ACL. Rohan Pandey. 2023. Semantic composition in visually grounded language models. Bachelor\u0026rsquo;s thesis, Carnegie Mellon University. Rohan Pandey. 2023. Syntax-guided neural module distillation to probe compositionality in sentence embeddings. In EACL. Yang Janet Liu, Jena D. Hwang, Nathan Schneider, Vivek Srikumar. 2022. Putting context in SNACS: A 5-Way classification of adpositional pragmatic markers. In LAW. Ola Wikander. 2022. The Borrowings Kṣuta-/kṣut- (“Inimical”) and Vidumāla- (“Retrograde”) in Sanskrit Astrological Texts and the Representation of Semitic ʿayn in Similar Loans. In History of Science in South Asia, 10:1\u0026ndash;283. Nathan Schneider, Amir Zeldes. 2021. Mischievous nominal constructions in Universal Dependencies. In UDW, SyntaxFest. Hilaria Cruz. 2021. Las tecnologías de Reconocimiento Automático de Voz y su incorporación a los métodos de transcripción de lenguas indígenas. In Anales de antropología 55(2). Lucas F.E. Ashby, \u0026hellip;, Winnie Yan. 2021. Results of the Second SIGMORPHON Shared Task on Multilingual Grapheme-to-Phoneme Conversion. In SIGMORPHON. Jakob Prange, Nathan Schneider. 2021. Draw mir a Sheep: A supersense-based analysis of German case and adposition semantics. In Künstliche Intelligenz 35:291\u0026ndash;306. Samopriya Basu. 2021. Inverse problems for a class of stochastic ordinary differential equations in a generalized fiducial framework. Ph.D. thesis, UNC, Chapel Hill. Amit Arora. 2014. Sustainability strategies in supply chain management. Ph.D. thesis, Georgia Southern University. ","permalink":"http://localhost:1313/acks/","summary":"Being acknowledged is way cooler than being cited, so here\u0026rsquo;s a list of things I am acknowledged in.\nI would like to acknowledge Chenglei Si for giving me the idea to do this.\nList Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang. 2024. Design2Code: How far are we from automating front-end engineering?. arXiv:2403.03163. Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush. 2024. ColorSwap: A color and word order dataset for multimodal evaluation.","title":"Acknowledgements"},{"content":"You can also see my profiles on Google Scholar and Semantic Scholar.\nAlso check out acknowledgements.\n2024 Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts. 2024. pyvene: A library for understanding and improving PyTorch models via interventions. arXiv:2403.07809. Aryaman Arora, Dan Jurafsky, Christopher Potts. 2024. CausalGym: Benchmarking causal interpretability methods on linguistic tasks. arXiv:2402.12560. Nay San, Georgios Paraskevopoulos, Aryaman Arora, Xiluo He, Prabhjot Kaur, Oliver Adams, Dan Jurafsky. 2024. Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens. In SIGTYP. Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and Noah D. Goodman. 2024. A reply to Makelov et al. (2023)’s “interpretability illusion” arguments. arXiv:2401.12631. Kabilan Prasanna, Aryaman Arora. 2024. IruMozhi: Automatically classifying diglossia in Tamil. In Findings of NAACL. 2023 Vedant Palit*, Rohan Pandey*, Aryaman Arora, Paul Pu Liang. 2023. Towards vision-language mechanistic interpretability: A causal tracing tool for BLIP. In 5th Workshop on Closing the Loop Between Vision and Language. Omer Goldman, Khuyagbaatar Batsuren, Salam Khalifa, Aryaman Arora, Garrett Nicolai, Reut Tsarfaty, Ekaterina Vylomova. 2023. SIGMORPHON–UniMorph 2023 Shared Task 0: Typologically diverse morphological inflection. In SIGMORPHON. Aryaman Arora, Adam Farris, Samopriya Basu, Suresh Kolichala. 2023. Jambu: A historical linguistic database for South Asian languages. In SIGMORPHON. Brett Reynolds, Aryaman Arora, Nathan Schneider. 2023. Unified syntactic annotation of English in the CGEL framework. In LAW. Aryaman Arora. 2023. Investigating induction heads in a small transformer language model. In MASC-SLL, Arlington, VA, USA. Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, Aryaman Arora. 2023. Localizing model behavior with path patching. arXiv:2304.05969. 2022 Ryan Cotterell, Richard Futrell, Kyle Mahowald, Clara Meister, Tiago Pimentel, Adina Williams, Aryaman Arora. 2022. Information theory in linguistics: Methods and applications. In COLING (tutorials). Brett Reynolds, Aryaman Arora, Nathan Schneider. 2022. CGELBank: CGEL as a framework for English syntax annotation. arXiv:2210.00394. Jordan Kodner, \u0026hellip;, Aryaman Arora, \u0026hellip;, Ekaterina Vylomova. 2022. SIGMORPHON–UniMorph 2022 Shared Task 0: Generalization and typologically diverse morphological inflection. In SIGMORPHON. Khuyagbaatar Batsuren, Gábor Bella, Aryaman Arora, \u0026hellip;, Ryan Cotterell, Ekaterina Vylomova. 2022. The SIGMORPHON 2022 Shared Task on Morpheme Segmentation. In SIGMORPHON. Aryaman Arora. 2022. Universal Dependencies for Punjabi. In LREC. Aryaman Arora, Nitin Venkateswaran, Nathan Schneider. 2022. MASALA: Modelling and analysing the semantics of adpositions in linguistic annotation of Hindi. In LREC. Khuyagbaatar Batsuren*, Omer Goldman*, \u0026hellip;, Aryaman Arora, \u0026hellip;, Ryan Cotterell, Reut Tsarfaty, Ekaterina Vylomova. 2022. UniMorph 4.0: Universal Morphology. In LREC. Aryaman Arora, Nathan Schneider, Brett Reynolds. 2022. A CGEL-formalism English treebank. In MASC-SLL, Philadelphia, PA, USA. Aryaman Arora, Clara Meister, Ryan Cotterell. 2022. Estimating the entropy of linguistic distributions. In ACL. Aryaman Arora, Adam Farris, Samopriya Basu, Suresh Kolichala. 2022. Computational historical linguistics and language diversity in South Asia. In ACL. Adam Farris*, Aryaman Arora*. 2022. DIPI: Dependency parsing for Ashokan Prakrit historical dialectology. In Towards a comparative historical dialectology: evidence from morphology and syntax, DGfS, Tübingen, Germany. 2021 Adam Farris*, Aryaman Arora*. 2021. For the purpose of curry: A UD Treebank for Ashokan Prakrit. In UDW, SyntaxFest. Aryaman Arora, Adam Farris, Gopalakrishnan R, Samopriya Basu. 2021. Bhāṣācitra: Visualising the dialect geography of South Asia. In LChange. Aryaman Arora, Ahmed Etebari. 2021. Kholosi Dictionary. Aryaman Arora, Nitin Venkateswaran, Nathan Schneider. 2021. Adposition and case supersenses v1.0: Guidelines for Hindi–Urdu. arXiv:2103.01399. Aryaman Arora, Nitin Venkateswaran, Nathan Schneider. 2021. SNACS annotation of case markers and adpositions in Hindi. In SCiL. 2020 Michael Kranzlein, Emma Manning, Siyao Peng, Shira Wein, Aryaman Arora, Nathan Schneider. 2020. PASTRIE: A corpus of prepositions annotated with supsersense tags in Reddit International English. In LAW. Aryaman Arora, Nathan Schneider. 2020. SNACS annotation of case markers and adpositions in Hindi. In SIGTYP. Non-archival extended abstract. Aryaman Arora, Luke Gessler, Nathan Schneider. 2020. Supervised grapheme-to-phoneme conversion of orthographic schwas in Hindi and Punjabi. In ACL. 2019 Aryaman Arora, John R. McIntyre. 2019. Quasi-passive lower and upper extremity robotic exoskeleton for strengthening human locomotion. In Sustainable Innovation. ","permalink":"http://localhost:1313/papers/","summary":"You can also see my profiles on Google Scholar and Semantic Scholar.\nAlso check out acknowledgements.\n2024 Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts. 2024. pyvene: A library for understanding and improving PyTorch models via interventions. arXiv:2403.07809. Aryaman Arora, Dan Jurafsky, Christopher Potts. 2024. CausalGym: Benchmarking causal interpretability methods on linguistic tasks. arXiv:2402.12560. Nay San, Georgios Paraskevopoulos, Aryaman Arora, Xiluo He, Prabhjot Kaur, Oliver Adams, Dan Jurafsky.","title":"Papers"}]